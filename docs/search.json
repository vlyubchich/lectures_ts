[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This is a collection of lecture notes on applied time series analysis and forecasting using the statistical programming language R. Many of these lectures are based on the original notes by Y. R. Gel for the course STAT-443 Forecasting (University of Waterloo, Canada) adapted and expanded by V. Lyubchich for the course MEES-713 Environmental Statistics 2 (University of Maryland, USA).\nEach lecture starts by listing the learning objectives and required reading materials, with additional references in the text. The notes introduce the methods and give a few examples but are less detailed than the reading materials. The notes do not substitute a textbook."
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Time Series Analysis",
    "section": "Authors",
    "text": "Authors\nVyacheslav Lyubchich is an Associate Research Professor at the University of Maryland Center for Environmental Science. https://www.umces.edu/vyacheslav-lyubchich\nYulia R. Gel is a Professor of Statistics at the University of Texas at Dallas. https://personal.utdallas.edu/~yxg142030/"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Time Series Analysis",
    "section": "Citation",
    "text": "Citation\nLyubchich, V. and Gel, Y. R. (2022) Time Series Analysis. Lecture Notes. Draft 2022-12."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Time Series Analysis",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "l02_tsintro.html",
    "href": "l02_tsintro.html",
    "title": "1  Introduction to Time Series Analysis",
    "section": "",
    "text": "The goal of this lecture is to introduce time series and their common components. You should become confident in identifying and characterizing trends, seasonal and other variability based on visual analysis of time series plots and plots of autocorrelation functions.\nObjectives\nReading materials"
  },
  {
    "objectID": "l02_tsintro.html#time-series-and-its-components",
    "href": "l02_tsintro.html#time-series-and-its-components",
    "title": "1  Introduction to Time Series Analysis",
    "section": "1.1 Time series and its components",
    "text": "1.1 Time series and its components\nA time series is a set of observations \\(Y_t\\), each one being recorded at a specific time \\(t\\) (Brockwell and Davis 2002). The index \\(t\\) will typically refer to some standard unit of time, e.g., seconds, hours, days, weeks, months, or years.\nA time series is a collection of observations made sequentially through time (Chatfield 2000).\nA stochastic process is sequence of random variables \\(Y_t\\), \\(t = 1, 2, \\ldots\\) indexed by time \\(t\\), can be written as \\(\\{Y_t, t\\in[1,T]\\}\\). A time series is a realization of a stochastic process.\nWe shall frequently use the term time series to mean both the data and the process.\n\n\nCode\np <- forecast::autoplot(TSstudio::Michigan_CS) + \n    xlab(\"\") + \n    ylab(\"Index\") + \n    theme_light()\nplotly::ggplotly(p)\n\n\n\n\n\nFigure 1.1: Monthly index of consumer sentiment, University of Michigan Consumer Survey (1966:Q1 = 100).\n\n\n\n\n\nCode\nggplot2::autoplot(JohnsonJohnson) + \n    xlab(\"\") + \n    ylab(\"Earnings per share (USD)\") + \n    theme_light()\n\n\n\n\n\nFigure 1.2: Quarterly earnings (dollars) per Johnson & Johnson share, 1960–1980.\n\n\n\n\n\n\nCode\nggplot2::autoplot(MASS::accdeaths) + \n    xlab(\"\") + \n    ylab(\"Number of accidental deaths\") + \n    theme_light()\n\n\n\n\n\nFigure 1.3: Monthly totals of accidental deaths in the USA, 1973–1978.\n\n\n\n\nIn this course, we will be interested in constructing time series models to learn more about their properties and to forecast (predict) future values of \\(Y_t\\), i.e., values of \\(Y_t\\) for \\(t\\) beyond the end of the data set. Typically, we will use the historical data (or some appropriate subset of it) to build our forecasting models.\n\n1.1.1 Decomposition of time series\nA time series can generally be expressed as a sum or product of four distinct components: \\[\nY_t = M_t + S_t + C_t + \\epsilon_t,\n\\] or \\[\nY_t = M_t \\cdot S_t \\cdot C_t \\cdot \\epsilon_t,\n\\] where\n\n\\(M_t\\) is the trend, representing the average change (change in the mean) in the time series over time. Examples of trends are:\n\\(M_t = \\beta_0\\) (constant over time, we usually refer to this case as ‘no trend’);\n\\(M_t = \\beta_0 + \\beta_1t\\) (linear increase or decrease over time);\n \\(M_t = \\beta_0 + \\beta_1t + \\beta_2t^2\\) (quadratic over time).\n\n\\(S_t\\) represents regular periodic fluctuations (a.k.a. seasonality) in the time series. \\(S_t\\) has the property that it is not constant but there exists an integer \\(m \\geqslant 2\\) and scaling factors \\(\\lambda_k > 0\\) such that \\(S_{t+km} = \\lambda_kS_t\\) for \\(1 \\leqslant t \\leqslant m\\) and each \\(k \\geqslant 1\\). The smallest such \\(m\\) is called the period. Periodic time series are quite common and include seasonal variability (period of 1 year), diurnal (period 24 hours), and other cycles such as tides (period 12 hours 25 minutes). This component can be modeled using sinusoidal functions or indicator variables.\n\\(C_t\\) represents irregular cyclical fluctuations, i.e., sinusoidal-type movements that are of irregular length and are not as predictable as the seasonal component, e.g., El Ni~{n}o Southern Oscillation (ENSO) and macroeconomic business cycles. We do not explicitly model \\(C_t\\) in this course.\n\\(\\epsilon_t\\) is the residual or error and represents the remaining unexplained variation in \\(Y_t\\). In other words, it is a random or stochastic component.\n\nFigure 1.4 illustrates an automatic decomposition, however, because the user has too little control of how the decomposition is done, this function is not recommended for use in your analysis. We will talk about the alternatives in the next lecture.\n\n\nCode\nbirths <- scan(\"http://robjhyndman.com/tsdldata/data/nybirths.dat\")\nbirths <- ts(births, frequency = 12, start = c(1946, 1))\nggplot2::autoplot(decompose(births))  + \n    theme_light()\n\n\n\n\n\nFigure 1.4: Time-series decomposition of monthly number of births in New York City (thousands), 1946–1959.\n\n\n\n\n\n\n1.1.2 General approach to time series modeling\n\nPlot the series and examine the main features of the graph, checking whether there is\n\na trend,\na seasonal or other periodic component,\nany apparent sharp changes in behavior,\nany outlying observations.\n\nRemove the trend and periodic components to get stationary residuals (this step is called detrending and deseasonalizing). Broadly speaking, a time series is said to be stationary if there is\n\nno systematic change in the mean (no trend);\nno systematic change in the variance, and\nno strictly periodic variations.\n\nChoose a model to fit the residuals, making use of various sample statistics, including the sample autocorrelation function (ACF).\nForecast the residuals and then invert the transformations to arrive at forecasts of the original series.\n\nThere are two general classes of forecasting models.\nUnivariate time series models include different types of exponential smoothing, trend models, autoregressive models, etc. The characteristic feature of these models is that we need only one time series to start with (\\(Y_t\\)), then we can build a regression of this time series over time (\\(Y_t \\sim t\\)) for estimating trend or, for example, an autoregressive model (‘auto’ \\(=\\) self). In autoregressive approach, the current value \\(Y_t\\) is modeled as a function of the past values: \\[\nY_t = f(Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-p}) + \\epsilon_t,\n\\tag{1.1}\\] while a linear autoregressive model has the form (assume that function \\(f(\\cdot)\\) is a linear parametric function, with parameters \\(\\phi_0, \\phi_1, \\ldots, \\phi_p\\)): \\[\nY_t = \\phi_0 + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots +\\phi_p Y_{t-p} + \\epsilon_t.\n\\]\nMultivariate models involve additional covariates (a.k.a. regressors, predictors, or independent variables) \\(X_{1,t}, X_{2,t}, \\ldots, X_{k,t}\\). A multivariate time series model can be as simple as a multiple regression \\[\nY_t = f(X_{1,t}, X_{2,t}, \\ldots, X_{k,t}) + \\epsilon_t,\n\\] or involve autoregressive components of the response (and predictors): \\[\n\\begin{split}\nY_t = f(&X_{1,t}, X_{2,t}, \\ldots, X_{k,t}, \\\\\n&X_{1,t-1}, X_{1,t-2}, \\ldots, X_{1,t-q1}, \\\\\n&\\cdots\\\\\n&X_{k,t-1}, X_{k,t-2}, \\ldots, X_{k,t-qk}, \\\\\n&Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-p}) + \\epsilon_t,\n\\end{split}\n\\tag{1.2}\\] where \\(p\\), \\(q1, \\ldots, qk\\) are the lags. You can start your analysis with many variables and build a forecasting model as complex as Equation 1.2, but remember that a simpler univariate model may also work well. You should create an appropriate univariate model (like in Equation 1.1) to serve as a baseline, then compare the models’ performance on some out-of-sample data, as described in Section 1.1.3.\n\n\n\n\n\n\nExample: Identify the time-series components in these plots\n\n\n\n\n\nCode\nggplot2::autoplot(sunspots) + \n    xlab(\"\") + \n    ylab(\"Sunspot number\") + \n    theme_light()\n\n\n\n\n\nFigure 1.5: Monthly mean relative sunspot numbers from 1749 to 1983. Collected at Swiss Federal Observatory, Zurich until 1960, then Tokyo Astronomical Observatory. Notice the apparent periodicity, large variability when the series is at a high level and small variability when the series is at a low level. This series is likely to have no trend over the time span we have been able to observe it, though it may have a nearly perfectly periodic component.\n\n\n\n\n\n\nCode\nggplot2::autoplot(lynx) + \n    xlab(\"\") + \n    ylab(\"Number of lynx trappings\") + \n    theme_light()\n\n\n\n\n\nFigure 1.6: Annual numbers of lynx trappings for 1821–1934 in Canada. There is a clear cycle of about 10 years in length (period is 10 years). There is potentially also a longer-term cycle.\n\n\n\n\n\n\nCode\nggplot2::autoplot(Nile) + \n    xlab(\"\") + \n    ylab(bquote('Annual flow  '(10^8~m^3))) + \n    theme_light()\n\n\n\n\n\nFigure 1.7: Annual flow of the river Nile at Aswan, 1871–1970. There are signs of nonlinear dynamics or a changepoint.\n\n\n\n\n\n\nCode\nggplot2::autoplot(co2) + \n    xlab(\"\") + \n    ylab(\"Concentration (ppm)\") + \n    theme_light()\n\n\n\n\n\nFigure 1.8: Monthly Mauna Loa atmospheric CO\\(_2\\) concentration, 1959–1997. There is a strong trend and a strong annual cycle.\n\n\n\n\n\n\nCode\nggplot2::autoplot(Ecdat::Consumption[,\"yd\"]/1000) + \n    xlab(\"\") + \n    ylab(\"Income (thousand 1986 dollars)\") + \n    theme_light()\n\n\n\n\n\nFigure 1.9: Quarterly personal disposable income (Canada, 1947–1996). There is a clear increasing trend of a relatively complex shape. Also, the variability increased in the later years.\n\n\n\n\n\n\nCode\nforecast::autoplot(as.ts(Ecdat::SP500[,1])) + \n    xlab(\"\") + \n    ylab(\"Earnings per share (USD)\") + \n    theme_light()\n\n\n\n\n\nFigure 1.10: Daily returns (change in log index) on Standard & Poor’s 500 Index, 1981-01 to 1991-04. Business days are used as the time index to avoid data gaps during non-trading days. The mean and variance are constant overall (nor increasing or decreasing), but there are clusters of volatility.\n\n\n\n\n\n\n\n\n1.1.3 Model comparison\nHow do we compare forecasting models to decide which one is better? We will look at various ways of choosing between models as the course progresses, but the most obvious answer is to see which one is better at predicting.\nSuppose we have used the data \\(Y_1, \\ldots, Y_n\\) to build \\(M\\) forecasting models \\(\\hat{Y}^{(m)}_t\\) (\\(m = 1,\\ldots,M\\)) and we now obtain future observation \\(Y_{n+1}, \\ldots, Y_{n+k}\\) that were not used to fit the models (also called out-of-sample data, or after-sample, or testing set; \\(k\\) is the size of this set). The difference \\(Y_t - \\hat{Y}^{(m)}_t\\) is the forecast (or prediction) error at time \\(t\\) for the \\(m\\)th model. For each model, compute the prediction mean square error (PMSE) \\[\nPMSE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left(Y_t - \\hat{Y}^{(m)}_t\\right)^2\n\\tag{1.3}\\] and prediction mean absolute error (PMAE) \\[\nPMAE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left|Y_t - \\hat{Y}^{(m)}_t\\right|\n\\tag{1.4}\\] and, similarly, prediction root mean square error (PRMSE; \\(PRMSE = \\sqrt{PMSE}\\)), prediction mean absolute percentage error (PMAPE, if \\(Y_t \\neq 0\\) in the testing period), etc. We choose the model with the smallest error.\nOne obvious drawback to the above method is that it requires you to wait for future observations in order to compare models. A way around this to take the historical dataset \\(Y_1, \\ldots, Y_n\\) and split it into a training set \\(Y_1, \\ldots, Y_k\\) and a testing set \\(Y_{k+1}, \\ldots, Y_n\\), where \\((n - k)\\ll k\\), i.e., most of the data goes into the training set.\n\n\n\n\n\n\nNote\n\n\n\nThis scheme of splitting time series into the testing and training sets is a simple form of cross-validation. Not all forms of cross-validation are applicable to time series due to the usual temporal dependence in time series data. We need to select cross-validation techniques that can accommodate such dependence. Usually, it implies selecting data for validation not at random but in consecutive chunks (periods) and, ideally, with testing or validation periods being after the training period.\n\n\nForecasting models are then built using only the training set, and used to ‘forecast’ values from the testing set. Sometimes it is called an out-of-sample forecast, because we predict values for the times we have not used for the model specification and estimation. The testing set is used as a set of future observations to compute the PMSE. The PMSE and other errors are computed for each model over the testing set, and then compared to see errors for which models are smaller.\nIf two models produce approximately the same errors, we choose the model that is simpler (involves fewer variables). This is called the law of parsimony.\nThe above error measures (PMSE, PRMSE, PMAE, etc.) compare observed and forecasted data points, hence, are measures of accuracy of the point forecasts. Another way of comparing models could be based on the quality of their interval forecasts, i.e., by assessing how good the prediction intervals are. To assess quality of interval forecasts, one may start by computing the empirical coverage (proportion of data points in the testing set that are within – covered by – corresponding prediction intervals for given confidence \\(C\\), e.g., 95%) and average interval width. Prediction intervals are well-calibrated if empirical coverage is close to \\(C\\) (more important) while intervals are not too wide (less important).\n\n\n\n\n\n\nNote\n\n\n\nTo select the best coverage, one can calculate the absolute differences between the nominal coverage \\(C\\) and each empirical coverage \\(\\hat{C}_m\\): \\[\n\\Delta_m = |C - \\hat{C}_m|.\n\\] Hence, we select the model with the smallest \\(\\Delta_m\\), not the largest coverage \\(C_m\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to obtain different prediction intervals from the same model. For example, you can calculate prediction intervals based on normal and bootstrapped distributions. In this case, point forecasts are the same, but interval forecasts differ.\n\n\nWe may compare a great number of models using the training set, and choose the best one (with smallest errors), however, it would be unfair to use the out-of-sample errors from testing set for demonstrating the model performance because this part of sample was used to select the model. Thus, it is advisable to have one more chunk of the same time series that was not used for model specification, estimation, or selection. Errors of the selected model on this validation set will be closer to the true (genuine) out-of-sample errors and can be used to improve coverage of true out-of-sample forecasts when the model is finally deployed."
  },
  {
    "objectID": "l02_tsintro.html#some-simple-time-series-models",
    "href": "l02_tsintro.html#some-simple-time-series-models",
    "title": "1  Introduction to Time Series Analysis",
    "section": "1.2 Some simple time series models",
    "text": "1.2 Some simple time series models\nRecall that random process is a sequence of random variables, so its model would be the joint distribution of these random variables \\[\nf_1(Y_{t_1}), \\; f_2(Y_{t_1}, Y_{t_2}), \\; f_3(Y_{t_1}, Y_{t_2}, Y_{t_3}), \\ldots\n\\]\nWith sample data, usually we cannot estimate so many parameters. Instead, we use only first- and second-order moments of the joint distributions, i.e., \\(\\mathrm{E}(Y_t)\\) and \\(\\mathrm{E}(Y_tY_{t+h})\\). In the case when all the joint distributions are multivariate normal (MVN), these second-order properties completely describe the sequence (we do not need any other information beyond the first two moments in this case).\n\ni.i.d. noise – independent and identically distributed random variables with zero mean. There is no dependence between observations, at any moment. The joint distribution function is \\[\n\\Pr[Y_1\\leqslant y_1, \\ldots, Y_n\\leqslant y_n] = \\Pr[Y_1\\leqslant y_1]\\cdots \\Pr[Y_n\\leqslant y_n].\n\\] and the forecast is 0 (zero).\nbinary process is a case of i.i.d. process with \\(0 < p < 1\\), and \\[\n\\begin{split}\n\\Pr[X_t=1]& = p, \\\\\n\\Pr[X_t=-1]& = 1-p.\n\\end{split}\n\\]\nrandom walk is a cumulative sum of i.i.d. noise: \\[\nS_t=X_1+X_2+\\ldots +X_t = \\sum_{i=1}^t X_i,\n\\] where \\(t=1,2,\\ldots\\), and \\(X_t\\) is i.i.d. noise.\nwhite noise (WN) is a sequence of uncorrelated random variables, each with zero mean and finite variance \\(\\sigma^2\\). An i.i.d.(0,\\(\\sigma^2\\)) series is also WN(0,\\(\\sigma^2\\)), but not conversely."
  },
  {
    "objectID": "l02_tsintro.html#autocorrelation-function",
    "href": "l02_tsintro.html#autocorrelation-function",
    "title": "1  Introduction to Time Series Analysis",
    "section": "1.3 Autocorrelation function",
    "text": "1.3 Autocorrelation function\nIt is time to give more formal definitions of stationarity. Loosely speaking, a time series \\(X_t\\) (\\(t=0,\\pm 1, \\ldots\\)) is stationary if it has statistical properties similar to those of the time shifted series \\(X_{t+h}\\) for each integer \\(h\\).\nLet \\(X_t\\) be a time series with \\(\\mathrm{E}(X^2_t)<\\infty\\), then the mean function of \\(X_t\\) is \\[\n\\mu_X(t)=\\mathrm{E}(X_t).\n\\]\nThe autocovariance function of \\(X_t\\) is \\[\n\\gamma_X(r,s) = \\mathrm{cov}(X_r,X_s) = \\mathrm{E}[(X_r-\\mu_X(r))(X_s-\\mu_X(s))]\n\\tag{1.5}\\] for all integer \\(r\\) and \\(s\\).\nWeak stationarity\n\\(X_t\\) is (weakly) stationary if \\(\\mu_X(t)\\) is independent of \\(t\\), and \\(\\gamma_X(t+h,t)\\) is independent of \\(t\\) for each \\(h\\) (consider only the first two moments): \\[\n\\begin{split}\n\\mathrm{E}(X_t) &= \\mu, \\\\\n\\mathrm{cov}(X_t, X_{t+h}) &= \\gamma_X(h) < \\infty.\n\\end{split}\n\\tag{1.6}\\]\nStrong stationarity\n\\(X_t\\) is strictly stationary if (\\(X_1, \\ldots, X_n\\)) and (\\(X_{1+h}, \\ldots, X_{n+h}\\)) have the same joint distribution (consider all moments).\nStrictly stationary \\(X_t\\) with finite variance \\(\\mathrm{E}(X_t^2) <\\infty\\) for all \\(t\\) is also weakly stationary.\nIf \\(X_t\\) is a Gaussian process, then strict and weak stationarity are equivalent (i.e., one form of stationarity implies the other).\n\n\n\n\n\n\nNote\n\n\n\nIn applications, it is usually very difficult if not impossible to verify strict stationarity. So in the vast majority of cases we are satisfied with the weak stationarity. Moreover, we usually omit the word ‘weak’ and simply talk about stationarity (but have the weak stationary in mind).\n\n\nIn view of the second condition of weak stationarity in Equation 1.6, we can write for stationary(!) time series the autocovariance function (ACVF) for the lag \\(h\\): \\[\n\\gamma_X(h)= \\gamma_X(t+h,t) = \\mathrm{cov}(X_{t+h},X_t)\n\\] and the autocorrelation function (ACF), which is the normalized autocovariance: \\[\n\\rho_X(h)=\\frac{\\gamma_X(h)}{\\gamma_X(0)}=\\mathrm{cor}(X_{t+h},X_t).\n\\]\nThe sample autocovariance function is defined as: \\[\n\\hat{\\gamma}_X(h)= n^{-1}\\sum_{t=1}^{n-k}(x_{t+h}- \\bar{x})(x_t - \\bar{x}),\n\\] with \\(\\hat{\\gamma}_X(h) = \\hat{\\gamma}_X(-h)\\) for \\(h = 0,1,\\ldots, n-1\\). In R, use acf(X, type = \"covariance\").\nThe sample autocorrelation function is defined as (in R, use acf(X)): \\[\n\\hat{\\rho}_X(h)=\\frac{\\hat{\\gamma}_X(h)}{\\hat{\\gamma}_X(0)}.\n\\]\n\n1.3.1 Properties of autocovariance and autocorrelation functions\n\nLinearity: if \\(\\mathrm{E}(X^2) < \\infty\\), \\(\\mathrm{E}(Y^2) < \\infty\\), \\(\\mathrm{E}(Z^2) < \\infty\\) and \\(a\\), \\(b\\), and \\(c\\) are any real constants, then \\[\n\\mathrm{cov}(aX + bY + c, Z) = a\\mathrm{cov}(X,Z) + b\\mathrm{cov}(Y,Z).\n\\]\n\\(\\gamma(0) \\geqslant 0\\).\n\\(|\\gamma(h)| \\leqslant \\gamma(0)\\) for all \\(h\\).\n\\(\\gamma(\\cdot)\\) is even: \\(\\gamma(h) = \\gamma(-h)\\) for all \\(h\\).\n\\(\\gamma(\\cdot)\\) is non-negative definite: \\[\n\\sum_{i,j=1}^na_i \\gamma(i-j)a_j\\geqslant 0,\n\\] for all positive integers \\(n\\) and vectors \\(\\boldsymbol{a} = (a_1, \\ldots, a_n)^{\\top}\\) with real-valued elements \\(a_i\\).\nThe autocorrelation function \\(\\rho(\\cdot)\\) has all the properties of autocovariance function plus \\(\\rho(0)=1\\).\nFor i.i.d. noise with finite variance, the sample autocorrelations \\(\\hat{\\rho}(h)\\), \\(h>0\\), are approximately \\(N(0,1/n)\\) for large sample size \\(n\\). Hence, approximately 95% of the sample autocorrelations should fall between the bounds \\(\\pm1.96/\\sqrt{n}\\) (1.96 is the 0.975th quantile of the standard normal distribution) – this is the bound automatically drawn by the R function acf(). Note that in R as a rule of thumb the default maximal lag is \\(10 \\log_{10}(n)\\), and the same \\(n\\) is used for the confidence bounds at all lags (however, in reality, samples of different sizes are used for each lag).\n\n\n\n\n\n\n\nExample: Compare ACFs of time series with and without a strong trend\n\n\n\nThe time series plot of births in Figure 1.11 shows a trend. The time series is not stationary. Thus, the calculated ACF is useless (it is calculated under the assumption of stationarity), but we notice some periodicity in the ACF – it is a sign of periodicity in the time series itself (seasonality). Need to remove the trend (and seasonality) and repeat the ACF analysis.\nCompare with the plots for accidental deaths in Figure 1.12.\n\n\nCode\np1 <- ggplot2::autoplot(births) + \n    xlab(\"\") +\n    ylab(\"Number of births (thousands)\") +\n    theme_light()\np2 <- forecast::ggAcf(births) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 1.11: Time series plot and sample ACF of monthly number of births in New York City, 1946–1959.\n\n\n\n\n\n\nCode\np1 <- ggplot2::autoplot(MASS::accdeaths) + \n    xlab(\"\") +\n    ylab(\"Number of accidental deaths\") +\n    theme_light()\np2 <- forecast::ggAcf(MASS::accdeaths) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 1.12: Time series plot and sample ACF of monthly totals of accidental deaths in the USA, 1973–1978.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn base-R plots, the lags in ACF plots are measured in the number of periods. The period information (or frequency, i.e., the number of observation per period) is saved in the format ts in R:\n\n\nCode\nis.ts(births)\n\n\n#> [1] TRUE\n\n\nCode\nfrequency(births)\n\n\n#> [1] 12\n\n\nYou don’t have to convert data to this format. If you plot ACF of just a vector without these attributes, the values are the same, just the labels on the horizontal axis are different (Figure 1.13). For monthly data, as in the example here, frequency is 12. Hence, here ACF at lag 0.5 (Figure 1.13 A) means autocorrelation with the lag \\(h=6\\) months (Figure 1.13 B); lag 1 (Figure 1.13 A) corresponds to one whole period of \\(h=12\\) months (Figure 1.13 B), and so on.\n\n\nCode\npar(mar = c(4, 4, 3, 1) + 0.1, mfrow = c(1, 2))\npar(pty = \"m\")\n\nacf(births, lag.max = 37, las = 1, main = \"A) Input is a ts object\")\nacf(as.numeric(births), lag.max = 37, las = 1, main = \"B) Input is a numeric vector\")\n\n\n\n\n\nFigure 1.13: In base-R graphics, the x-axis labels in ACF plots differ depending on the input format.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe ts objects in R cannot incorporate varying periods (e.g., different number of days or weeks because of leap years), therefore, we recommend using this format only for monthly or annual data. Setting frequency = 365.25 for daily data could be an option (although not very accurate) to accommodate leap years.\nConverting to the ts format usually is not required for analysis. Most of the times we use plain numeric vectors and data frames in R.\nContributed R packages offer additional formats for time series, e.g., see packages xts and zoo."
  },
  {
    "objectID": "l02_tsintro.html#partial-autocorrelation-function",
    "href": "l02_tsintro.html#partial-autocorrelation-function",
    "title": "1  Introduction to Time Series Analysis",
    "section": "1.4 Partial autocorrelation function",
    "text": "1.4 Partial autocorrelation function\nShumway and Stoffer (2011) and Shumway and Stoffer (2014) provide the following explanation of the concept.\nRecall that if \\(X\\), \\(Y\\), and \\(Z\\) are random variables, then the partial correlation between \\(X\\) and \\(Y\\) given \\(Z\\) is obtained by regressing \\(X\\) on \\(Z\\) to obtain \\(\\hat{X}\\), regressing \\(Y\\) on \\(Z\\) to obtain \\(\\hat{Y}\\), and then calculating \\[\n\\rho_{XY|Z} = \\mathrm{cor}(X - \\hat{X}, Y - \\hat{Y}).\n\\] The idea is that \\(\\rho_{XY|Z}\\) measures the correlation between \\(X\\) and \\(Y\\) with the linear effect of \\(Z\\) removed (or partialled out). If the variables are multivariate normal, then this definition coincides with \\(\\rho_{XY|Z} = \\mathrm{cor}(X,Y | Z)\\).\nThe partial autocorrelation function (PACF) of a stationary process, \\(X_t\\), denoted \\(\\rho_{hh}\\), for \\(h = 1,2,\\ldots\\), is \\[\n\\rho_{11} = \\mathrm{cor}(X_{t+1}, X_t) = \\rho(1)\n\\] and \\[\n\\rho_{hh} = \\mathrm{cor}(X_{t+h} - \\hat{X}_{t+h}, X_t - \\hat{X}_t), \\; h\\geqslant 2.\n\\]\nBoth \\((X_{t+h} - \\hat{X}_{t+h})\\) and \\((X_t - \\hat{X}_t)\\) are uncorrelated with \\(\\{ X_{t+1}, \\ldots, X_{t+h-1}\\}\\). The PACF, \\(\\rho_{hh}\\), is the correlation between \\(X_{t+h}\\) and \\(X_t\\) with the linear dependence of everything between them, namely \\(\\{ X_{t+1}, \\ldots, X_{t+h-1}\\}\\), on each, removed.\nTo obtain sample estimates of PACF in R, use pacf(X) or acf(X, type = \"partial\").\nCorrelation and partial correlation coefficients measure strength and direction of the relationship, changing within \\([-1, 1]\\). Percent of explained variance for the case of two variables is measured by squared correlation (r-squared; \\(R^2\\); coefficient of determination) changing within \\([0, 1]\\), so correlation of 0.2 means only 4% of variance explained by the simple linear relationship (regression). To report a partial correlation, for example, of 0.2 at lag 3, one could say something like ‘The partial autocorrelation at lag 3 (after removing influence of the intermediate lags 1 and 2) is 0.2’ (depending on the application, the correlation of 0.2 can be considered weak or moderate strength) or ‘After accounting for autocorrelation at intermediate lags 1 and 2, the linear relationship at lag 3 can explain 4% of the remaining variability.’\n\n\n\n\n\n\nExample: ACF and PACF of accdeaths\n\n\n\nBy plotting ACF and PACF (Figure 1.14), we observe that most of the temporal dependence (autocorrelation) in this time series is due to correlation with the immediately preceding values (see PACF at lag 1).\n\n\nCode\np1 <- forecast::ggAcf(MASS::accdeaths) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np2 <- forecast::ggAcf(MASS::accdeaths, type = \"partial\") + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 1.14: ACF and PACF plots of the monthly accidental deaths."
  },
  {
    "objectID": "l02_tsintro.html#detrending",
    "href": "l02_tsintro.html#detrending",
    "title": "1  Introduction to Time Series Analysis",
    "section": "1.5 Detrending",
    "text": "1.5 Detrending\nHow do we get to stationary models when most of the time series have some sort of trends? We can detrend time series (i.e., eliminate the trend) using either\n\nMoving average smoothing\nExponential smoothing\nPolynomial fitting (analytical smoothing) to extract deterministic trend\nDifferencing: eliminate the (stochastic) trend using consecutive differencing\n\nVery rarely, a time series may contain both deterministic and stochastic trends that need to be modeled or removed for further analysis."
  },
  {
    "objectID": "l02_tsintro.html#conclusion",
    "href": "l02_tsintro.html#conclusion",
    "title": "1  Introduction to Time Series Analysis",
    "section": "1.6 Conclusion",
    "text": "1.6 Conclusion\nWe have defined components of the time series including trend, periodic component, and unexplained variability (errors, residuals). Our goal will be to model trends and periodicity, extract them from the time series, then extract as much information as possible from the remainder, so the ultimate residuals become white noise.\nWhite noise is a sequence of uncorrelated random variables with zero mean and finite variance. White noise is also an example of a weakly stationary time series. The i.i.d. noise is strictly stationary (all moments of the distribution stay identical through time), and i.i.d. noise with finite variance is also white noise.\nTime-series dependence can be quantified using a (partial) autocorrelation function. We defined (P)ACF for stationary series; R functions also assume stationarity of the time series when calculating ACF or PACF.\nAfter developing several models for modeling and forecasting time series, we can compare them quantitatively in cross-validation. For time series, it is typical to have the testing set (or a validation fold) to be after the training set. Models can be compared based on the accuracy of their point forecasts and interval forecasts.\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton\n\n\nShumway RH, Stoffer DS (2011) Time series analysis and its applications with r examples, 3rd edn. Springer, New York\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications with r examples, 3-EZ. Free Texts in Statistics"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "The R packages used in this book include (in alphabetic order):\n\ncar (Fox et al. 2021)\ndplyr (Wickham et al. 2022b)\nfma (Hyndman 2020)\nforecast (Hyndman et al. 2022) \nggplot2 (Wickham et al. 2022a)\nlawstat (Gastwirth et al. 2020)\nlmtest (Hothorn et al. 2022)\npatchwork (Pedersen 2020)\npracma (Borchers 2021)\nrandtests (Caeiro and Mateus 2014)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBorchers HW (2021) Pracma: Practical numerical math functions. R package version 2.3.6, https://CRAN.R-project.org/package=pracma\n\n\nCaeiro F, Mateus A (2014) Randtests: Testing randomness in r. R package version 1.0, https://CRAN.R-project.org/package=randtests\n\n\nFox J, Weisberg S, Price B (2021) Car: Companion to applied regression. R package version 3.0-12, https://CRAN.R-project.org/package=car\n\n\nGastwirth JL, Gel YR, Hui WLW, et al (2020) Lawstat: Tools for biostatistics, public policy, and law. R package version 3.4, https://CRAN.R-project.org/package=lawstat\n\n\nHothorn T, Zeileis A, Farebrother RW, Cummins C (2022) Lmtest: Testing linear regression models. R package version 0.9-40, https://CRAN.R-project.org/package=lmtest\n\n\nHyndman R (2020) Fma: Data sets from \"forecasting: Methods and applications\" by makridakis, wheelwright & hyndman (1998). R package version 2.4, https://CRAN.R-project.org/package=fma\n\n\nHyndman R, Athanasopoulos G, Bergmeir C, et al (2022) Forecast: Forecasting functions for time series and linear models. R package version 8.16, https://CRAN.R-project.org/package=forecast\n\n\nPedersen TL (2020) Patchwork: The composer of plots. R package version 1.1.1, https://CRAN.R-project.org/package=patchwork\n\n\nWickham H, Chang W, Henry L, et al (2022a) ggplot2: Create elegant data visualisations using the grammar of graphics. R package version 3.3.6, https://CRAN.R-project.org/package=ggplot2\n\n\nWickham H, François R, Henry L, Müller K (2022b) Dplyr: A grammar of data manipulation. R package version 1.0.8, https://CRAN.R-project.org/package=dplyr"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Borchers HW (2021) Pracma: Practical numerical math functions. R package\nversion 2.3.6, https://CRAN.R-project.org/package=pracma\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and\nforecasting, 2nd edn. Springer, New York\n\n\nCaeiro F, Mateus A (2014) Randtests: Testing randomness in r. R package\nversion 1.0, https://CRAN.R-project.org/package=randtests\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton\n\n\nFox J, Weisberg S, Price B (2021) Car: Companion to applied regression.\nR package version 3.0-12, https://CRAN.R-project.org/package=car\n\n\nGastwirth JL, Gel YR, Hui WLW, et al (2020) Lawstat: Tools for\nbiostatistics, public policy, and law. R package version 3.4, https://CRAN.R-project.org/package=lawstat\n\n\nHothorn T, Zeileis A, Farebrother RW, Cummins C (2022) Lmtest: Testing\nlinear regression models. R package version 0.9-40, https://CRAN.R-project.org/package=lmtest\n\n\nHyndman R (2020) Fma: Data sets from \"forecasting: Methods and\napplications\" by makridakis, wheelwright & hyndman (1998). R package\nversion 2.4, https://CRAN.R-project.org/package=fma\n\n\nHyndman R, Athanasopoulos G, Bergmeir C, et al (2022) Forecast:\nForecasting functions for time series and linear models. R package\nversion 8.16, https://CRAN.R-project.org/package=forecast\n\n\nPedersen TL (2020) Patchwork: The composer of plots. R package version\n1.1.1, https://CRAN.R-project.org/package=patchwork\n\n\nShumway RH, Stoffer DS (2011) Time series analysis\nand its applications with r examples, 3rd edn. Springer, New York\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications\nwith r examples, 3-EZ. Free Texts in Statistics\n\n\nWickham H, Chang W, Henry L, et al (2022a) ggplot2: Create elegant data\nvisualisations using the grammar of graphics. R package version 3.3.6,\nhttps://CRAN.R-project.org/package=ggplot2\n\n\nWickham H, François R, Henry L, Müller K (2022b) Dplyr: A grammar of\ndata manipulation. R package version 1.0.8, https://CRAN.R-project.org/package=dplyr"
  }
]
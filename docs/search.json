[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Code\nlibrary(lmtest)\nlibrary(randtests)\n# TS intro\nlibrary(Ecdat)\n# Smoothing\n# library(fma)\n# library(forecast)\nlibrary(pracma)\n# ARMA\nlibrary(funtimes)\nlibrary(forecast)\n# Reg1\nlibrary(tseries)\nlibrary(dynlm)\nlibrary(urca)\nlibrary(dplyr)\nlibrary(ecm)\n# Reg2\nlibrary(forecast)\n# plotting\nlibrary(ggplot2)\nlibrary(plotly)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Code\nx= 1 + 1\n\n\nhere x is 2\nTry new edits"
  },
  {
    "objectID": "l03_smoothing.html",
    "href": "l03_smoothing.html",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "",
    "text": "The goal of this lecture is to learn a variety of methods used for trend visualization (such that make a trend in noisy data more apparent), trend modeling (such that can help us to come up with ‘an equation’ for the trend, which could be further studied or used for interpolation, forecasting, or detrending), and detrending (just to remove trend from the series, with or without modeling it). You should be able to choose a method based on the goals of your analysis. We will start with simple and widely used methods that can be further combined into more elaborate algorithms.\nObjectives\nReading materials"
  },
  {
    "objectID": "l03_smoothing.html#introduction",
    "href": "l03_smoothing.html#introduction",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nRecall the classical decomposition \\[\nY_t = M_t + S_t + \\epsilon_t,\n\\tag{2.1}\\] where \\(M_t\\) is slowly changing function (trend component), \\(\\epsilon_t\\) is stationary random noise component, and \\(S_t\\) is the periodic term with period \\(m\\geqslant 2\\) (seasonal component) and scaling factors \\(\\lambda_k > 0\\) such that \\(S_{t+km} = \\lambda_kS_t\\) for \\(1 \\leqslant t \\leqslant m\\) and each \\(k \\geqslant 1\\). For identification, we need \\(\\sum_{t=1}^m S_t = 0\\) and \\(\\mathrm{E}(\\epsilon_t)=0\\).\nOur goal is to estimate and extract \\(M_t\\) and \\(S_t\\) with a hope that the residual or noise component \\(\\epsilon_t\\) will turn out to be a stationary time series (Section 2.2–Section 2.5). Alternatively, Box–Jenkins models use difference operators repeatedly to the series \\(Y_t\\) until a stationary time series is obtained (see Section 2.6 on differencing).\nThe seasonal component is said to have constant seasonal variation if the scaling factors satisfy \\(\\lambda_{k} = 1\\) for all \\(k\\). In other words, \\(S_{t+m} = S_{t}\\) for all \\(t\\). The constant variation is an assumption of most modeling techniques we will be using in this course. Unfortunately, many real seasonal time series do not have this constancy property and it is necessary to first perform a variance-stabilizing transformation to modify the original time series into one with constant variation. To some extent this is a matter of trial-and-error, where we work from ‘weaker’ transformations to ‘stronger’ transformations as needed. Typically, power transformations are used \\(Y_{t} \\rightarrow Y^{\\lambda}_{t}\\), where \\(0 < \\lambda < 1\\) or, log or even log log transformations, e.g., \\(Y_{t} \\rightarrow \\log Y_{t}\\).\nThe log transformation of \\(Y_t\\) is convenient when we assume not an additive model as Equation 2.1 is, but a multiplicative model as \\[\nY_t = M_t \\cdot S_t \\cdot \\epsilon_t.\n\\tag{2.2}\\] When applying variance-stabilizing log transformation to Equation 2.2, we get an additive result: \\[\n\\log Y_t = \\log M_t + \\log S_t + \\log \\epsilon_t,\n\\tag{2.3}\\] which is now similar to Equation 2.1. We will refer to Equation 2.1 as additive seasonality, and to Equation 2.2 as multiplicative seasonality. Only few methods can deal with the multiplicative case Equation 2.2 directly; most methods we consider in this course will require us to apply a transformation as in Equation 2.3.\nIn the following sections, we consider methods for estimation and elimination of \\(M_t\\) (for non-seasonal data, when \\(S_t = 0\\) in the additive case Equation 2.1 or \\(S_t = 1\\) in the multiplicative case Equation 2.2) and of both \\(M_t\\) and \\(S_t\\) (for seasonal data)."
  },
  {
    "objectID": "l03_smoothing.html#sec-movavg",
    "href": "l03_smoothing.html#sec-movavg",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.2 Finite moving average smoothing",
    "text": "2.2 Finite moving average smoothing\nNon-seasonal model with trend: \\[\nY_t = M_t + \\epsilon_t,\n\\] where \\(\\mathrm{E}(\\epsilon_t)=0\\) and \\(t=1,\\ldots,n\\).\nLet \\(q\\) be a non-negative integer (\\(q \\in \\mathbb{N}^{+}\\)) and consider the two-sided moving average of the series \\(Y_t\\): \\[\n\\begin{split}\nW_t &= \\frac{1}{2q+1}\\sum_{j=-q}^q Y_{t-j}\\\\\n&= \\frac{1}{2q+1}\\sum_{j=-q}^q M_{t-j} + \\frac{1}{2q+1}\\sum_{j=-q}^q \\epsilon_{t-j} \\\\\n&\\approx M_t,\n\\end{split}\n\\tag{2.4}\\] where \\(w = 2q+1\\) is size of the moving window. Note that the above approximation is correct if the average value of \\(\\epsilon_{t}\\) within each window is close to 0 (important for selecting \\(q\\)).\n\n\n\n\n\n\nExample: Shampoo plots\n\n\n\nFigure 2.1 shows a time series plot of monthly shampoo sales. While the data are monthly, the seasonal component is not visible:\n\nno strong periodicity in the time series plot;\nthe ACF at seasonal lag (1 year) is not significant.\n\nTherefore, we apply Equation 2.4 as to non-seasonal data.\n\n\nCode\n# Get the data from the package fma\nshampoo <- fma::shampoo \nshampoo\n\n\n#>     Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n#> 1 266.0 145.9 183.1 119.3 180.3 168.5 231.8 224.5 192.8 122.9 336.5 185.9\n#> 2 194.3 149.5 210.1 273.3 191.4 287.0 226.0 303.6 289.9 421.6 264.5 342.3\n#> 3 339.7 440.4 315.9 439.3 401.3 437.4 575.5 407.6 682.0 475.3 581.3 646.9\n\n\n\n\nCode\npshampoo <- autoplot(shampoo, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Sales\") + \n    theme_light()\np2 <- forecast::ggAcf(shampoo) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\npshampoo + p2 +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.1: Monthly shampoo sales over three years and a corresponding sample ACF.\n\n\n\n\n\n\nThere are multiple ways to calculate moving averages in R, and formulas for \\(W_t\\) may vary. If using someone’s else functions, remember to read the help files or source code to find out the exact formula in place of Equation 2.4. Below are some examples. Note that the package forecast is now retired in favor of the package fable.\n\n\n\n\n\n\nNote\n\n\n\nThe package forecast is now retired in favor of the package fable.\n\n\nFor example, forecast::ma() has the default option centre = TRUE, and the order argument represents the window size. Odd order and centre = TRUE correspond exactly to our definitions in Equation 2.4:\n\n\nCode\nforecast::ma(shampoo, order = 5)\n\n\n#>      Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    Nov\n#> 1     NA     NA 178.92 159.42 176.60 184.88 199.58 188.10 221.70 212.52 206.48\n#> 2 215.26 202.62 203.72 222.26 237.56 256.26 259.58 305.62 301.12 324.38 331.60\n#> 3 340.56 375.52 387.32 406.86 433.88 452.22 500.76 515.56 544.34 558.62     NA\n#>      Dec\n#> 1 197.82\n#> 2 361.70\n#> 3     NA\n\n\nFor example, pracma::movavg() averages last n data points:\n\n\nCode\npracma::movavg(shampoo, n = 5)\n\n\n#>  [1] 266.0000 205.9500 198.3333 178.5750 178.9200 159.4200 176.6000 184.8800\n#>  [9] 199.5800 188.1000 221.7000 212.5200 206.4800 197.8200 215.2600 202.6200\n#> [17] 203.7200 222.2600 237.5600 256.2600 259.5800 305.6200 301.1200 324.3800\n#> [25] 331.6000 361.7000 340.5600 375.5200 387.3200 406.8600 433.8800 452.2200\n#> [33] 500.7600 515.5600 544.3400 558.6200\n\n\nThe base-R function stats::filter() can also be used. Its odd window size and sides = 2 correspond to our definitions in Equation 2.4:\n\n\nCode\nwindow_size <- 5\nstats::filter(shampoo, filter = 1/rep(window_size, window_size), sides = 2)\n\n\n#>      Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    Nov\n#> 1     NA     NA 178.92 159.42 176.60 184.88 199.58 188.10 221.70 212.52 206.48\n#> 2 215.26 202.62 203.72 222.26 237.56 256.26 259.58 305.62 301.12 324.38 331.60\n#> 3 340.56 375.52 387.32 406.86 433.88 452.22 500.76 515.56 544.34 558.62     NA\n#>      Dec\n#> 1 197.82\n#> 2 361.70\n#> 3     NA\n\n\nThe sides = 1 in stats::filter() corresponds to the pracma::movavg() results above:\n\n\nCode\nstats::filter(shampoo, filter = 1/rep(window_size, window_size), sides = 1)\n\n\n#>      Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    Nov\n#> 1     NA     NA     NA     NA 178.92 159.42 176.60 184.88 199.58 188.10 221.70\n#> 2 206.48 197.82 215.26 202.62 203.72 222.26 237.56 256.26 259.58 305.62 301.12\n#> 3 331.60 361.70 340.56 375.52 387.32 406.86 433.88 452.22 500.76 515.56 544.34\n#>      Dec\n#> 1 212.52\n#> 2 324.38\n#> 3 558.62\n\n\nSee Figure 2.2 and Figure 2.3 for the effects of the window size.\n\n\n\n\n\n\nExample: Shampoo moving average smoothing\n\n\n\n\n\nCode\nps <- lapply(c(1, 2, 3, 5), function(q){\n    w <- 2 * q + 1\n    Wt <- forecast::ma(shampoo, order = w)\n    pshampoo + \n        geom_line(aes(y = Wt), col = 4, lwd = 1.5) + \n        ggtitle(paste0(\"q = \", q, \" (window = \", w, \")\"))\n})\nwrap_plots(ps, ncol = 2) +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.2: Shampoo sales over three years smoothed with centered moving average filters, where the window size is \\(w = 2q+1\\).\n\n\n\n\n\n\nCode\nps <- lapply(c(1, 2, 3, 5), function(q){\n    w <- 2 * q + 1\n    Wt <- stats::filter(shampoo, filter = 1/rep(w, w), sides = 1)\n    pshampoo + \n        geom_line(aes(y = Wt), col = 4, lwd = 1.5) + \n        ggtitle(paste0(\"q = \", q, \" (window = \", w, \")\"))\n})\nwrap_plots(ps, ncol = 2) +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.3: Shampoo sales over three years smoothed with non-centered moving average filters, where the window size is \\(w = 2q+1\\).\n\n\n\n\n\n\n\n2.2.1 Moving average smoothing for seasonal data\nCome back to the trend-seasonal model Equation 2.1: \\[\nY_t = M_t + S_t + \\epsilon_t.\n\\]\nWe apply the following steps for estimating seasonality and trend:\n\nApply a moving average filter specially chosen to eliminate the seasonal component and dampen the noise – get \\(\\hat{M}_t\\). Remember that sum of \\(S_t\\) within each period is 0. Hence, to smooth out the seasonal component (and noise) and get an estimate of just \\(M_t\\), we will use a moving window \\(w\\) sized as the seasonal period \\(m\\) (more generally, \\(w = km\\), where \\(k\\in \\mathbb{N}^{+}\\), if a greater smoothness is desired). It will often lead to the window size \\(w\\) being even. For even \\(w\\), Equation 2.4 is modified as follows (we now use \\(km + 1\\) elements so the window is still centered, but the elements on the ends receive half the weight): \\[\n\\hat{M}_t = \\frac{0.5Y_{t-km/2} + Y_{t-km/2 + 1} +\\ldots+Y_{t+km/2-1} + 0.5Y_{t+km/2}}{km}.\n\\tag{2.5}\\]\nUse \\(Y_t - \\hat{M}_t\\) to estimate the seasonal component – get \\(\\hat{S}_t\\). If needed, correct the estimates to sum up to 0 in each period. Let \\(\\hat{S}^*_t\\) be the corrected values.\nUse deseasonalized data \\(Y_t - \\hat{S}^*_t\\) to re-estimate the trend. Let \\(\\hat{M}^*_t\\) be the corrected trend estimate.\nThe estimated random noise is then: \\(\\hat{\\epsilon}_t = Y_t - \\hat{M}^*_t - \\hat{S}^*_t\\).\n\n\n\n\n\n\n\nNote\n\n\n\nFor the multiplicative case, replace subtractions with divisions, let the product of \\(\\hat{S}^*_t\\) be 1 within each seasonal period.\n\n\nThe above algorithm is implemented in stats::decompose(), but its main disadvantage is in the centered moving average filter used to estimate \\(M_t\\) that does not produce smoothed values for the most recent observations. We can elaborate this algorithm, first, by using different estimators for \\(M_t\\); second, by also replacing estimators for \\(S_t\\) (e.g., see the next section).\n\n\n\n\n\n\nExample: Secchi automatic (simple moving average-based) decomposition\n\n\n\nConsider monthly data of Secchi disk depth measured at the station CB1.1 in Chesapeake Bay (Figure 2.4), with the decomposition in Figure 2.5.\n\n\nCode\nload(\"./data/Secchi_CB1.1.RData\")\npSecchi <- autoplot(Secchi, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Secchi depth (m)\") + \n    theme_light()\np2 <- forecast::ggAcf(Secchi) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\npSecchi + p2 +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.4: Monthly average Secchi disk depth at station CB1.1 and its ACF.\n\n\n\n\n\n\nCode\nautoplot(stats::decompose(Secchi))  + \n    theme_light()\n\n\n\n\n\nFigure 2.5: Trend-seasonal decomposition of monthly average Secchi disk depth at station CB1.1."
  },
  {
    "objectID": "l03_smoothing.html#lowess-smoothing",
    "href": "l03_smoothing.html#lowess-smoothing",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.3 Lowess smoothing",
    "text": "2.3 Lowess smoothing\nOne of the alternatives for estimating trends \\(M_t\\) is locally weighted regression (shortened as ‘lowess’ or ‘loess’) (Cleveland 1979). We can apply lowess as a regression of \\(Y_t\\) on time, for example, using the algorithm outlined by Berk (2016):\n\nChoose the smoothing parameter such as bandwidth, \\(f\\), which is a proportion between 0 and 1.\nChoose a point \\(t_0\\) and its \\(w = f n\\) nearest points on the time axis.\nFor these \\(w\\) nearest neighbor points, compute a weighted least squares regression line for \\(Y_t\\) on \\(t\\). The coefficients \\(\\boldsymbol{\\beta}\\) of such regression are estimated by minimizing the residual sum of squares \\[\n\\text{RSS}^*(\\boldsymbol{\\beta}) = (\\boldsymbol{Y}^* - \\boldsymbol{X}^* \\boldsymbol{\\beta})^{\\top} \\boldsymbol{W}^* (\\boldsymbol{Y}^* - \\boldsymbol{X}^* \\boldsymbol{\\beta}),\n\\] where the asterisk indicates that only the observations in the window are included. The regressor matrix \\(\\boldsymbol{X}^*\\) can contain polynomial terms of time, \\(t\\). The \\(\\boldsymbol{W}^*\\) is a diagonal matrix conforming to \\(\\boldsymbol{X}^*\\), with diagonal elements being a function of distance from \\(t_0\\) (observations closer to \\(t_0\\) receive higher weights).\nCalculate the fitted value \\(\\tilde{Y}_t\\) for that single \\(t_0\\).\nRepeat Steps 2–4 for each \\(t_0 = 1,\\ldots,n\\).\n\nBy selecting large bandwidth \\(f\\) in the lowess algorithm, we can obtain greater smoothing (see Figure 2.6).\n\n\nCode\ndata(WWWusage)\nps <- lapply(c(0.1, 0.3, 0.5, 0.75), function(w){\n    autoplot(WWWusage, col = \"grey50\")  + \n        ggtitle(paste0(\"span = \", w)) +\n        theme_light() + \n        geom_smooth(method = \"loess\", span = w, se = FALSE)\n})\nwrap_plots(ps, ncol = 2) +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.6: A lowess illustration, adapted from Berk (2016).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function ggplot2::geom_smooth() has the default setting se = TRUE that produces a confidence interval around the smooth. In time series applications, autocorrelated residuals may lead to underestimated standard errors and incorrect (typically, too narrow) confidence intervals. Hence, we set se = FALSE. For testing significance of the identified trends, see lectures on trend testing (detection).\n\n\n\n2.3.1 Lowess smoothing for seasonal data\nNow use the Secchi time series (Figure 2.4) and implement the procedure of detrending and deseasonalizing from Section 2.2.1, but replace the simple moving average estimates of \\(M_t\\) with lowess estimates.\n\n\n\n\n\n\nExample: Secchi lowess-based decomposition\n\n\n\n\n\nCode\nYt <- Secchi\nt <- as.vector(time(Yt))\nmonth <- as.factor(cycle(Yt))\n# 1. Initial trend estimate\nMt <- loess(Yt ~ t, span = 0.25)$fitted\n# 2. Estimate of a cycle, corrected to sum up to 0\nSt <- tapply(Yt - Mt, month, mean)\nSt\n\n\n#>           1           2           3           4           5           6 \n#> -0.03593763  0.04567476 -0.24831439 -0.26834156 -0.07164770 -0.01293677 \n#>           7           8           9          10          11          12 \n#>  0.14110808  0.20244954  0.12762356  0.11081730  0.03912786 -0.01649349\n\n\nCode\nsum(St)\n\n\n#> [1] 0.01312957\n\n\nCode\nSt_star <- St - sum(St)/12\nsum(St_star)\n\n\n#> [1] 3.469447e-18\n\n\nCode\n# 3. Refine trend estimate\nMt_star <- loess((Yt - St_star[month]) ~ t, span = 0.25)$fitted\n# 4. Noise\net <- Yt - Mt_star - St_star[month]\n# Convert back to ts format for plotting\net <- ts(as.vector(et), start = start(Secchi), frequency = 12)\n\n\nSee Figure 2.7 for the results.\n\n\nCode\np1 <- pSecchi + \n    geom_line(aes(y = Mt_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 <- pSecchi + \n    geom_line(aes(y = Mt_star + St_star[month]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 <- autoplot(et) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\") +\n    theme_light()\np4 <- forecast::ggAcf(et) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") +\n    theme_light()\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.7: Detrending using lowess and deseasonalizing the Secchi depth data.\n\n\n\n\nIn Figure 2.8, we use the decomposition function stl() forced to have approximately the same values as we used in the steps above and as shown in Figure 2.7. Hence, Figure 2.7 and Figure 2.8 are similar. However, the function stl() is more flexible – it also automatically smooths the seasonal component (across Januaries, across Februaries, etc.) – and can provide finer estimates (see Figure 2.9).\n\n\nCode\n# Span we used above (the fraction)\nspan = 0.25 \n# Window size in number of lags (observations)\nw <- span * length(Secchi)/2 \nD <- stl(Yt, s.window = \"periodic\", t.window = w)\np1 <- pSecchi +\n    geom_line(aes(y = D$time.series[,\"trend\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 <- pSecchi + \n    geom_line(aes(y = D$time.series[,\"trend\"] + D$time.series[,\"seasonal\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 <- autoplot(D$time.series[,\"remainder\"]) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\") +\n    theme_light()\np4 <- forecast::ggAcf(D$time.series[,\"remainder\"]) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") +\n    theme_light()\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.8: Detrending and deseasonalizing the Secchi depth data using lowess within the function stl() forced to behave similarly to Figure 2.7.\n\n\n\n\n\n\nCode\nD <- stl(Yt, s.window = 24, s.degree = 1, t.window = w)\np1 <- pSecchi +\n    geom_line(aes(y = D$time.series[,\"trend\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 <- pSecchi + \n    geom_line(aes(y = D$time.series[,\"trend\"] + D$time.series[,\"seasonal\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 <- autoplot(D$time.series[,\"remainder\"]) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\") +\n    theme_light()\np4 <- forecast::ggAcf(D$time.series[,\"remainder\"]) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") +\n    theme_light()\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.9: Detrending and deseasonalizing the Secchi depth data using lowess within the function stl() with default settings."
  },
  {
    "objectID": "l03_smoothing.html#exponential-smoothing",
    "href": "l03_smoothing.html#exponential-smoothing",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.4 Exponential smoothing",
    "text": "2.4 Exponential smoothing\nExponential smoothing (ES) is a successful forecasting technique. It turns out that ES can be modified to be used effectively for time series with\n\nslowly drifting trend (double exponential smoothing);\ntrends (Holt’s method);\nseasonal patterns;\ncombination of trend and seasonality (Holt–Winters method).\n\nES is easy to adjust for past errors and easy to prepare follow-on forecasts. ES is ideal for situations where many forecasts must be prepared. Several different functional forms of ES are used depending on presence of trend or cyclical variations.\nIn short, an ES is an averaging technique that uses unequal weights while the weights applied to past observations decline in an exponential manner.\nSingle exponential smoothing calculates the smoothed series as a damping coefficient \\(\\alpha\\) (\\(\\alpha \\in [0, 1]\\)) times the actual series plus \\(1 - \\alpha\\) times the lagged value of the smoothed series. For the model \\[\nY_{t} = M_t + \\epsilon_{t},\n\\]\nthe updating equation is \\[\n\\begin{split}\n\\hat{M}_1 &= Y_1\\\\\n\\hat{M}_t &= \\alpha Y_t + (1 - \\alpha) \\hat{M}_{t - 1}\n\\end{split}\n\\]\nand forecast is \\[\n\\hat{Y}_{t+1} = \\hat{M}_t.\n\\]\nAn exponential smoothing over an already smoothed time series is called double exponential smoothing. In some cases, it might be necessary to extend it even to a triple exponential smoothing.\nHolt’s linear exponential smoothing Suppose that the series \\(Y_t\\) is non-seasonal but displays a trend. Now we need to estimate both the current mean (or level in some professional jargon) and the current trend.\nThe updating equations express ideas similar to those for exponential smoothing. However, now we have two smoothing parameters, \\(\\alpha\\) and \\(\\beta\\) (\\(\\alpha \\in [0, 1]\\); \\(\\beta \\in [0, 1]\\)).\nThe updating equations are \\[\na_{t} = \\alpha Y_{t} + \\left( 1- \\alpha \\right) \\left( a_{t - 1} + b_{t - 1} \\right)\n\\] for the mean and \\[\nb_{t} = \\beta \\left( a_{t} - a_{t-1} \\right) + \\left( 1 - \\beta \\right) b_{t-1}\n\\] for the trend.\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = a_{t} + kb_{t}.\n\\]\nUsually, the initial (starting) values are \\[\n\\begin{split}\na_{1} & = Y_{2}, \\\\\nb_{1} & = Y_{2} - Y_{1}.\n\\end{split}\n\\]\n\n\n\n\n\n\nExample: Shampoo exponential smoothing\n\n\n\nSee an example in Figure 2.10.\n\n\nCode\nclrs <- c(\"0.3\" = 4, \"0.7\" = 2)\nm1 <- HoltWinters(shampoo, alpha = 0.3, beta = FALSE, gamma = FALSE)\nm2 <- HoltWinters(shampoo, alpha = 0.7, beta = FALSE, gamma = FALSE)\np1 <- pshampoo + \n    geom_line(data = fitted(m1)[,1], aes(col = \"0.3\"), lwd = 1.5) + \n    geom_line(data = fitted(m2)[,1], aes(col = \"0.7\"), lwd = 1.5) +\n    ggtitle(\"Simple exponential smoothing\") +\n    labs(color = \"\\u03b1\") +\n    scale_color_manual(values = clrs)\nclrs <- c(\"(0.3, 0.3)\" = 4, \"(0.7, 0.7)\" = 2)\nm1 <- HoltWinters(shampoo, alpha = 0.3, beta = 0.3, gamma = FALSE)\nm2 <- HoltWinters(shampoo, alpha = 0.7, beta = 0.7, gamma = FALSE)\np2 <- pshampoo + \n    geom_line(data = fitted(m1)[,1], aes(col = \"(0.3, 0.3)\"), lwd = 1.5) + \n    geom_line(data = fitted(m2)[,1], aes(col = \"(0.7, 0.7)\"), lwd = 1.5) +\n    ggtitle(\"Holt's method\") +\n    labs(color = \"(\\u03b1, \\u03b2)\") +\n    scale_color_manual(values = clrs)\np1 + p2 +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.10: Shampoo sales over a three-year period smoothed with different exponential smoothing filters.\n\n\n\n\n\n\n\n2.4.1 Exponential smoothing for seasonal data\nMultiplicative Holt–Winters procedure\nNow in addition to the Holt parameters, suppose that the series exhibits multiplicative seasonality and let \\(S_{t}\\) be the multiplicative seasonal factor at time \\(t\\).\nSuppose also that there are \\(m\\) observations in one period (in a year). For example, \\(m = 4\\) for quarterly data and \\(m = 12\\) for monthly data.\nIn some time series, seasonal variation is so strong it obscures any trends or other cycles, which are important for understanding of the observed process. Holt–Winters smoothing method can remove seasonality and makes long-term fluctuations in the series stand out more clearly.\nA simple way of detecting trend in seasonal data is to take averages over a certain period. If these averages change with time we can say that there is evidence of a trend in the series.\nWe now use three smoothing parameters: \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) (\\(\\alpha \\in [0, 1]\\); \\(\\beta \\in [0, 1]\\); \\(\\gamma \\in [0, 1]\\)).\nThe updating equations for the level (\\(a_t\\)), local trend (\\(b_t\\)), and seasonal factor (\\(S_t\\)) are: \\[\n\\begin{split}\na_{t} & =  \\alpha Y_{t} / S_{t - m} + (1 - \\alpha) ( a_{t - 1} + b_{t - 1}), \\\\\nb_{t} & =  \\beta (a_{t} - a_{t - 1}) + (1 - \\beta) b_{t - 1}, \\\\\nS_{t} & =  \\gamma Y_{t} / a_{t} + (1 - \\gamma) S_{t - m}.\n\\end{split}\n\\]\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = (a_{t} + kb_{t}) S_{t+k-m},\n\\] where \\(k = 1, 2, \\ldots, m\\).\nTo obtain starting values, one may use an average over the first few periods (years) of the data.\nThe smoothing parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) are estimated by minimizing the sum of the squared one-step prediction errors.\nAdditive Holt–Winters procedure\nThe updating equations are \\[\n\\begin{split}\na_{t} & =  \\alpha (Y_{t} - S_{t - m})  + (1 - \\alpha) (a_{t - 1} + b_{t - 1}), \\\\\nb_{t} & =  \\beta (a_{t} - a_{t - 1}) + (1 - \\beta) b_{t - 1} \\\\\nS_{t} & =  \\gamma (Y_{t} - a_{t}) + (1 - \\gamma) S_{t - m}.\n\\end{split}\n\\]\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = a_{t} + kb_{t} + S_{t + k - m},\n\\] where \\(k = 1, 2, \\ldots, m\\).\n\n\n\n\n\n\nExample: Airline passengers non-seasonal and seasonal exponential smoothing\n\n\n\nMonthly totals of international airline passengers, 1949–1960 (see Figure 2.11). This is a classical example and benchmark in time series analysis.\n\n\nCode\npAirPassengers <- autoplot(AirPassengers, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers (thousand)\") + \n    theme_light()\np2 <- forecast::ggAcf(AirPassengers) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\npAirPassengers + p2 +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.11: Monthly AirPassengers data and a corresponding sample ACF.\n\n\n\n\nComparison of various exponential smoothing techniques for the AirPassengers data:\n\n\nCode\nm1 <- HoltWinters(AirPassengers, beta = FALSE, gamma = FALSE)\nm1$SSE\n\n\n#> [1] 162510.6\n\n\nCode\nc(m1$alpha, m1$beta, m1$gamma)\n\n\n#> [1] 0.9999339 0.0000000 0.0000000\n\n\nCode\nm2 <- HoltWinters(AirPassengers, gamma = FALSE)\nm2$SSE\n\n\n#> [1] 163634.1\n\n\nCode\nc(m2$alpha, m2$beta, m2$gamma)\n\n\n#>       alpha        beta             \n#> 1.000000000 0.003218516 0.000000000\n\n\nCode\nm3 <- HoltWinters(AirPassengers)\nm3$SSE\n\n\n#> [1] 21860.18\n\n\nCode\nc(m3$alpha, m3$beta, m3$gamma)\n\n\n#>      alpha       beta      gamma \n#> 0.24795949 0.03453373 1.00000000\n\n\nCode\nm4 <- HoltWinters(AirPassengers, seasonal = \"multiplicative\")\nm4$SSE\n\n\n#> [1] 16570.78\n\n\nCode\nc(m4$alpha, m4$beta, m4$gamma)\n\n\n#>      alpha       beta      gamma \n#> 0.27559247 0.03269295 0.87072922\n\n\nThe last multiplicative model is the best one, based on the sum of squared errors (SSE) in the training set (see Figure 2.12). For a more thorough comparison, consider using out-of-sample data in a cross-validation.\n\n\nCode\nk = 12\nfm1 <- predict(m1, n.ahead = k)\nfm2 <- predict(m2, n.ahead = k)\nfm3 <- predict(m3, n.ahead = k)\nfm4 <- predict(m4, n.ahead = k)\np1 <- pAirPassengers + \n    geom_line(data = m1$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm1) +\n    ggtitle(\"Exponential smoothing\") +\n    theme(legend.position = \"none\")\np2 <- pAirPassengers + \n    geom_line(data = m2$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm2) +\n    ggtitle(\"Holt's method\") +\n    theme(legend.position = \"none\")\np3 <- pAirPassengers + \n    geom_line(data = m3$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm3) +\n    ggtitle(\"Additive Holt-Winters\") +\n    theme(legend.position = \"none\")\np4 <- pAirPassengers + \n    geom_line(data = m4$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm4) +\n    ggtitle(\"Multiplicative Holt-Winters\") +\n    theme(legend.position = \"none\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.12: Plots of the observed, fitted, and predicted AirPassengers data, using various smoothing procedures."
  },
  {
    "objectID": "l03_smoothing.html#sec-regressionTime",
    "href": "l03_smoothing.html#sec-regressionTime",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.5 Polynomial regression on time",
    "text": "2.5 Polynomial regression on time\nThis is a very intuitive procedure for fitting parametric trend functions to time series:\n\nMake a parametric model assumption about \\(M_t\\) as a function of time \\(t\\), for example, quadratic trend: \\[\nM_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2\n\\]\nFit the model using the usual regression estimators (least squares or maximum likelihood)\n\n\n\n\n\n\n\nExample: Airline passengers non-seasonal polynomial smoothing\n\n\n\nContinue using the AirPassengers data and fit linear and quadratic trends.\n\n\nCode\nt <- as.vector(time(AirPassengers))\nt2 <- t^2\ntm1 <- lm(AirPassengers ~ t)\ntm2.1 <- lm(AirPassengers ~ t + t2)\ntm2.2 <- lm(AirPassengers ~ t + I(t^2))\ntm2.3 <- lm(AirPassengers ~ poly(t, degree = 2))\n\n\nAfter estimating the trend coefficients with OLS, visualize the results (Figure 2.13).\n\n\nCode\np1 <- pAirPassengers + \n    geom_line(aes(y = tm1$fitted.values), col = 4, lwd = 1.5) + \n    ggtitle(\"Linear trend\")\np2 <- pAirPassengers + \n    geom_line(aes(y = tm2.3$fitted.values), col = 4, lwd = 1.5) + \n    ggtitle(\"Quadratic trend\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.13: The plots of the AirPassengers data with estimated parametric linear and quadratic trends.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe models tm2.1 and tm2.2 are equivalent. Model tm2.1 uses a pre-calculated quadratic transformation, while model tm2.2 applies the transformation on-the-fly within the R formula call, using the I() syntax (without the I() wrapper, the output of tm2.2 would be the same as of tm1, not tm2.1). However, both models tm2.1 and tm2.2 can easily violate the assumption of independence of predictors in linear regression, especially when values of the time index \\(t\\) are large. That is the case for us, because \\(t\\) represents decimal years, from 1949 to 1960.9166667, and \\(\\widehat{\\mathrm{cor}}(t, t^2) =\\) 0.9999997. Therefore, model tm2.3 is preferred, because it is evaluated on orthogonal polynomials (basically, centering and normalization is applied to \\(t\\) and \\(t^2\\)).\n\n\n\n2.5.1 Seasonal regression with dummy variables\nTo each time \\(t\\) and each season \\(k\\) we will assign an indicator which ‘turns on’ if and only if \\(t\\) falls into that season. In principle, a seasonal period of any positive integer length \\(m \\geqslant 2\\) is possible, however, in most cases, the length of the seasonal cycle is one year and so the seasonal period of the time series depends on the number of observations per year. The most common periods are \\(m = 12\\) (monthly data) and \\(m = 4\\) (quarterly data). For each \\(k = 1, \\ldots, m\\), we define the indicator \\[\nX_{k,t} = \\left\\{\n\\begin{array}{cl}\n1, & \\mbox{if} ~  t ~ \\text{corresponds to season} ~ k, \\\\\n0, & \\mbox{if} ~  t ~ \\text{does not correspond to season} ~  k.  \\\\\n\\end{array}\n\\right.\n\\]\nNote that for each \\(t\\) we have \\(X_{1,t} + X_{2,t} + \\ldots + X_{m,t} = 1\\) since each \\(t\\) corresponds to exactly one season. Thus, given any \\(m - 1\\) of the variables, the remaining variable is known and thus redundant. Because of this linear dependence, we must drop one of the indicator variables (seasons) from the model. It does not matter which season is dropped although sometimes there are choices which are simpler from the point of view of constructing or labeling the design matrix. Thus the general form of a seasonal model looks like \\[\n\\begin{split}\nf (Y_{t}) &= M_{t} + \\beta_{2} X_{2,t} + \\beta_{3} X_{3,t} + \\ldots + \\beta_{m} X_{m,t} + \\epsilon_{t} \\\\\n&= M_{t} + \\sum_{i=2}^{m}\\beta_{i} X_{i,t} + \\epsilon_{t},\n\\end{split}\n\\] where \\(M_{t}\\) is a trend term which may also include some \\(\\beta\\) parameters and explanatory variables. The function \\(f\\) represents the appropriate variance-stabilizing transformations as needed. (Here the 1st season has been dropped from the model.)\n\n\n\n\n\n\nExample: Airline passengers polynomial smoothing with dummy variables for the seasons\n\n\n\nConsider adding the dummy variables to model seasonality additionally to the quadratic trend in the AirPassengers time series. We noticed multiplicative seasonality in this time series before, therefore we apply logarithmic transformation to the original data to transform the multiplicative seasonality into additive. That is, we need to estimate the model \\[\n\\ln(Y_{t}) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\sum_{i=2}^{12}\\beta_{i} X_{i,t} + \\epsilon_{t},\n\\] where \\(X_{i,t}\\) are the dummy variables for the months. (Here the first month has been dropped from the model, the same as in R the first factor level is dropped.)\nIf we were doing the regression analysis by hand, we would create a design matrix with few top rows looking like this:\n\n\n#>              t      t2 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12\n#>  [1,] 1949.000 3798601  0  0  0  0  0  0  0  0   0   0   0\n#>  [2,] 1949.083 3798926  1  0  0  0  0  0  0  0   0   0   0\n#>  [3,] 1949.167 3799251  0  1  0  0  0  0  0  0   0   0   0\n#>  [4,] 1949.250 3799576  0  0  1  0  0  0  0  0   0   0   0\n#>  [5,] 1949.333 3799900  0  0  0  1  0  0  0  0   0   0   0\n#>  [6,] 1949.417 3800225  0  0  0  0  1  0  0  0   0   0   0\n#>  [7,] 1949.500 3800550  0  0  0  0  0  1  0  0   0   0   0\n#>  [8,] 1949.583 3800875  0  0  0  0  0  0  1  0   0   0   0\n#>  [9,] 1949.667 3801200  0  0  0  0  0  0  0  1   0   0   0\n#> [10,] 1949.750 3801525  0  0  0  0  0  0  0  0   1   0   0\n#> [11,] 1949.833 3801850  0  0  0  0  0  0  0  0   0   1   0\n#> [12,] 1949.917 3802175  0  0  0  0  0  0  0  0   0   0   1\n#> [13,] 1950.000 3802500  0  0  0  0  0  0  0  0   0   0   0\n\n\nIn R, it is enough to have one variable representing the seasons. This variable should be saved as factor. Then we apply the OLS method to find the model coefficients (see results in Figure 2.14):\n\n\nCode\nMonth <- as.factor(cycle(AirPassengers))\nm <- lm(log(AirPassengers) ~ poly(t, degree = 2) + Month)\nsummary(m)\n\n\n#> \n#> Call:\n#> lm(formula = log(AirPassengers) ~ poly(t, degree = 2) + Month)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.12748 -0.03709  0.00418  0.03197  0.11529 \n#> \n#> Coefficients:\n#>                       Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)           5.457163   0.013924 391.912  < 2e-16 ***\n#> poly(t, degree = 2)1  5.022507   0.048367 103.841  < 2e-16 ***\n#> poly(t, degree = 2)2 -0.398373   0.048201  -8.265 1.41e-13 ***\n#> Month2               -0.022270   0.019678  -1.132 0.259839    \n#> Month3                0.107786   0.019679   5.477 2.15e-07 ***\n#> Month4                0.076388   0.019680   3.882 0.000164 ***\n#> Month5                0.073929   0.019682   3.756 0.000259 ***\n#> Month6                0.196033   0.019684   9.959  < 2e-16 ***\n#> Month7                0.299975   0.019686  15.238  < 2e-16 ***\n#> Month8                0.290723   0.019689  14.765  < 2e-16 ***\n#> Month9                0.146174   0.019693   7.423 1.33e-11 ***\n#> Month10               0.008145   0.019697   0.414 0.679912    \n#> Month11              -0.135401   0.019701  -6.873 2.36e-10 ***\n#> Month12              -0.021321   0.019706  -1.082 0.281286    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.0482 on 130 degrees of freedom\n#> Multiple R-squared:  0.9892, Adjusted R-squared:  0.9881 \n#> F-statistic: 912.7 on 13 and 130 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\np1 <- pAirPassengers + \n    geom_line(aes(y = exp(m$fitted.values)), col = 4, lwd = 1.5)\np2 <- forecast::ggAcf(m$residuals) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.14: Plot of the AirPassengers data, with estimated parametric quadratic trend and seasonality (modeled using dummy variables), and ACF of the residuals. Since the model was fitted on the log scale, the trend-cycle estimates need to be exponentiated to match the original scale of the data.\n\n\n\n\n\n\n\n\n2.5.2 Seasonal regression with Fourier series\nIt is a spoiler for the future lecture on spectral analysis of time series, but seasonal regression modeling would not be complete without introducing the trigonometric Fourier series.\nWe can fit a linear regression model using several pairs of trigonometric functions as predictors. For example, for monthly observations \\[\n\\begin{split}\ncos_{k} (i) &= \\cos (2 \\pi ki / 12), \\\\\nsin_{k} (i) &= \\sin (2 \\pi ki / 12),\n\\end{split}\n\\] where \\(i\\) is the month within the year, and the trigonometric function has \\(k\\) cycles per year.\n\n\n\n\n\n\nExample: Airline passengers polynomial smoothing with Fourier series for the seasons\n\n\n\nNow let us apply the method of sinusoids to the AirPassengers time series. We have one prominent and, possibly, one less prominent peak per year. Hence, we can test the model with \\(k = 1\\) and \\(k = 2\\). We will construct the following trigonometric predictors: \\[\n\\begin{split}\ncos_{1,t} &= \\cos(2 \\pi \\text{month}_t/12),\\\\\nsin_{1,t} &= \\sin(2 \\pi \\text{month}_t/12),\\\\\ncos_{2,t} &= \\cos(4 \\pi \\text{month}_t/12),\\\\\nsin_{2,t} &= \\sin(4 \\pi \\text{month}_t/12)\n\\end{split}\n\\] to use in the model \\[\n\\ln(Y_{t}) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\beta_{1}cos_{1,t} + \\beta_{2}sin_{1,t} + \\beta_{3}cos_{2,t} + \\beta_{4}sin_{2,t} + \\epsilon_{t}.\n\\]\nCalculate the predictors and estimate the model in R (see results in Figure 2.15):\n\n\nCode\nmonth <- as.numeric(cycle(AirPassengers))\ncos1 <- cos(2 * pi * month / 12)\nsin1 <- sin(2 * pi * month / 12)\ncos2 <- cos(4 * pi * month / 12)\nsin2 <- sin(4 * pi * month / 12)\nm <- lm(log(AirPassengers) ~ poly(t, degree = 2) + cos1 + sin1 + cos2 + sin2)\nsummary(m)\n\n\n#> \n#> Call:\n#> lm(formula = log(AirPassengers) ~ poly(t, degree = 2) + cos1 + \n#>     sin1 + cos2 + sin2)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.200784 -0.035391 -0.000559  0.040648  0.137714 \n#> \n#> Coefficients:\n#>                       Estimate Std. Error  t value Pr(>|t|)    \n#> (Intercept)           5.542176   0.004933 1123.463  < 2e-16 ***\n#> poly(t, degree = 2)1  5.029199   0.059360   84.723  < 2e-16 ***\n#> poly(t, degree = 2)2 -0.398191   0.059199   -6.726 4.34e-10 ***\n#> cos1                 -0.141521   0.006978  -20.282  < 2e-16 ***\n#> sin1                 -0.049228   0.006991   -7.042 8.35e-11 ***\n#> cos2                 -0.022762   0.006978   -3.262   0.0014 ** \n#> sin2                  0.078740   0.006980   11.282  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.0592 on 137 degrees of freedom\n#> Multiple R-squared:  0.9828, Adjusted R-squared:  0.982 \n#> F-statistic:  1303 on 6 and 137 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\np1 <- pAirPassengers + \n    geom_line(aes(y = exp(m$fitted.values)), col = 4, lwd = 1.5)\np2 <- forecast::ggAcf(m$residuals) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.15: Plot of the AirPassengers data, with estimated parametric quadratic trend and seasonality (modeled using trigonometric functions), and ACF of the residuals. Since the model was fitted on the log scale, the trend-cycle estimates need to be exponentiated to match the original scale of the data."
  },
  {
    "objectID": "l03_smoothing.html#sec-differencing",
    "href": "l03_smoothing.html#sec-differencing",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.6 Differencing",
    "text": "2.6 Differencing\nA big subset of time series models (particularly, models dealing with random walk series) applies differencing to eliminate trends.\nLet \\(\\Delta\\) be the difference operator (\\(\\nabla\\) notation is also used sometimes), then the simple first-order differences are: \\[\n\\begin{split}\n\\Delta Y_t &= Y_t - Y_{t-1} \\\\\n&=(1-B)Y_t,\n\\end{split}\n\\] where \\(B\\) is a backshift operator.\nBackshift operator and difference operator are useful for convenient representation of higher-order differences. We will often use backshift operators in future lectures: \\[\n\\begin{split}\nB^0Y_{t} &= Y_{t} \\\\\nBY_{t} &= Y_{t-1} \\\\\nB^{2}Y_{t} &= Y_{t-2} \\\\\n& \\vdots \\\\\nB^{k}Y_{t} &= Y_{t-k}.\n\\end{split}\n\\] The convenience is due to the fact that the powers of the operators can be treated as powers of elements of usual polynomials, so we can make different operations for more complex cases.\nFor example, if we took second-order differences, \\(\\Delta^2Y_t = (1-B)^2Y_t\\), to remove a trend and then used differences with the seasonal lag of 12 months to remove strong seasonal component, what would be the final form of transformed series? The transformed series, \\(Y^*_t\\), will be: \\[\n\\begin{split}\nY^*_t & = (1-B)^2 (1 - B^{12})Y_t \\\\\n& = (1 - 2B + B^2) (1 - B^{12})Y_t \\\\\n& = (1 - 2B + B^2 - B^{12} + 2B^{13} - B^{14})Y_t \\\\\n& = Y_t - 2Y_{t-1} + Y_{t-2} - Y_{t-12} + 2Y_{t-13}-Y_{t-14},\n\\end{split}\n\\] but we will often use just the top-row notations.\nWe will discuss formal tests to identify an appropriate order of differences in future lectures (unit-root tests), but for now use the rule of thumb: for time trends looking linear use 1st order differences (\\(\\Delta Y_t = Y_t - Y_{t-1}\\)), for parabolic shapes use differences of the 2nd order, etc. Apply differencing of higher orders until the time series looks stationary (plot the transformed series and its ACF at each step). In practice, differences of order 3 or higher are rarely needed.\n\n\n\n\n\n\nExample: Shampoo detrending by differencing\n\n\n\nRemove trend from the shampoo series by applying the differences.\nBased on Figure 2.16, differencing once is enough to remove the trend. The resulting detrended series is \\[\nY^*_t = \\Delta Y_t = (1 - B)Y_t = Y_t - Y_{t-1}.\n\\]\n\n\nCode\np1 <- pshampoo\np2 <- forecast::ggAcf(shampoo) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np3 <- autoplot(diff(shampoo)) + \n    xlab(\"Year\") + \n    ylab(\"Sales\") + \n    theme_light()\np4 <- forecast::ggAcf(diff(shampoo)) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np5 <- autoplot(diff(shampoo, differences = 2)) + \n    xlab(\"Year\") + \n    ylab(\"Sales\") + \n    theme_light()\np6 <- forecast::ggAcf(diff(shampoo, differences = 2)) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.16: Time series plot of shampoo sales with an estimated ACF and the detrended (differenced) series with their ACFs.\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Airline passengers detrending and deseasonalizing by differencing\n\n\n\nRemove trend and seasonality from the log transformed AirPassengers series by applying the differences.\nBased on Figure 2.17, differencing once with the non-seasonal and the seasonal lags is enough to remove the trend and strong seasonality. The final series (denoted as D1D12) is \\[\n\\begin{split}\nD1D12_t & = (1 - B)(1 - B^{12})\\lg Y_t = (1 - B - B^{12} + B^{13})\\lg Y_t\\\\\n& = \\lg Y_t - \\lg Y_{t-1} - \\lg Y_{t-12} + \\lg Y_{t-13}.\n\\end{split}\n\\]\n\n\nCode\nYt <- log10(AirPassengers)\n# Apply first-order (non-seasonal) differences\nD1 <- diff(Yt)\n# Additionally, apply first-order seasonal differences\nD1D12 <- diff(D1, lag = 12)\np1 <- autoplot(Yt) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"Yt\") +\n    theme_light()\np2 <- forecast::ggAcf(Yt) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np3 <- autoplot(D1) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"D1\") +\n    theme_light()\np4 <- forecast::ggAcf(D1) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np5 <- autoplot(D1D12) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"D1D12\") +\n    theme_light()\np6 <- forecast::ggAcf(D1D12) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.17: Time series plot of shampoo sales with an estimated ACF and the detrended (differenced) series with their ACFs."
  },
  {
    "objectID": "l03_smoothing.html#conclusion",
    "href": "l03_smoothing.html#conclusion",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.7 Conclusion",
    "text": "2.7 Conclusion\nThe choice of the method to use depends on many things, among them:\n\nThe methods require different skill levels (and time commitment) for their implementation and can be categorized as automatic and non-automatic.\nSome methods are very flexible when dealing with seasonality (e.g., Holt–Winters), while others are not.\nThe goal of smoothing and desired degree of smoothness in the output.\nSome of the methods have a convenient way of getting forecasts (e.g., exponential smoothing and polynomial regression), while other methods are more suitable for interpolation (polynomial regression) and simple visualization (simple moving average), or just trend elimination (all of the considered methods are capable of eliminating trends, but differencing is the method that does nothing else but the elimination).\n\nRemember that forecasted values (point forecasts) are not valuable if their uncertainty is not quantified. To quantify the uncertainty, we provide prediction intervals that are based on the past behavior of residuals and careful residual diagnostics (such as described in the previous lectures).\nThe evaluation of the methods may involve splitting the time series into training and testing periods. Overall, this lecture focused on presenting the methods themselves, while residual diagnostics and evaluation on a testing set were omitted due to time constraints.\nWhile the considered methods are among the most common, there are many more smoothing methods that can be applied to time series: splines (such as used in generalized additive models, GAMs), kernels, machine learning techniques, etc."
  },
  {
    "objectID": "l03_smoothing.html#appendix",
    "href": "l03_smoothing.html#appendix",
    "title": "2  Smoothing (Detrending and Deseasonalizing)",
    "section": "2.8 Appendix",
    "text": "2.8 Appendix\n\n\n\n\n\n\nExample: Smoothing blocks\n\n\n\nA student was concerned about applying smoothing to data that have a block-like structure. Without a real data set on hand, below we simulate a series \\(X_t\\) with the mentioned structure.\n\n\nCode\nset.seed(123)\n# Set number of blocks\nn <- 10 \n# Let the values of blocks come from standard normal distribution,\n# and length of block be a random variable generated from Poisson distribution:\nXt <- rep(rnorm(n), times = rpois(n, 5))\n\n\nFigure 2.18 shows an example of lowess applied to \\(X_t\\) to get a smoothed series \\(M_t\\).\nWe see that lowess estimates smooth trends. If we assume that trends are not smooth, then some other techniques (e.g., piecewise linear estimation) should be used.\n\n\nCode\nt <- c(1:length(Xt))\nggplot(data.frame(t = t, Xt = Xt), aes(x = t, y = Xt)) + \n    geom_line() + \n    geom_smooth(se = FALSE, span = 0.25) + \n    theme_light()\n\n\n\n\n\nFigure 2.18: Smoothing block-like data \\(X_t\\).\n\n\n\n\n\n\n\n\n\n\nBerk RA (2016) Statistical learning from a regression perspective, 2nd edn. Springer, Switzerland\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York\n\n\nCleveland WS (1979) Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association 74:829–836. https://doi.org/10.1080/01621459.1979.10481038\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern time series analysis. Springer-Verlag, Berlin"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Berk RA (2016) Statistical learning\nfrom a regression perspective, 2nd edn. Springer, Switzerland\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and\nforecasting, 2nd edn. Springer, New York\n\n\nCleveland WS (1979) Robust locally weighted regression and smoothing\nscatterplots. Journal of the American Statistical Association\n74:829–836. https://doi.org/10.1080/01621459.1979.10481038\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern\ntime series analysis. Springer-Verlag, Berlin"
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This is a collection of lecture notes on applied time series analysis and forecasting using the statistical programming language R. Many of these lectures are based on the original notes by Y. R. Gel and C. Cutler for the course STAT-443 Forecasting (University of Waterloo, Canada) adapted and expanded by V. Lyubchich for the course MEES-713 Environmental Statistics 2 (University of Maryland, USA).\nEach lecture starts by listing the learning objectives and required reading materials, with additional references in the text. The notes introduce the methods and give a few examples but are less detailed than the reading materials. The notes do not substitute a textbook."
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Time Series Analysis",
    "section": "Authors",
    "text": "Authors\nVyacheslav Lyubchich is an Associate Research Professor at the University of Maryland Center for Environmental Science. https://www.umces.edu/vyacheslav-lyubchich\nYulia R. Gel is a Professor of Statistics at the University of Texas at Dallas. https://personal.utdallas.edu/~yxg142030/"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "Time Series Analysis",
    "section": "Citation",
    "text": "Citation\nLyubchich, V. and Gel, Y. R. (2022) Time Series Analysis. Lecture Notes. Draft 2022-12."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Time Series Analysis",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "l01_regression.html",
    "href": "l01_regression.html",
    "title": "1  Review of Linear Regression",
    "section": "",
    "text": "After this lecture, you should be competent (again) in assessing violations of various assumptions of linear regression models, particularly assumptions about model residuals, by being able to apply visual assessments and formal statistical tests, interpret the results and effects of the violations.\nObjectives\nReading materials"
  },
  {
    "objectID": "l01_regression.html#diagnostics-for-the-simple-linear-regression-residual-analysis",
    "href": "l01_regression.html#diagnostics-for-the-simple-linear-regression-residual-analysis",
    "title": "1  Review of Linear Regression",
    "section": "1.1 Diagnostics for the simple linear regression: residual analysis",
    "text": "1.1 Diagnostics for the simple linear regression: residual analysis\nGiven a simple linear regression (SLR) model \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t} + \\epsilon_{t},\n\\] where \\(Y_{t}\\) is the dependent variable and \\(X_{t}\\) is the regressor (independent, predictor) variable, \\(t = 1,\\ldots,n\\), and \\(n\\) is the sample size.\nThe Gauss–Markov theorem If \\(\\epsilon_{t}\\) are uncorrelated random variables with common variance, then of all possible estimators \\(\\beta^{\\ast}_{0}\\) and \\(\\beta^{\\ast}_{1}\\) that are linear functions of \\(Y_{t}\\), the least squares estimators have the smallest variance.\nThus, the ordinary least squares (OLS) assumptions are:\n\nthe residuals \\(\\epsilon_{t}\\) have common variance (\\(\\epsilon_{t}\\) are homoscedastic);\nthe residuals \\(\\epsilon_{t}\\) are uncorrelated;\nto provide prediction intervals (PIs), confidence intervals (CIs), and to test hypotheses about the parameters in our model, we also need to assume that\nthe residuals \\(\\epsilon_{t}\\) are normally distributed (\\(\\epsilon_{t} \\sim N (0, \\sigma^{ 2} )\\)).\n\n\n\n\n\n\n\nNote\n\n\n\nIf the residuals are independent and identically distributed and normal (\\(\\epsilon_{t} \\sim\\) i.i.d. \\(N(0, \\sigma^{2}\\))), then all three above properties are automatically satisfied. In this case, \\(\\epsilon_{t}\\) are not only uncorrelated, \\(\\epsilon_{t}\\) are independent. To be independent is a much stronger property than to be uncorrelated.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile a given model may still have useful predictive value even when the OLS assumptions are violated, the confidence intervals, prediction intervals, and \\(p\\)-values associated with the \\(t\\)-statistics will generally be incorrect when the OLS assumptions do not hold.\n\n\nA basic technique for investigating the aptness of a regression model is based on analyzing the residuals \\(\\epsilon_{t}\\). In a residual analysis we attempt to assess the validity of the OLS assumptions by examining the estimated residuals \\(\\hat{\\epsilon}_{1}, \\ldots, \\hat{\\epsilon}_{n}\\) to see if they actually satisfy the imposed conditions. If the model is apt, the observed residuals should reflect the assumptions listed above.\nWe perform our diagnostics analysis from a step-by-step verification of each assumption. We start with visual diagnostics, then proceed with formal tests. A lot of useful diagnostic information may be obtained from a residual plot.\n\n1.1.1 Homoscedasticity\nWe plot the residuals \\(\\hat{\\epsilon}_{t}\\) vs. time, fitted values \\(\\hat{Y}_{t}\\), and predictor values \\(X_t\\). If the assumption of constant variance is satisfied, \\(\\hat{\\epsilon}_{t}\\) fluctuate around the zero mean with more or less constant amplitude and this amplitude does not change with time, fitted values \\(\\hat{Y}_{t}\\), and predictor values \\(X_t\\).\nIf the (linear) model is not appropriate, the mean of the residuals may be non-constant, i.e., not always 0. Figure 1.1 shows an example of random pattern that we would like the residuals to have (no systematic patterns).\n\n\nCode\nset.seed(1)\nn = 26; m = 0; s = 522\nx <- ts(rnorm(n, mean = m, sd = s))\nforecast::autoplot(x) + \n    geom_hline(yintercept = 0, lty = 2, col = 4) +\n    theme_light()\n\n\n\n\n\nFigure 1.1: A time series plot of ‘ideal’ residuals. These residuals \\(x_t\\) are simulated i.i.d. normal.\n\n\n\n\nWhat can we notice in a residual plot?\n\nChange of variability with time indicates heterogeneity of variance of the residuals.\nObvious lack of symmetry (around 0) in the plot suggests a lack of normality or presence of outliers.\nSystematic trends in the residuals suggest correlations between the residuals or inadequateness of the proposed model.\n\nSometimes it is possible to transform the dependent or independent variables to remedy these problems, i.e., to get rid of the correlated residuals or to stabilize the variance.  Otherwise, we need to change (re-specify) the model.\nA useful technique that can guide us in this process is to plot \\(\\hat{\\epsilon}_{t}\\) vs. \\(\\hat{Y}_{t}\\) and \\(\\hat{\\epsilon}_{t}\\) vs. each predictor \\(X_t\\). Similarly to their time series plot, \\(\\hat{\\epsilon}_{t}\\) should fluctuate around the zero mean with more or less constant amplitude.\n\n\n\n\n\n\nExample: Dishwasher shipments model and patterns in residuals\n\n\n\nFigure 1.2 shows R code and residuals of a simple linear regression exploring dishwasher shipments (DISH) and private residential investments (RES) for a number of years.\nHow different are the patterns in Figure 1.2 from those in Figure 1.1?\n\n\nCode\nD <- read.delim(\"./data/dish.txt\") %>% \n    rename(Year = YEAR)\nmod1 <- lm(DISH ~ RES, data = D)\np1 <- ggplot(D, aes(x = Year, y = mod1$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ylab(\"Residuals\") +\n    theme_light()\np2 <- ggplot(D, aes(x = mod1$fitted.values, y = mod1$residuals)) + \n    geom_point() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    xlab(\"Fitted values\") +\n    ylab(\"Residuals\") +\n    theme_light()\np3 <- ggplot(D, aes(x = RES, y = mod1$residuals)) + \n    geom_point() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    xlab(\"Residential investments (RES)\") +\n    ylab(\"Residuals\") +\n    theme_light()\np1 + p2 + p3 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 1.2: Estimated residuals plotted vs. time, fitted values, and predictor.\n\n\n\n\n\n\n\n\n1.1.2 Uncorrelatedness\nIt is a deep topic that we shall discuss many times in different variations in the future. When observations are obtained in a time sequence (the topic of time series analysis and of our course), there is a high possibility that the errors \\(\\epsilon_{t}\\) are correlated. For instance, if the residual is positive (or negative) for a given day \\(t\\), it is likely that the residual for the following day \\(t+1\\) is also positive (or negative). Such residuals are said to be autocorrelated (i.e., serially correlated). In many environmental time series autocorrelation is positive.\nWhen the residuals \\(\\epsilon_{t}\\) are related over time, a model for the residuals frequently employed is the first-order autoregressive model, i.e., the AR(1) model.\nThe autoregressive model of the first order, AR(1), is defined as \\[\n\\epsilon_{t} = \\rho \\epsilon_{t - 1} + u_{t},\n\\] where \\(\\rho\\) is the autoregression coefficient (\\(- 1 < \\rho < 1\\)) and \\(u_{t}\\) are uncorrelated \\(N (0, \\sigma^{2})\\).\nThe model assumes that the residual \\(\\epsilon_{t}\\) at time \\(t\\) contains a component resulting from the residual \\(\\epsilon_{t - 1}\\) at time \\(t - 1\\) and a random disturbance \\(u_{t}\\) that is independent of the earlier periods.\nEffects of autocorrelation If the OLS method is employed for the parameter estimation and the residuals \\(\\epsilon_{t}\\) are autocorrelated of the first order, then the consequences are:\n\nThe OLS estimators will still be unbiased, but they no longer have the minimum variance property (see the Gauss–Markov theorem); they tend to be relatively inefficient.\nThe residual mean square error (MSE) can seriously underestimate the true variance of the error terms in the model.\nStandard procedures for CI, PI, and tests using the \\(F\\) and Student’s \\(t\\) distributions are no longer strictly applicable.\n\nFor example, see Section 5.2 in Chatterjee and Simonoff (2013) for more details.\nDurbin–Watson test\nA widely used test for examining whether the residuals in a regression model are correlated is the Durbin–Watson test. This test is based on the AR(1) model for \\(\\epsilon_{t}\\). The one-tail test alternatives are \\[\n\\begin{align}\nH_{0}{:} ~ \\rho = 0 & ~~ vs. ~~ H_{1}{:} ~ \\rho > 0,\\\\\nH_{0}{:} ~ \\rho = 0 & ~~ vs. ~~ H_{1}{:} ~ \\rho < 0,\n\\end{align}\n\\] and the two-tail test is \\[\nH_{0}{:} ~ \\rho = 0 ~~ vs. ~~ H_{1}{:} ~ \\rho \\neq 0.\\\\\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWhen dealing with real data, the positive autocorrelation is usually the case.\n\n\nThe Durbin–Watson test statistic DW is based on the differences between the adjacent residuals, \\(\\epsilon_{t} - \\epsilon_{t - 1}\\), and is of the following form: \\[\n\\text{DW} = \\frac{\\sum^{n}_{t = 2} \\left( \\epsilon_{t} - \\epsilon_{t - 1} \\right)^{2}}{\\sum^{n}_{t = 1} \\epsilon^{2}_{t}},\n\\] where \\(\\epsilon_{t}\\) is the regression residual at time \\(t\\) and \\(n\\) is the number of observations.\nThe DW statistic takes on values in the range \\([0, 4]\\). In fact,\n\nWhen \\(\\epsilon_{t}\\) are positively correlated, adjacent residuals tend to be of similar magnitude so that the numerator of DW will be relatively small or 0.\nWhen \\(\\epsilon_{t}\\) are negatively correlated, adjacent residuals tend to be of similar magnitude but with the opposite sign so that the numerator of DW will be relatively large or equal to 4.\n\nHence, low DW corresponds to positive autocorrelation. Values of DW that tend towards 4 are in the region for negative autocorrelation.\nThe exact action limit for the Durbin–Watson test is difficult to calculate. Hence, the test is used with a lower bound \\(d_{L}\\) and an upper bound \\(d_{U}\\). %When the test statistic is less than \\(d_{L}\\), we conclude that the positive autocorrelation is present. When the test statistic exceeds the upper bound \\(d_{U}\\), we do not reject the \\(H_0\\). We may use Table 1.1 as a rule of thumb.\n\n\nTable 1.1: Regions of rejection of the null hypothesis for the Durbin–Watson test\n\n\n\n\n\n\n\n\n\nfrom 0 to \\(d_{L}\\)\nfrom \\(d_{L}\\) to \\(d_{U}\\)\nfrom \\(d_{U}\\) to \\(4 - d_{U}\\)\nfrom \\(4 - d_{U}\\) to \\(4 - d_{L}\\)\nfrom \\(4 - d_{L}\\) to 4\n\n\n\n\nReject \\(H_{0}\\), positive autocorrelation\nNeither accept \\(H_{1}\\) or reject \\(H_{0}\\)\nDo not reject \\(H_{0}\\)\nNeither accept \\(H_{1}\\) or reject \\(H_{0}\\)\nReject \\(H_{0}\\), negative autocorrelation\n\n\n\n\nThe critical values \\(d_{L}\\) and \\(d_{U}\\) have been tabulated for combinations of various sample sizes, significance levels, and number of regressors in a model. For large samples, normal approximation can be used (Chatterjee and Simonoff 2013): \\[\nz = \\left(\\frac{\\text{DW}}{2} - 1 \\right)\\sqrt{n}.\n\\] Statistical software packages usually provide exact \\(p\\)-values based on the null distribution of the test statistic (a linear combination of \\(\\chi^2\\) variables).\n\n\n\n\n\n\nExample: Dishwasher residuals DW test\n\n\n\nApply the Durbin–Watson test to the residuals from the dishwashers example, i.e., DISH vs. RES, using the R package lmtest.\n\n\nCode\nlmtest::dwtest(D$DISH ~ D$RES, alternative = \"greater\")\n\n\n#> \n#>  Durbin-Watson test\n#> \n#> data:  D$DISH ~ D$RES\n#> DW = 0.6, p-value = 3e-06\n#> alternative hypothesis: true autocorrelation is greater than 0\n\n\nBased on the low \\(p\\)-value we can reject the \\(H_{0}\\): \\(\\rho = 0\\) at the 95% confidence level and accept the alternative \\(H_{1}\\): \\(\\rho > 0\\).\n\n\nRuns test\nDepartures of randomness can take so many forms that no single test for randomness is best for all situations. For instance, one of the most common departures from randomness is the tendency of a sequence to persist in its direction of movement.\nWe can count the number of times a sequence of observations crossed a cut-off line, for example, the median line, and use this information to assess randomness of \\(\\epsilon_t\\). Alternatively we count a succession of plus or minus signs, surrounded by the opposite signs. Each such succession is called a run.\nThe formal test is the following. When a sequence of \\(N\\) observations with \\(n\\) observations in positive runs and \\(m\\) observations in negative runs is a random process with independent values generated from a continuous distribution, then the sampling distribution of the number of runs \\(R\\) has the mean and variance \\[\n\\mathrm{E}(R) = \\frac{1 + 2nm}{N}, \\qquad \\sigma^2(R) = \\frac{2nm(2nm-n-m)}{N^2(N-1)},\n\\] where \\(N = n + m\\) is the total sample size.\nThe only assumption for this test is that all sample observations come from a continuous distribution.\nThe two-tail alternative is as follows\n\n\\(H_{0}\\): Sequence is generated by a random process;\n\\(H_{1}\\): Sequence is generated by a process containing either persistence or frequent changes in direction.\n\nWhen positive autocorrelation (or persistence) is present, \\(R\\) will clearly be small. On the other hand, if the process involves frequent changes in direction (negative autocorrelation or anti-persistence), \\(R\\) will be too large.\nWhen the number of observations is sufficiently large, i.e., \\(N > 30\\), the runs test statistic \\(R\\) is based on the standardized normal test statistic \\[\nz = \\frac{R - \\mathrm{E}(R)}{ \\sigma(R)}.\n\\] Here \\(z\\) follows approximately a standard normal distribution.\nRuns test is very easy to interpret. Runs test allows to assess only the first order serial correlation in the residuals, i.e., to test whether two residuals that are one lag apart are correlated.\n\n\n\n\n\n\nExample: Dishwasher residuals runs test\n\n\n\n\n\nCode\npar(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))\npar(pty = \"m\")\nlawstat::runs.test(x, plot.it = TRUE)\n\n\n#> \n#>  Runs Test - Two sided\n#> \n#> data:  x\n#> Standardized Runs Statistic = -0.4, p-value = 0.7\n\n\nCode\nlawstat::runs.test(mod1$residuals, plot.it = TRUE)\n\n\n#> \n#>  Runs Test - Two sided\n#> \n#> data:  mod1$residuals\n#> Standardized Runs Statistic = -3, p-value = 0.005\n\n\n\n\n\nFigure 1.3: Runs tests and plots of independent normally distributed simulations \\(x_t\\) and the DISH residuals.\n\n\n\n\nThe \\(p\\)-value for the runs test for the residuals is very low (Figure 1.3), which supports findings of the DW test that residuals are first-order serially correlated.\n\n\n\n\n1.1.3 Normality\nThere are two major ways of checking normality. Graphical methods visualize differences between empirical data and theoretical normal distribution. Numerical methods conduct statistical tests on the null hypothesis that the variable is normally distributed.\n\n1.1.3.1 Graphical methods\nGraphical methods visualize the data using graphs, such as histograms, stem-and-leaf plot, box plot, etc. For example, Figure 1.4 shows a histogram of the simulated normally distributed data and the residuals from the dishwashers example with superimposed normal curves with the corresponding mean and standard deviation.\n\n\nCode\np1 <- ggplot(data.frame(x = x), aes(x = x)) + \n    geom_histogram(aes(y = ..density..), binwidth = 300, fill = \"grey50\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = mean(x), sd = sd(x)),\n                  col = 1, lwd = 1.5) +\n    ylab(\"Density\") +\n    ggtitle(\"Random normal values\") + \n    theme_light()\np2 <- ggplot(x, aes(x = mod1$residuals)) + \n    geom_histogram(aes(y = ..density..), binwidth = 300, fill = \"grey50\") +\n    stat_function(fun = dnorm, \n                  args = list(mean = mean(mod1$residuals), sd = sd(mod1$residuals)),\n                  col = 1, lwd = 1.5) +\n    ylab(\"Density\") +\n    ggtitle(\"Model residuals\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 1.4: Histograms of the simulated normally distributed values and estimated regression residuals.\n\n\n\n\nAnother very popular graphical method of assessing normality is the quantile-quantile (Q-Q) plot. The Q-Q plot compares the ordered values of a variable with the corresponding ordered values of the normal distribution.\n\n\n\n\n\n\nNote\n\n\n\nQ-Q plots can also be used to compare sample quantiles with quantiles of other, not normal, distribution (e.g., \\(t\\) or gamma distribution), or to compare quantiles of two samples (to assess if both samples come from the same, unspecified, distribution).\n\n\nLet \\(X\\) be a random variable having the property that the equation \\[\n\\Pr \\left( X \\leqslant x \\right) = \\alpha\n\\] has a unique solution \\(x = x_{(\\alpha)}\\) for each \\(0 < \\alpha < 1\\). That is, there exists \\(x_{(\\alpha)}\\) such that \\[\n\\Pr \\left( X \\leqslant x_{(\\alpha)} \\right) = \\alpha\n\\tag{1.1}\\] and no other value of \\(x\\) satisfies Equation 1.1. Then we will call \\(x_{(\\alpha)}\\) the \\(\\alpha\\)th (population) quantile of \\(X\\). Note that any normal distribution has this uniqueness property. If we consider a standard normal \\(Z \\sim N(0, 1)\\), then some well-known quantiles are:\n\n\\(z_{(0.5)} = 0\\) (the median, )\n\\(z_{(0.05)} = -1.645\\) and \\(z_{(0.95)} = 1.645\\)\n\\(z_{(0.025)} = -1.96\\) and \\(z_{(0.975)} = 1.96\\)\n\nWe call the 0.25th, 0.5th, 0.75th quantiles the first, the second, and the third quartiles, respectively. The quartiles divide our data into 4 equal parts.\nNow suppose \\(X \\sim N (\\mu, \\sigma^{2})\\). By standardizing to \\(Z \\sim N(0, 1)\\), we obtain \\[\n\\alpha = \\Pr \\left( X \\leqslant x_{(\\alpha)} \\right) = \\Pr \\left( \\frac{X - \\mu}{ \\sigma} \\leqslant \\frac{x_{(\\alpha)} - \\mu}{\\sigma} \\right) = \\Pr \\left( Z \\leqslant \\frac{x_{(\\alpha)} - \\mu}{ \\sigma} \\right) .\n\\]\nWe also have \\(\\alpha = \\Pr (Z \\leqslant z_{(\\alpha)} )\\) by definition. Thus, %from the uniqueness property of the \\(N (0, 1)\\), it follows that \\[\nz_{(\\alpha)} = \\frac{x_{(\\alpha)} - \\mu}{ \\sigma} ~~~~ \\mbox{and hence} ~~~~ x_{(\\alpha)} = \\sigma z_{(\\alpha)} + \\mu.\n\\]\nThus, if \\(X\\) is truly normal, a plot of the quantiles of \\(X\\) vs. the quantiles of the standard normal distribution should yield a straight line. A plot of the quantiles of \\(X\\) vs. the quantiles of \\(Z\\) is called a Q-Q plot.\nEstimating quantiles from data Let \\(X_{1}, \\ldots, X_{n}\\) be a sequence of observations. Ideally, \\(X_{1}, \\ldots, X_{n}\\) should represent i.i.d. observations but we will be happy if preliminary tests indicate that they are homoscedastic and uncorrelated (see the previous sections). We order them from the smallest to the largest and indicate this using the notation \\[\nX_{(1/n)} < X_{(2/n)} < X_{(3/n)} < \\ldots < X_{\\left((n - 1)/n\\right)} < X_{(n/n)}.\n\\]\nThe above ordering assumes no ties, but ties can be quite common in data, even continuous data, because of rounding. As long as the proportion of ties is small, this method can be used.\nNote that the proportion of observations less than or equal to \\(X_{(k/n)}\\) is exactly \\(k/n\\). Hence \\(X_{(k/n)}\\), called the \\(k\\)th {}, is an estimate of the population quantile \\(x_{(k/n)}\\).\nThe normal Q-Q plot is obtained by plotting the sample quantiles vs. the quantiles of the standard normal distribution. The base-R function qqnorm() produces a normal Q-Q plot of data and the function qqline() adds a line that passes through the first and third quartiles. The R package ggplot2 has analogous functions ggplot2::stat_qq() and ggplot2::stat_qq_line(). The R package car also draws Q-Q plots and adds a point-wise confidence envelope with its function car::qqPlot().\n\n\n\n\n\n\nExample: Dishwasher residuals normal Q-Q plot\n\n\n\nFigure 1.5 shows the Q-Q plots of the residuals from the dishwashers example and the simulated normal data with the same mean and standard deviation.\n\n\nCode\npar(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))\npar(pty = \"m\")\ncar::qqPlot(x, las = 1, id = FALSE,\n    xlab = \"Standard normal quantiles\", \n    main = \"Random normal values\")\ncar::qqPlot(mod1$residuals, las = 1, id = FALSE,\n    xlab = \"Standard normal quantiles\", \n    main = \"Model residuals\")\n\n\n\n\n\nFigure 1.5: Normal Q-Q plots of the normally distributed simulated values \\(x_t\\) and the dishwasher residuals.\n\n\n\n\n\n\nAlthough visually appealing, these graphical methods do not provide objective criteria to determine the normality of variables.\nShapiro–Wilk normality test\nOne of the most popular numerical methods for assessing normality is the Shapiro–Wilk (SW) test:\n\n\\(H_0\\): the sample data come from a normally distributed population;\n\\(H_1\\): the population is not normally distributed).\n\nThe SW test is the ratio of the best estimator of the variance to the usual corrected sum of squares estimator of the variance. It has been originally constructed by considering the regression of ordered sample values on corresponding expected normal order statistics. The SW statistic is given by \\[\n\\mbox{SW} = \\frac{\\left(\\sum a_{i} x_{(i)} \\right)^{2}}{\\sum \\left(x_{i} - \\bar{x} \\right)^{2}},\n\\] where \\(x_{(i)}\\) are the ordered sample values (\\(x_{(1)}\\) is the smallest) and the \\(a_{i}\\) are constants generated from the means, variances and covariances of the order statistics of a sample of size \\(n\\) from a normal distribution. The SW statistics lies between 0 and 1. If the SW statistic is close to 1, this indicates normality of the data. The SW statistic requires the sample size \\(n\\) to be between 7 and 2000.\n\n\n\n\n\n\nExample: Dishwasher residuals normality test\n\n\n\nBased on the \\(p\\)-values below, we cannot reject the null hypothesis of normality in both cases.\n\n\nCode\nshapiro.test(x)\n\n\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  x\n#> W = 0.9, p-value = 0.08\n\n\nCode\nshapiro.test(mod1$residuals)\n\n\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  mod1$residuals\n#> W = 1, p-value = 0.8\n\n\n\n\n\n\n\n1.1.4 Summary of the simple linear regression residual diagnostics\n\nThe residuals do not have a constant mean.\nThe residuals seem to have a constant variance.\nThe residuals are positively correlated.\nThe residuals look normally distributed (but the SW statistic might be affected by serial correlation of the residuals)."
  },
  {
    "objectID": "l01_regression.html#multiple-linear-regression",
    "href": "l01_regression.html#multiple-linear-regression",
    "title": "1  Review of Linear Regression",
    "section": "1.2 Multiple linear regression",
    "text": "1.2 Multiple linear regression\nHere we consider a case of \\(p\\) explanatory variables \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t,1} + \\ldots + \\beta_{p} X_{t,p} + \\epsilon_{t} \\quad (t = 1,\\ldots,n).\n\\]\nThis can be expressed more compactly in a matrix notation as \\[\n\\boldsymbol{Y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},\n\\] where \\(\\boldsymbol{Y} = (Y_{1}, \\ldots, Y_{n})^{\\top}\\), \\(\\boldsymbol{\\beta} = (\\beta_{0} , \\ldots, \\beta_{p})^{\\top}\\), \\(\\boldsymbol{\\epsilon} = (\\epsilon_{1} , \\ldots, \\epsilon_{n})^{\\top}\\); \\(\\boldsymbol{X}\\) is an \\(n \\times (p + 1)\\) design matrix \\[\n\\boldsymbol{X} = \\left(\n\\begin{array}{cccc}\n1 & X_{1,1} & \\ldots & X_{1,p} \\\\\n1 & X_{2,1}& \\ldots & X_{2,p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & X_{n,1}& \\ldots & X_{n,p}\n\\end{array}\n\\right).\n\\]\nHere the historical data for the dependent variable consist of the observations \\(Y_{1}, \\ldots, Y_{n}\\); the historical data for the independent variables consist of the observations in the matrix \\(\\boldsymbol{X}\\).\nMinimizing \\(SSE =(\\boldsymbol{Y} - \\boldsymbol{X} \\hat{\\boldsymbol{\\beta}})^{\\top} (\\boldsymbol{Y} - \\boldsymbol{X} \\hat{\\boldsymbol{\\beta}})\\) yields the least squares solutions \\[\n\\hat{\\boldsymbol{\\beta}} = \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^{\\top} \\boldsymbol{Y}\n\\] for non-singular \\(\\boldsymbol{X}^{\\top}\\boldsymbol{X}\\).\nThe forecast of a future value \\(Y_{t}\\) is then given by \\[\n\\hat{Y}_{t} = \\boldsymbol{x}^{\\top}_{t} \\hat{\\boldsymbol{\\beta}},\n\\] where \\(\\boldsymbol{x}_{t}\\) is a (column) vector at time \\(t\\).\nUnder the OLS assumptions (recall them) we obtain \\[\n\\mathrm{var} \\left( \\hat{\\beta}_{j} \\right) = \\sigma^{2} \\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj},\n\\] where the \\(\\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}\\) denotes the \\(j\\)th diagonal element of \\(\\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}\\).\nThis yields \\[\ns.e. \\left( \\hat{\\beta}_{j} \\right) = \\hat{\\sigma} \\sqrt{ \\left( \\boldsymbol{X }^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}}.\n\\]\nNote that here the degrees of freedom (d.f.) are \\(n - (p + 1) = n - p - 1\\). (The number of estimated parameters for the independent variables is \\(p\\), plus one for the intercept, i.e., \\(p + 1\\).)\nUnder the OLS assumptions, a \\(100(1 - \\alpha)\\)% confidence interval for the parameter \\(\\beta_{j}\\) (\\(j = 0, 1, \\ldots, p\\)) is given by \\[\n\\begin{split}\n\\hat{\\beta}_{j} &\\pm t_{\\alpha / 2, n - (p+1)} s.e.\\left( \\hat{\\beta}_{j} \\right) \\text{ or} \\\\\n\\hat{\\beta}_{j} &\\pm t_{\\alpha / 2, n - (p+1)} \\hat{\\sigma} \\sqrt{\\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}_{jj}}.\n\\end{split}\n\\tag{1.2}\\]\nTypically, \\(s.e.(\\hat{\\beta}_{j})\\) is available directly from the R output, so Equation 1.2 is calculated automatically.\nUnder the OLS assumptions it can be shown that \\[\n\\mathrm{var} \\left( Y_{t} - \\hat{Y}_{t} \\right) = \\sigma^{2} \\left( \\boldsymbol{x}^{\\top}_{t} \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{- 1} \\boldsymbol{x}_{t} + 1 \\right),\n\\] yielding a \\(100(1 - \\alpha)\\)% prediction interval for \\(Y_{t}\\): \\[\n\\boldsymbol{x}^{\\top}_{t} \\hat{\\boldsymbol{\\beta}} \\pm t_{\\alpha / 2, n-(p+1)} \\hat{\\sigma} \\sqrt{ \\boldsymbol{x}^{\\top}_{t} \\left( \\boldsymbol{X}^{\\top} \\boldsymbol{X} \\right)^{-1}\\boldsymbol{x}_{t} + 1}.\n\\]\nWe usually never perform these calculations by hand and will use the corresponding software functions, e.g., using the function predict(), see an example code below.\nWhat else can we get from the regression output?\nAs in SLR, we will look for the \\(t\\)-statistics and \\(p\\)-values to get an idea about statistical significance of each of the predictors \\(X_{t,1}, X_{t,2}, \\ldots, X_{t,p}\\). The confidence intervals constructed above correspond to individual tests of hypothesis about a parameter, i.e., \\(H_{0}\\): \\(\\beta_{j} = 0\\) vs. \\(H_{1}\\): \\(\\beta_{j} \\neq 0\\).\nWe can also make use of the \\(F\\)-test. The \\(F\\)-test considers all parameters (other than the intercept \\(\\beta_{0}\\)) simultaneously, testing \\[\n\\begin{split}\nH_{0}{:} ~ \\beta_{1} &= \\ldots = \\beta_{p} = 0 ~~~ \\text{vs.} \\\\\nH_{1}{:} ~ \\beta_{j} &\\neq 0 ~~~ \\mbox{for at least one} ~~~ j \\in \\{1, \\ldots, p \\}.\n\\end{split}\n\\]\nFormally, \\(F_{\\rm{obs}} = \\rm{MSR/MSE}\\) (the ratio of the mean square due to regression and the mean square due to stochastic errors).\nWe reject \\(H_{0}\\) when \\(F_{\\rm{obs}}\\) is too large relative to a cut-off point determined by the degrees of freedom of the \\(F\\)-distribution. The \\(p\\)-value for this \\(F\\)-test is provided in the lm() output. Rejecting \\(H_{0}\\) is equivalent to stating that the model has some explanatory value within the range of the data set, meaning that changes in at least some of the explanatory \\(X\\)-variables correlate to changes in the average value of \\(Y\\).\nRecall that \\[\n\\begin{split}\n\\mathrm{SST} &= \\sum_{i=1}^n(Y_t-\\overline{Y})^2= \\mathrm{SSR} + \\mathrm{SSE},\\\\\n\\mathrm{SSE} &=\\sum_{t=1}^n(Y_t-\\hat{\\beta}_0 - \\hat{\\beta}_1 X_{t,1}-\\ldots- \\hat{\\beta}_p X_{t,p})\n\\end{split}\n\\] and, hence, \\[\n\\rm{SSR}=\\rm{SST}-\\rm{SSE}.\n\\]\nTo conclude that the model has a reasonable fit, however, we would additionally like to see a high \\(R^{2}\\) value, where \\[\nR^{2} = \\rm{SSR/SST}\n\\] is the proportion of the total sum of squares explained by regression.\nSmall \\(R^{2}\\) means that the stochastic fluctuations around the regression line (or prediction equation) are large, making the prediction task difficult, even though there may be a genuine explanatory relationship between the average value \\(\\mathrm{E}(Y)\\) and some of the \\(X\\)-variables.\nAnother criterion to judge about aptness of the obtained model is the adjusted \\(R^2\\): \\[\nR^2_{adj}=1-\\frac{n-1}{n-p}\\left( 1-R^2 \\right).\n\\]\nUnlike \\(R^2\\) itself, \\(R^2_{adj}\\) need not increase if an arbitrary (even useless) predictor is added to the model because of the correction \\((n-1)/(n-p)\\).\n\n\n\n\n\n\nNote\n\n\n\nThe intercept \\(\\beta_{0}\\) is not included in the \\(F\\)-test because there is no explanatory variable associated with it. In other words, \\(\\beta_{0}\\) does not contribute to the regression part of the model.\n\n\n\n\n\n\n\n\nExample: Dishwasher shipments multiple linear regression\n\n\n\nLet us now extend the SLR model that we considered previously and include another potential predictor, the durable goods expenditures (billion of 1972 dollars). The goal is to build a model to predict the unit factory shipments of dishwashers (DISH) vs. private residential investment (RES) and durable goods expenditures (DUR) using a multiple linear regression model (MLR): \\[\nY_{t} = \\beta_{0} + \\beta_{1} X_{t,1} + \\beta_{2} X_{t,2} + \\epsilon_t,\n\\] where \\(X_{t,1}\\) is the private residential investment RES; \\(X_{t,2}\\) is the durable goods expenditures DUR.\nNow we apply the OLS method to estimate the coefficients \\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\beta_{2}\\) using the function lm().\n\n\nCode\nmod2 <- lm(DISH ~ RES + DUR, data = D)\nsummary(mod2)\n\n\n#> \n#> Call:\n#> lm(formula = DISH ~ RES + DUR, data = D)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -643.9 -263.2  -22.1  190.2  920.0 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -1603.09     391.31   -4.10  0.00044 ***\n#> RES            50.97      11.37    4.48  0.00017 ***\n#> DUR            13.77       2.78    4.95  5.2e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 378 on 23 degrees of freedom\n#> Multiple R-squared:  0.877,  Adjusted R-squared:  0.867 \n#> F-statistic: 82.3 on 2 and 23 DF,  p-value: 3.31e-11\n\n\nWe have a high \\(R^2_{adj} =\\) 0.867 and both predictors are statistically significant, but can we already wholeheartedly trust these results?\nPerform the residual diagnostics\nPlot of the estimated residuals \\(\\hat{\\epsilon}_{t} = \\hat{Y}_{t} - Y_{t}\\) vs. the observed \\(Y_{t}\\), (\\(t = 1, 2, \\ldots, 26\\) or Year) – Figure 1.6.\n\n\nCode\np1 <- ggplot(D, aes(x = Year, y = mod2$residuals)) + \n    geom_line() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    ylab(\"Residuals\") +\n    theme_light()\np2 <- ggplot(D, aes(x = mod2$fitted.values, y = mod2$residuals)) + \n    geom_point() + \n    geom_hline(yintercept = 0, lty = 2, col = 4) + \n    xlab(\"Fitted values\") +\n    ylab(\"Residuals\") +\n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 1.6: Residuals vs. time and vs. fitted values.\n\n\n\n\nCheck that the residuals \\(\\epsilon_{t}\\) are uncorrelated (the Durbin–Watson and the runs tests):\n\n\nCode\nlmtest::dwtest(D$DISH ~ D$RES + D$DUR)\n\n\n#> \n#>  Durbin-Watson test\n#> \n#> data:  D$DISH ~ D$RES + D$DUR\n#> DW = 0.4, p-value = 7e-09\n#> alternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\nCode\nlawstat::runs.test(mod2$residuals, plot.it = FALSE)\n\n\n#> \n#>  Runs Test - Two sided\n#> \n#> data:  mod2$residuals\n#> Standardized Runs Statistic = -4, p-value = 3e-04\n\n\nCheck that the residuals \\(\\epsilon_{t}\\) are normally distributed using the Q-Q plot (Figure 1.7) and Shapiro–Wilk test.\n\n\nCode\nshapiro.test(mod2$residuals)\n\n\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  mod2$residuals\n#> W = 1, p-value = 0.9\n\n\n\n\nCode\nggplot(D, aes(sample = mod2$residuals)) +\n    stat_qq() +\n    stat_qq_line() +\n    xlab(\"Standard normal quantiles\") +\n    ylab(\"Sample quantiles\") +\n    ggtitle(\"Model residuals\") +\n    theme_light()\n\n\n\n\n\nFigure 1.7: Normal Q-Q plot of the multiple regression residuals.\n\n\n\n\n\n\n\n1.2.1 Summary of the multiple linear regression residual diagnostics\n\nThe \\(R^{2}\\) has been improved.\nWe do not see a visible improvement in terms of mean and variance of the residuals.\nThe residuals are still positively correlated.\nThe residuals look normally distributed.\n\nEven though not all OLS assumptions are satisfied we shall consider how to predict the future values of \\(Y\\) and to construct the prediction intervals using R.\nFor example, assume that we need to predict the future unit factory shipments of dishwashers (DISH) based on the private residential investment 100 billion USD and durable goods expenditures 150 billion USD.\nSupply new values of independent variables and use the function predict().\n\n\nCode\nnewData <- data.frame(RES = c(100), DUR = c(150))\npredict(mod2, newData, se.fit = TRUE, interval = \"prediction\", level = 0.95)\n\n\n#> $fit\n#>    fit  lwr  upr\n#> 1 5559 4227 6891\n#> \n#> $se.fit\n#> [1] 521\n#> \n#> $df\n#> [1] 23\n#> \n#> $residual.scale\n#> [1] 378"
  },
  {
    "objectID": "l01_regression.html#conclusion",
    "href": "l01_regression.html#conclusion",
    "title": "1  Review of Linear Regression",
    "section": "1.3 Conclusion",
    "text": "1.3 Conclusion\nWe have recalled the standard assumptions about residuals of linear regression models. Remember that there are some other assumptions (e.g., about linear independence of predictors) that must be verified. Refer to the reading materials for a complete list.\nThe methods we have used to test the homogeneity of residuals included various residual plots. The normality of residuals can be assessed using histograms or Q-Q plots and statistical tests such as Shapiro–Wilk normality test.\nThe use of time series in regression presents additional ways to assess patterns in the regression residuals. A plot of residuals vs. time is assessed for homogeneity and absence of trends. Less obvious patterns, such as autocorrelation, can be tested with parametric and nonparametric tests, such as the Durbin–Watson and runs tests.\nThe goal of the statistical techniques we will learn is to model or extract as much information from time series (including the autocorrelation of regression residuals) as possible, such that the remaining series are completely random."
  },
  {
    "objectID": "l01_regression.html#appendix",
    "href": "l01_regression.html#appendix",
    "title": "1  Review of Linear Regression",
    "section": "1.4 Appendix",
    "text": "1.4 Appendix\nAlternative versions of the runs test are available, e.g., in the package randtests.\n\n\nCode\nrandtests::runs.test(mod2$residuals)\n\n\n#> \n#>  Runs Test\n#> \n#> data:  mod2$residuals\n#> statistic = -4, runs = 5, n1 = 13, n2 = 13, n = 26, p-value = 3e-04\n#> alternative hypothesis: nonrandomness\n\n\nDifference sign test\nThe logic behind the difference sign test is that in a random process there will be roughly the same number of ups (positive differences between consecutive values, i.e., \\(X_t-X_{t-1}\\)) and downs (negative differences).\nBrockwell and Davis (2002): “The difference-sign test must be used with caution. A set of observations exhibiting a strong cyclic component will pass the difference-sign test for randomness, since roughly half of the observations will be points of increase.” You may see (Figure 1.2 and Figure 1.6), it is the case for our residuals, even we have no cyclic component, but have rise then slide down.\n\n\nCode\nrandtests::difference.sign.test(x)\n\n\n#> \n#>  Difference Sign Test\n#> \n#> data:  x\n#> statistic = -2, n = 26, p-value = 0.1\n#> alternative hypothesis: nonrandomness\n\n\nCode\nrandtests::difference.sign.test(mod1$residuals)\n\n\n#> \n#>  Difference Sign Test\n#> \n#> data:  mod1$residuals\n#> statistic = 0.3, n = 26, p-value = 0.7\n#> alternative hypothesis: nonrandomness\n\n\nCode\nrandtests::difference.sign.test(mod2$residuals)\n\n\n#> \n#>  Difference Sign Test\n#> \n#> data:  mod2$residuals\n#> statistic = -0.3, n = 26, p-value = 0.7\n#> alternative hypothesis: nonrandomness\n\n\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn. John Wiley & Sons, Hoboken, New Jersey\n\n\nChatterjee S, Simonoff JS (2013) Handbook of regression analysis. John Wiley & Sons, Hoboken, New Jersey"
  },
  {
    "objectID": "l02_tsintro.html",
    "href": "l02_tsintro.html",
    "title": "2  Introduction to Time Series Analysis",
    "section": "",
    "text": "The goal of this lecture is to introduce time series and their common components. You should become confident in identifying and characterizing trends, seasonal and other variability based on visual analysis of time series plots and plots of autocorrelation functions.\nObjectives\nReading materials"
  },
  {
    "objectID": "l02_tsintro.html#time-series-and-its-components",
    "href": "l02_tsintro.html#time-series-and-its-components",
    "title": "2  Introduction to Time Series Analysis",
    "section": "2.1 Time series and its components",
    "text": "2.1 Time series and its components\nA time series is a set of observations \\(Y_t\\), each one being recorded at a specific time \\(t\\) (Brockwell and Davis 2002). The index \\(t\\) will typically refer to some standard unit of time, e.g., seconds, hours, days, weeks, months, or years.\nA time series is a collection of observations made sequentially through time (Chatfield 2000).\nA stochastic process is sequence of random variables \\(Y_t\\), \\(t = 1, 2, \\ldots\\) indexed by time \\(t\\), can be written as \\(\\{Y_t, t\\in[1,T]\\}\\). A time series is a realization of a stochastic process.\nWe shall frequently use the term time series to mean both the data and the process.\n\n\nCode\np <- forecast::autoplot(TSstudio::Michigan_CS) + \n    xlab(\"\") + \n    ylab(\"Index\") + \n    theme_light()\nplotly::ggplotly(p)\n\n\n\n\n\nFigure 2.1: Monthly index of consumer sentiment, University of Michigan Consumer Survey (1966:Q1 = 100).\n\n\n\n\n\nCode\nggplot2::autoplot(JohnsonJohnson) + \n    xlab(\"\") + \n    ylab(\"Earnings per share (USD)\") + \n    theme_light()\n\n\n\n\n\nFigure 2.2: Quarterly earnings (dollars) per Johnson & Johnson share, 1960–1980.\n\n\n\n\n\n\nCode\nggplot2::autoplot(MASS::accdeaths) + \n    xlab(\"\") + \n    ylab(\"Number of accidental deaths\") + \n    theme_light()\n\n\n\n\n\nFigure 2.3: Monthly totals of accidental deaths in the USA, 1973–1978.\n\n\n\n\nIn this course, we will be interested in constructing time series models to learn more about their properties and to forecast (predict) future values of \\(Y_t\\), i.e., values of \\(Y_t\\) for \\(t\\) beyond the end of the data set. Typically, we will use the historical data (or some appropriate subset of it) to build our forecasting models.\n\n2.1.1 Decomposition of time series\nA time series can generally be expressed as a sum or product of four distinct components: \\[\nY_t = M_t + S_t + C_t + \\epsilon_t\n\\] or \\[\nY_t = M_t \\cdot S_t \\cdot C_t \\cdot \\epsilon_t,\n\\] where\n\n\\(M_t\\) is the trend, representing the average change (change in the mean) in the time series over time. Examples of trends are:\n\\(M_t = \\beta_0\\) (constant over time, we usually refer to this case as ‘no trend’);\n\\(M_t = \\beta_0 + \\beta_1t\\) (linear increase or decrease over time);\n \\(M_t = \\beta_0 + \\beta_1t + \\beta_2t^2\\) (quadratic over time).\n\n\\(S_t\\) represents regular periodic fluctuations (a.k.a. seasonality) in the time series. \\(S_t\\) has the property that it is not constant but there exists an integer \\(m \\geqslant 2\\) and scaling factors \\(\\lambda_k > 0\\) such that \\(S_{t+km} = \\lambda_kS_t\\) for \\(1 \\leqslant t \\leqslant m\\) and each \\(k \\geqslant 1\\). The smallest such \\(m\\) is called the period. Periodic time series are quite common and include seasonal variability (period of 1 year), diurnal (period 24 hours), and other cycles such as tides (period 12 hours 25 minutes). This component can be modeled using sinusoidal functions or indicator variables.\n\\(C_t\\) represents irregular cyclical fluctuations, i.e., sinusoidal-type movements that are of irregular length and are not as predictable as the seasonal component, e.g., El Ni~{n}o Southern Oscillation (ENSO) and macroeconomic business cycles. We do not explicitly model \\(C_t\\) in this course.\n\\(\\epsilon_t\\) is the residual or error and represents the remaining unexplained variation in \\(Y_t\\). In other words, it is a random or stochastic component.\n\nFigure 2.4 illustrates an automatic decomposition, however, because the user has too little control of how the decomposition is done, this function is not recommended for use in your analysis. We will talk about the alternatives in the next lecture.\n\n\nCode\nbirths <- scan(\"http://robjhyndman.com/tsdldata/data/nybirths.dat\")\nbirths <- ts(births, frequency = 12, start = c(1946, 1))\nggplot2::autoplot(decompose(births))  + \n    theme_light()\n\n\n\n\n\nFigure 2.4: Time-series decomposition of monthly number of births in New York City (thousands), 1946–1959.\n\n\n\n\n\n\n2.1.2 General approach to time series modeling\n\nPlot the series and examine the main features of the graph, checking whether there is\n\na trend,\na seasonal or other periodic component,\nany apparent sharp changes in behavior,\nany outlying observations.\n\nRemove the trend and periodic components to get stationary residuals (this step is called detrending and deseasonalizing). Broadly speaking, a time series is said to be stationary if there is\n\nno systematic change in the mean (no trend);\nno systematic change in the variance, and\nno strictly periodic variations.\n\nChoose a model to fit the residuals, making use of various sample statistics, including the sample autocorrelation function (ACF).\nForecast the residuals and then invert the transformations to arrive at forecasts of the original series.\n\nThere are two general classes of forecasting models.\nUnivariate time series models include different types of exponential smoothing, trend models, autoregressive models, etc. The characteristic feature of these models is that we need only one time series to start with (\\(Y_t\\)), then we can build a regression of this time series over time (\\(Y_t \\sim t\\)) for estimating trend or, for example, an autoregressive model (‘auto’ \\(=\\) self). In autoregressive approach, the current value \\(Y_t\\) is modeled as a function of the past values: \\[\nY_t = f(Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-p}) + \\epsilon_t,\n\\tag{2.1}\\] while a linear autoregressive model has the form (assume that function \\(f(\\cdot)\\) is a linear parametric function, with parameters \\(\\phi_0, \\phi_1, \\ldots, \\phi_p\\)): \\[\nY_t = \\phi_0 + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\ldots +\\phi_p Y_{t-p} + \\epsilon_t.\n\\]\nMultivariate models involve additional covariates (a.k.a. regressors, predictors, or independent variables) \\(X_{1,t}, X_{2,t}, \\ldots, X_{k,t}\\). A multivariate time series model can be as simple as a multiple regression \\[\nY_t = f(X_{1,t}, X_{2,t}, \\ldots, X_{k,t}) + \\epsilon_t,\n\\] or involve autoregressive components of the response (and predictors): \\[\n\\begin{split}\nY_t = f(&X_{1,t}, X_{2,t}, \\ldots, X_{k,t}, \\\\\n&X_{1,t-1}, X_{1,t-2}, \\ldots, X_{1,t-q1}, \\\\\n&\\cdots\\\\\n&X_{k,t-1}, X_{k,t-2}, \\ldots, X_{k,t-qk}, \\\\\n&Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-p}) + \\epsilon_t,\n\\end{split}\n\\tag{2.2}\\] where \\(p\\), \\(q1, \\ldots, qk\\) are the lags. You can start your analysis with many variables and build a forecasting model as complex as Equation 2.2, but remember that a simpler univariate model may also work well. You should create an appropriate univariate model (like in Equation 2.1) to serve as a baseline, then compare the models’ performance on some out-of-sample data, as described in Section 2.1.3.\n\n\n\n\n\n\nExample: Identify the time-series components in these plots\n\n\n\n\n\nCode\nggplot2::autoplot(sunspots) + \n    xlab(\"\") + \n    ylab(\"Sunspot number\") + \n    theme_light()\n\n\n\n\n\nFigure 2.5: Monthly mean relative sunspot numbers from 1749 to 1983. Collected at Swiss Federal Observatory, Zurich until 1960, then Tokyo Astronomical Observatory. Notice the apparent periodicity, large variability when the series is at a high level and small variability when the series is at a low level. This series is likely to have no trend over the time span we have been able to observe it, though it may have a nearly perfectly periodic component.\n\n\n\n\n\n\nCode\nggplot2::autoplot(lynx) + \n    xlab(\"\") + \n    ylab(\"Number of lynx trappings\") + \n    theme_light()\n\n\n\n\n\nFigure 2.6: Annual numbers of lynx trappings for 1821–1934 in Canada. There is a clear cycle of about 10 years in length (period is 10 years). There is potentially also a longer-term cycle.\n\n\n\n\n\n\nCode\nggplot2::autoplot(Nile) + \n    xlab(\"\") + \n    ylab(bquote('Annual flow  '(10^8~m^3))) + \n    theme_light()\n\n\n\n\n\nFigure 2.7: Annual flow of the river Nile at Aswan, 1871–1970. There are signs of nonlinear dynamics or a changepoint.\n\n\n\n\n\n\nCode\nggplot2::autoplot(co2) + \n    xlab(\"\") + \n    ylab(\"Concentration (ppm)\") + \n    theme_light()\n\n\n\n\n\nFigure 2.8: Monthly Mauna Loa atmospheric CO\\(_2\\) concentration, 1959–1997. There is a strong trend and a strong annual cycle.\n\n\n\n\n\n\nCode\nggplot2::autoplot(Ecdat::Consumption[,\"yd\"]/1000) + \n    xlab(\"\") + \n    ylab(\"Income (thousand 1986 dollars)\") + \n    theme_light()\n\n\n\n\n\nFigure 2.9: Quarterly personal disposable income (Canada, 1947–1996). There is a clear increasing trend of a relatively complex shape. Also, the variability increased in the later years.\n\n\n\n\n\n\nCode\nforecast::autoplot(as.ts(Ecdat::SP500[,1])) + \n    xlab(\"\") + \n    ylab(\"Earnings per share (USD)\") + \n    theme_light()\n\n\n\n\n\nFigure 2.10: Daily returns (change in log index) on Standard & Poor’s 500 Index, 1981-01 to 1991-04. Business days are used as the time index to avoid data gaps during non-trading days. The mean and variance are constant overall (nor increasing or decreasing), but there are clusters of volatility.\n\n\n\n\n\n\n\n\n2.1.3 Model comparison\nHow do we compare forecasting models to decide which one is better? We will look at various ways of choosing between models as the course progresses, but the most obvious answer is to see which one is better at predicting.\nSuppose we have used the data \\(Y_1, \\ldots, Y_n\\) to build \\(M\\) forecasting models \\(\\hat{Y}^{(m)}_t\\) (\\(m = 1,\\ldots,M\\)) and we now obtain future observation \\(Y_{n+1}, \\ldots, Y_{n+k}\\) that were not used to fit the models (also called out-of-sample data, or after-sample, or testing set; \\(k\\) is the size of this set). The difference \\(Y_t - \\hat{Y}^{(m)}_t\\) is the forecast (or prediction) error at time \\(t\\) for the \\(m\\)th model. For each model, compute the prediction mean square error (PMSE) \\[\nPMSE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left(Y_t - \\hat{Y}^{(m)}_t\\right)^2\n\\tag{2.3}\\] and prediction mean absolute error (PMAE) \\[\nPMAE_m = k^{-1}\\sum_{t=n+1}^{n+k}\\left|Y_t - \\hat{Y}^{(m)}_t\\right|\n\\tag{2.4}\\] and, similarly, prediction root mean square error (PRMSE; \\(PRMSE = \\sqrt{PMSE}\\)), prediction mean absolute percentage error (PMAPE, if \\(Y_t \\neq 0\\) in the testing period), etc. We choose the model with the smallest error.\nOne obvious drawback to the above method is that it requires you to wait for future observations in order to compare models. A way around this to take the historical dataset \\(Y_1, \\ldots, Y_n\\) and split it into a training set \\(Y_1, \\ldots, Y_k\\) and a testing set \\(Y_{k+1}, \\ldots, Y_n\\), where \\((n - k)\\ll k\\), i.e., most of the data goes into the training set.\n\n\n\n\n\n\nNote\n\n\n\nThis scheme of splitting time series into the testing and training sets is a simple form of cross-validation. Not all forms of cross-validation are applicable to time series due to the usual temporal dependence in time series data. We need to select cross-validation techniques that can accommodate such dependence. Usually, it implies selecting data for validation not at random but in consecutive chunks (periods) and, ideally, with testing or validation periods being after the training period.\n\n\nForecasting models are then built using only the training set, and used to ‘forecast’ values from the testing set. Sometimes it is called an out-of-sample forecast, because we predict values for the times we have not used for the model specification and estimation. The testing set is used as a set of future observations to compute the PMSE. The PMSE and other errors are computed for each model over the testing set, and then compared to see errors for which models are smaller.\nIf two models produce approximately the same errors, we choose the model that is simpler (involves fewer variables). This is called the law of parsimony.\nThe above error measures (PMSE, PRMSE, PMAE, etc.) compare observed and forecasted data points, hence, are measures of accuracy of the point forecasts. Another way of comparing models could be based on the quality of their interval forecasts, i.e., by assessing how good the prediction intervals are. To assess quality of interval forecasts, one may start by computing the empirical coverage (proportion of data points in the testing set that are within – covered by – corresponding prediction intervals for given confidence \\(C\\), e.g., 95%) and average interval width. Prediction intervals are well-calibrated if empirical coverage is close to \\(C\\) (more important) while intervals are not too wide (less important).\n\n\n\n\n\n\nNote\n\n\n\nTo select the best coverage, one can calculate the absolute differences between the nominal coverage \\(C\\) and each empirical coverage \\(\\hat{C}_m\\): \\[\n\\Delta_m = |C - \\hat{C}_m|.\n\\] Hence, we select the model with the smallest \\(\\Delta_m\\), not the largest coverage \\(C_m\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to obtain different prediction intervals from the same model. For example, you can calculate prediction intervals based on normal and bootstrapped distributions. In this case, point forecasts are the same, but interval forecasts differ.\n\n\nWe may compare a great number of models using the training set, and choose the best one (with smallest errors), however, it would be unfair to use the out-of-sample errors from testing set for demonstrating the model performance because this part of sample was used to select the model. Thus, it is advisable to have one more chunk of the same time series that was not used for model specification, estimation, or selection. Errors of the selected model on this validation set will be closer to the true (genuine) out-of-sample errors and can be used to improve coverage of true out-of-sample forecasts when the model is finally deployed."
  },
  {
    "objectID": "l02_tsintro.html#some-simple-time-series-models",
    "href": "l02_tsintro.html#some-simple-time-series-models",
    "title": "2  Introduction to Time Series Analysis",
    "section": "2.2 Some simple time series models",
    "text": "2.2 Some simple time series models\nRecall that random process is a sequence of random variables, so its model would be the joint distribution of these random variables \\[\nf_1(Y_{t_1}), \\; f_2(Y_{t_1}, Y_{t_2}), \\; f_3(Y_{t_1}, Y_{t_2}, Y_{t_3}), \\ldots\n\\]\nWith sample data, usually we cannot estimate so many parameters. Instead, we use only first- and second-order moments of the joint distributions, i.e., \\(\\mathrm{E}(Y_t)\\) and \\(\\mathrm{E}(Y_tY_{t+h})\\). In the case when all the joint distributions are multivariate normal (MVN), these second-order properties completely describe the sequence (we do not need any other information beyond the first two moments in this case).\n\ni.i.d. noise – independent and identically distributed random variables with zero mean. There is no dependence between observations, at any moment. The joint distribution function is \\[\n\\Pr[Y_1\\leqslant y_1, \\ldots, Y_n\\leqslant y_n] = \\Pr[Y_1\\leqslant y_1]\\cdots \\Pr[Y_n\\leqslant y_n].\n\\] and the forecast is 0 (zero).\nbinary process is a case of i.i.d. process with \\(0 < p < 1\\), and \\[\n\\begin{split}\n\\Pr[X_t=1]& = p, \\\\\n\\Pr[X_t=-1]& = 1-p.\n\\end{split}\n\\]\nrandom walk is a cumulative sum of i.i.d. noise: \\[\nS_t=X_1+X_2+\\ldots +X_t = \\sum_{i=1}^t X_i,\n\\] where \\(t=1,2,\\ldots\\), and \\(X_t\\) is i.i.d. noise.\nwhite noise (WN) is a sequence of uncorrelated random variables, each with zero mean and finite variance \\(\\sigma^2\\). An i.i.d.(0,\\(\\sigma^2\\)) series is also WN(0,\\(\\sigma^2\\)), but not conversely."
  },
  {
    "objectID": "l02_tsintro.html#autocorrelation-function",
    "href": "l02_tsintro.html#autocorrelation-function",
    "title": "2  Introduction to Time Series Analysis",
    "section": "2.3 Autocorrelation function",
    "text": "2.3 Autocorrelation function\nIt is time to give more formal definitions of stationarity. Loosely speaking, a time series \\(X_t\\) (\\(t=0,\\pm 1, \\ldots\\)) is stationary if it has statistical properties similar to those of the time shifted series \\(X_{t+h}\\) for each integer \\(h\\).\nLet \\(X_t\\) be a time series with \\(\\mathrm{E}(X^2_t)<\\infty\\), then the mean function of \\(X_t\\) is \\[\n\\mu_X(t)=\\mathrm{E}(X_t).\n\\]\nThe autocovariance function of \\(X_t\\) is \\[\n\\gamma_X(r,s) = \\mathrm{cov}(X_r,X_s) = \\mathrm{E}[(X_r-\\mu_X(r))(X_s-\\mu_X(s))]\n\\tag{2.5}\\] for all integer \\(r\\) and \\(s\\).\nWeak stationarity\n\\(X_t\\) is (weakly) stationary if \\(\\mu_X(t)\\) is independent of \\(t\\), and \\(\\gamma_X(t+h,t)\\) is independent of \\(t\\) for each \\(h\\) (consider only the first two moments): \\[\n\\begin{split}\n\\mathrm{E}(X_t) &= \\mu, \\\\\n\\mathrm{cov}(X_t, X_{t+h}) &= \\gamma_X(h) < \\infty.\n\\end{split}\n\\tag{2.6}\\]\nStrong stationarity\n\\(X_t\\) is strictly stationary if (\\(X_1, \\ldots, X_n\\)) and (\\(X_{1+h}, \\ldots, X_{n+h}\\)) have the same joint distribution (consider all moments).\nStrictly stationary \\(X_t\\) with finite variance \\(\\mathrm{E}(X_t^2) <\\infty\\) for all \\(t\\) is also weakly stationary.\nIf \\(X_t\\) is a Gaussian process, then strict and weak stationarity are equivalent (i.e., one form of stationarity implies the other).\n\n\n\n\n\n\nNote\n\n\n\nIn applications, it is usually very difficult if not impossible to verify strict stationarity. So in the vast majority of cases we are satisfied with the weak stationarity. Moreover, we usually omit the word ‘weak’ and simply talk about stationarity (but have the weak stationary in mind).\n\n\nIn view of the second condition of weak stationarity in Equation 2.6, we can write for stationary(!) time series the autocovariance function (ACVF) for the lag \\(h\\): \\[\n\\gamma_X(h)= \\gamma_X(t+h,t) = \\mathrm{cov}(X_{t+h},X_t)\n\\] and the autocorrelation function (ACF), which is the normalized autocovariance: \\[\n\\rho_X(h)=\\frac{\\gamma_X(h)}{\\gamma_X(0)}=\\mathrm{cor}(X_{t+h},X_t).\n\\]\nThe sample autocovariance function is defined as: \\[\n\\hat{\\gamma}_X(h)= n^{-1}\\sum_{t=1}^{n-k}(x_{t+h}- \\bar{x})(x_t - \\bar{x}),\n\\] with \\(\\hat{\\gamma}_X(h) = \\hat{\\gamma}_X(-h)\\) for \\(h = 0,1,\\ldots, n-1\\). In R, use acf(X, type = \"covariance\").\nThe sample autocorrelation function is defined as (in R, use acf(X)): \\[\n\\hat{\\rho}_X(h)=\\frac{\\hat{\\gamma}_X(h)}{\\hat{\\gamma}_X(0)}.\n\\]\n\n2.3.1 Properties of autocovariance and autocorrelation functions\n\nLinearity: if \\(\\mathrm{E}(X^2) < \\infty\\), \\(\\mathrm{E}(Y^2) < \\infty\\), \\(\\mathrm{E}(Z^2) < \\infty\\) and \\(a\\), \\(b\\), and \\(c\\) are any real constants, then \\[\n\\mathrm{cov}(aX + bY + c, Z) = a\\mathrm{cov}(X,Z) + b\\mathrm{cov}(Y,Z).\n\\]\n\\(\\gamma(0) \\geqslant 0\\).\n\\(|\\gamma(h)| \\leqslant \\gamma(0)\\) for all \\(h\\).\n\\(\\gamma(\\cdot)\\) is even: \\(\\gamma(h) = \\gamma(-h)\\) for all \\(h\\).\n\\(\\gamma(\\cdot)\\) is non-negative definite: \\[\n\\sum_{i,j=1}^na_i \\gamma(i-j)a_j\\geqslant 0,\n\\] for all positive integers \\(n\\) and vectors \\(\\boldsymbol{a} = (a_1, \\ldots, a_n)^{\\top}\\) with real-valued elements \\(a_i\\).\nThe autocorrelation function \\(\\rho(\\cdot)\\) has all the properties of autocovariance function plus \\(\\rho(0)=1\\).\nFor i.i.d. noise with finite variance, the sample autocorrelations \\(\\hat{\\rho}(h)\\), \\(h>0\\), are approximately \\(N(0,1/n)\\) for large sample size \\(n\\). Hence, approximately 95% of the sample autocorrelations should fall between the bounds \\(\\pm1.96/\\sqrt{n}\\) (1.96 is the 0.975th quantile of the standard normal distribution) – this is the bound automatically drawn by the R function acf(). Note that in R as a rule of thumb the default maximal lag is \\(10 \\log_{10}(n)\\), and the same \\(n\\) is used for the confidence bounds at all lags (however, in reality, samples of different sizes are used for each lag).\n\n\n\n\n\n\n\nExample: Compare ACFs of time series with and without a strong trend\n\n\n\nThe time series plot of births in Figure 2.11 shows a trend. The time series is not stationary. Thus, the calculated ACF is useless (it is calculated under the assumption of stationarity), but we notice some periodicity in the ACF – it is a sign of periodicity in the time series itself (seasonality). Need to remove the trend (and seasonality) and repeat the ACF analysis.\nCompare with the plots for accidental deaths in Figure 2.12.\n\n\nCode\np1 <- ggplot2::autoplot(births) + \n    xlab(\"\") +\n    ylab(\"Number of births (thousands)\") +\n    theme_light()\np2 <- forecast::ggAcf(births) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.11: Time series plot and sample ACF of monthly number of births in New York City, 1946–1959.\n\n\n\n\n\n\nCode\np1 <- ggplot2::autoplot(MASS::accdeaths) + \n    xlab(\"\") +\n    ylab(\"Number of accidental deaths\") +\n    theme_light()\np2 <- forecast::ggAcf(MASS::accdeaths) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.12: Time series plot and sample ACF of monthly totals of accidental deaths in the USA, 1973–1978.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn base-R plots, the lags in ACF plots are measured in the number of periods. The period information (or frequency, i.e., the number of observation per period) is saved in the format ts in R:\n\n\nCode\nis.ts(births)\n\n\n#> [1] TRUE\n\n\nCode\nfrequency(births)\n\n\n#> [1] 12\n\n\nYou don’t have to convert data to this format. If you plot ACF of just a vector without these attributes, the values are the same, just the labels on the horizontal axis are different (Figure 2.13). For monthly data, as in the example here, frequency is 12. Hence, here ACF at lag 0.5 (Figure 2.13 A) means autocorrelation with the lag \\(h=6\\) months (Figure 2.13 B); lag 1 (Figure 2.13 A) corresponds to one whole period of \\(h=12\\) months (Figure 2.13 B), and so on.\n\n\nCode\npar(mar = c(4, 4, 3, 1) + 0.1, mfrow = c(1, 2))\npar(pty = \"m\")\n\nacf(births, lag.max = 37, las = 1, main = \"A) Input is a ts object\")\nacf(as.numeric(births), lag.max = 37, las = 1, main = \"B) Input is a numeric vector\")\n\n\n\n\n\nFigure 2.13: In base-R graphics, the x-axis labels in ACF plots differ depending on the input format.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe ts objects in R cannot incorporate varying periods (e.g., different number of days or weeks because of leap years), therefore, we recommend using this format only for monthly or annual data. Setting frequency = 365.25 for daily data could be an option (although not very accurate) to accommodate leap years.\nConverting to the ts format usually is not required for analysis. Most of the times we use plain numeric vectors and data frames in R.\nContributed R packages offer additional formats for time series, e.g., see packages xts and zoo."
  },
  {
    "objectID": "l02_tsintro.html#partial-autocorrelation-function",
    "href": "l02_tsintro.html#partial-autocorrelation-function",
    "title": "2  Introduction to Time Series Analysis",
    "section": "2.4 Partial autocorrelation function",
    "text": "2.4 Partial autocorrelation function\nShumway and Stoffer (2011) and Shumway and Stoffer (2014) provide the following explanation of the concept.\nRecall that if \\(X\\), \\(Y\\), and \\(Z\\) are random variables, then the partial correlation between \\(X\\) and \\(Y\\) given \\(Z\\) is obtained by regressing \\(X\\) on \\(Z\\) to obtain \\(\\hat{X}\\), regressing \\(Y\\) on \\(Z\\) to obtain \\(\\hat{Y}\\), and then calculating \\[\n\\rho_{XY|Z} = \\mathrm{cor}(X - \\hat{X}, Y - \\hat{Y}).\n\\] The idea is that \\(\\rho_{XY|Z}\\) measures the correlation between \\(X\\) and \\(Y\\) with the linear effect of \\(Z\\) removed (or partialled out). If the variables are multivariate normal, then this definition coincides with \\(\\rho_{XY|Z} = \\mathrm{cor}(X,Y | Z)\\).\nThe partial autocorrelation function (PACF) of a stationary process, \\(X_t\\), denoted \\(\\rho_{hh}\\), for \\(h = 1,2,\\ldots\\), is \\[\n\\rho_{11} = \\mathrm{cor}(X_{t+1}, X_t) = \\rho(1)\n\\] and \\[\n\\rho_{hh} = \\mathrm{cor}(X_{t+h} - \\hat{X}_{t+h}, X_t - \\hat{X}_t), \\; h\\geqslant 2.\n\\]\nBoth \\((X_{t+h} - \\hat{X}_{t+h})\\) and \\((X_t - \\hat{X}_t)\\) are uncorrelated with \\(\\{ X_{t+1}, \\ldots, X_{t+h-1}\\}\\). The PACF, \\(\\rho_{hh}\\), is the correlation between \\(X_{t+h}\\) and \\(X_t\\) with the linear dependence of everything between them, namely \\(\\{ X_{t+1}, \\ldots, X_{t+h-1}\\}\\), on each, removed.\nTo obtain sample estimates of PACF in R, use pacf(X) or acf(X, type = \"partial\").\nCorrelation and partial correlation coefficients measure strength and direction of the relationship, changing within \\([-1, 1]\\). Percent of explained variance for the case of two variables is measured by squared correlation (r-squared; \\(R^2\\); coefficient of determination) changing within \\([0, 1]\\), so correlation of 0.2 means only 4% of variance explained by the simple linear relationship (regression). To report a partial correlation, for example, of 0.2 at lag 3, one could say something like ‘The partial autocorrelation at lag 3 (after removing influence of the intermediate lags 1 and 2) is 0.2’ (depending on the application, the correlation of 0.2 can be considered weak or moderate strength) or ‘After accounting for autocorrelation at intermediate lags 1 and 2, the linear relationship at lag 3 can explain 4% of the remaining variability.’\n\n\n\n\n\n\nExample: ACF and PACF of accdeaths\n\n\n\nBy plotting ACF and PACF (Figure 2.14), we observe that most of the temporal dependence (autocorrelation) in this time series is due to correlation with the immediately preceding values (see PACF at lag 1).\n\n\nCode\np1 <- forecast::ggAcf(MASS::accdeaths) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np2 <- forecast::ggAcf(MASS::accdeaths, type = \"partial\") + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 2.14: ACF and PACF plots of the monthly accidental deaths."
  },
  {
    "objectID": "l02_tsintro.html#conclusion",
    "href": "l02_tsintro.html#conclusion",
    "title": "2  Introduction to Time Series Analysis",
    "section": "2.5 Conclusion",
    "text": "2.5 Conclusion\nWe have defined components of the time series including trend, periodic component, and unexplained variability (errors, residuals). Our goal will be to model trends and periodicity, extract them from the time series, then extract as much information as possible from the remainder, so the ultimate residuals become white noise.\nWhite noise is a sequence of uncorrelated random variables with zero mean and finite variance. White noise is also an example of a weakly stationary time series. The i.i.d. noise is strictly stationary (all moments of the distribution stay identical through time), and i.i.d. noise with finite variance is also white noise.\nTime-series dependence can be quantified using a (partial) autocorrelation function. We defined (P)ACF for stationary series; R functions also assume stationarity of the time series when calculating ACF or PACF.\nAfter developing several models for modeling and forecasting time series, we can compare them quantitatively in cross-validation. For time series, it is typical to have the testing set (or a validation fold) to be after the training set. Models can be compared based on the accuracy of their point forecasts and interval forecasts.\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton\n\n\nShumway RH, Stoffer DS (2011) Time series analysis and its applications with r examples, 3rd edn. Springer, New York\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications with r examples, 3-EZ. Free Texts in Statistics"
  },
  {
    "objectID": "l03_smoothing.html",
    "href": "l03_smoothing.html",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "",
    "text": "The goal of this lecture is to learn a variety of methods used for trend visualization (such that make a trend in noisy data more apparent), trend modeling (such that can help us to come up with ‘an equation’ for the trend, which could be further studied or used for interpolation, forecasting, or detrending), and detrending (just to remove trend from the series, with or without modeling it). You should be able to choose a method based on the goals of your analysis. We will start with simple and widely used methods that can be further combined into more elaborate algorithms.\nObjectives\nReading materials"
  },
  {
    "objectID": "l03_smoothing.html#introduction",
    "href": "l03_smoothing.html#introduction",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nRecall the classical decomposition \\[\nY_t = M_t + S_t + \\epsilon_t,\n\\tag{3.1}\\] where \\(M_t\\) is slowly changing function (trend component), \\(\\epsilon_t\\) is stationary random noise component, and \\(S_t\\) is the periodic term with period \\(m\\geqslant 2\\) (seasonal component) and scaling factors \\(\\lambda_k > 0\\) such that \\(S_{t+km} = \\lambda_kS_t\\) for \\(1 \\leqslant t \\leqslant m\\) and each \\(k \\geqslant 1\\). For identification, we need \\(\\sum_{t=1}^m S_t = 0\\) and \\(\\mathrm{E}(\\epsilon_t)=0\\).\nOur goal is to estimate and extract \\(M_t\\) and \\(S_t\\) with a hope that the residual or noise component \\(\\epsilon_t\\) will turn out to be a stationary time series (Section 3.2–Section 3.5). Alternatively, Box–Jenkins models use difference operators repeatedly to the series \\(Y_t\\) until a stationary time series is obtained (see Section 3.6 on differencing).\nThe seasonal component is said to have constant seasonal variation if the scaling factors satisfy \\(\\lambda_{k} = 1\\) for all \\(k\\). In other words, \\(S_{t+m} = S_{t}\\) for all \\(t\\). The constant variation is an assumption of most modeling techniques we will be using in this course. Unfortunately, many real seasonal time series do not have this constancy property and it is necessary to first perform a variance-stabilizing transformation to modify the original time series into one with constant variation. To some extent this is a matter of trial-and-error, where we work from ‘weaker’ transformations to ‘stronger’ transformations as needed. Typically, power transformations are used \\(Y_{t} \\rightarrow Y^{\\lambda}_{t}\\), where \\(0 < \\lambda < 1\\) or, log or even log log transformations, e.g., \\(Y_{t} \\rightarrow \\log Y_{t}\\).\nThe log transformation of \\(Y_t\\) is convenient when we assume not an additive model as Equation 3.1 is, but a multiplicative model as \\[\nY_t = M_t \\cdot S_t \\cdot \\epsilon_t.\n\\tag{3.2}\\] When applying variance-stabilizing log transformation to Equation 3.2, we get an additive result: \\[\n\\log Y_t = \\log M_t + \\log S_t + \\log \\epsilon_t,\n\\tag{3.3}\\] which is now similar to Equation 3.1. We will refer to Equation 3.1 as additive seasonality, and to Equation 3.2 as multiplicative seasonality. Only few methods can deal with the multiplicative case Equation 3.2 directly; most methods we consider in this course will require us to apply a transformation as in Equation 3.3.\nIn the following sections, we consider methods for estimation and elimination of \\(M_t\\) (for non-seasonal data, when \\(S_t = 0\\) in the additive case Equation 3.1 or \\(S_t = 1\\) in the multiplicative case Equation 3.2) and of both \\(M_t\\) and \\(S_t\\) (for seasonal data)."
  },
  {
    "objectID": "l03_smoothing.html#sec-movavg",
    "href": "l03_smoothing.html#sec-movavg",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.2 Finite moving average smoothing",
    "text": "3.2 Finite moving average smoothing\nNon-seasonal model with trend: \\[\nY_t = M_t + \\epsilon_t,\n\\] where \\(\\mathrm{E}(\\epsilon_t)=0\\) and \\(t=1,\\ldots,n\\).\nLet \\(q\\) be a non-negative integer (\\(q \\in \\mathbb{N}^{+}\\)) and consider the two-sided moving average of the series \\(Y_t\\): \\[\n\\begin{split}\nW_t &= \\frac{1}{2q+1}\\sum_{j=-q}^q Y_{t-j}\\\\\n&= \\frac{1}{2q+1}\\sum_{j=-q}^q M_{t-j} + \\frac{1}{2q+1}\\sum_{j=-q}^q \\epsilon_{t-j} \\\\\n&\\approx M_t,\n\\end{split}\n\\tag{3.4}\\] where \\(w = 2q+1\\) is size of the moving window. Note that the above approximation is correct if the average value of \\(\\epsilon_{t}\\) within each window is close to 0 (important for selecting \\(q\\)).\n\n\n\n\n\n\nExample: Shampoo plots\n\n\n\nFigure 3.1 shows a time series plot of monthly shampoo sales. While the data are monthly, the seasonal component is not visible:\n\nno strong periodicity in the time series plot;\nthe ACF at seasonal lag (1 year) is not significant.\n\nTherefore, we apply Equation 3.4 as to non-seasonal data.\n\n\nCode\n# Get the data from the package fma\nshampoo <- fma::shampoo\nshampoo\n\n\n#>   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#> 1 266 146 183 119 180 168 232 224 193 123 336 186\n#> 2 194 150 210 273 191 287 226 304 290 422 264 342\n#> 3 340 440 316 439 401 437 576 408 682 475 581 647\n\n\n\n\nCode\npshampoo <- ggplot2::autoplot(shampoo, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Sales\") + \n    theme_light()\np2 <- forecast::ggAcf(shampoo) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\npshampoo + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.1: Monthly shampoo sales over three years and a corresponding sample ACF.\n\n\n\n\n\n\nThere are multiple ways to calculate moving averages in R, and formulas for \\(W_t\\) may vary. If using someone’s else functions, remember to read the help files or source code to find out the exact formula in place of Equation 3.4. Below are some examples. Note that the package forecast is now retired in favor of the package fable.\n\n\n\n\n\n\nNote\n\n\n\nThe package forecast is now retired in favor of the package fable.\n\n\nFor example, forecast::ma() has the default option centre = TRUE, and the order argument represents the window size. Odd order and centre = TRUE correspond exactly to our definitions in Equation 3.4:\n\n\nCode\nforecast::ma(shampoo, order = 5)\n\n\n#>   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#> 1  NA  NA 179 159 177 185 200 188 222 213 206 198\n#> 2 215 203 204 222 238 256 260 306 301 324 332 362\n#> 3 341 376 387 407 434 452 501 516 544 559  NA  NA\n\n\nFor example, pracma::movavg() averages last n data points:\n\n\nCode\npracma::movavg(shampoo, n = 5)\n\n\n#>  [1] 266 206 198 179 179 159 177 185 200 188 222 213 206 198 215 203 204 222 238\n#> [20] 256 260 306 301 324 332 362 341 376 387 407 434 452 501 516 544 559\n\n\nThe base-R function stats::filter() can also be used. Its odd window size and sides = 2 correspond to our definitions in Equation 3.4:\n\n\nCode\nwindow_size <- 5\nstats::filter(shampoo, filter = 1/rep(window_size, window_size), sides = 2)\n\n\n#>   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#> 1  NA  NA 179 159 177 185 200 188 222 213 206 198\n#> 2 215 203 204 222 238 256 260 306 301 324 332 362\n#> 3 341 376 387 407 434 452 501 516 544 559  NA  NA\n\n\nThe sides = 1 in stats::filter() corresponds to the pracma::movavg() results above:\n\n\nCode\nstats::filter(shampoo, filter = 1/rep(window_size, window_size), sides = 1)\n\n\n#>   Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n#> 1  NA  NA  NA  NA 179 159 177 185 200 188 222 213\n#> 2 206 198 215 203 204 222 238 256 260 306 301 324\n#> 3 332 362 341 376 387 407 434 452 501 516 544 559\n\n\nSee Figure 3.2 and Figure 3.3 for the effects of the window size.\n\n\n\n\n\n\nExample: Shampoo moving average smoothing\n\n\n\n\n\nCode\nps <- lapply(c(1, 2, 3, 5), function(q){\n    w <- 2 * q + 1\n    Wt <- forecast::ma(shampoo, order = w)\n    pshampoo + \n        geom_line(aes(y = Wt), col = 4, lwd = 1.5) + \n        ggtitle(paste0(\"q = \", q, \" (window = \", w, \")\"))\n})\nwrap_plots(ps, ncol = 2) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.2: Shampoo sales over three years smoothed with centered moving average filters, where the window size is \\(w = 2q+1\\).\n\n\n\n\n\n\nCode\nps <- lapply(c(1, 2, 3, 5), function(q){\n    w <- 2 * q + 1\n    Wt <- stats::filter(shampoo, filter = 1/rep(w, w), sides = 1)\n    pshampoo + \n        geom_line(aes(y = Wt), col = 4, lwd = 1.5) + \n        ggtitle(paste0(\"q = \", q, \" (window = \", w, \")\"))\n})\nwrap_plots(ps, ncol = 2) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.3: Shampoo sales over three years smoothed with non-centered moving average filters, where the window size is \\(w = 2q+1\\).\n\n\n\n\n\n\n\n3.2.1 Moving average smoothing for seasonal data\nCome back to the trend-seasonal model Equation 3.1: \\[\nY_t = M_t + S_t + \\epsilon_t.\n\\]\nWe apply the following steps for estimating seasonality and trend:\n\nApply a moving average filter specially chosen to eliminate the seasonal component and dampen the noise – get \\(\\hat{M}_t\\). Remember that sum of \\(S_t\\) within each period is 0. Hence, to smooth out the seasonal component (and noise) and get an estimate of just \\(M_t\\), we will use a moving window \\(w\\) sized as the seasonal period \\(m\\) (more generally, \\(w = km\\), where \\(k\\in \\mathbb{N}^{+}\\), if a greater smoothness is desired). It will often lead to the window size \\(w\\) being even. For even \\(w\\), Equation 3.4 is modified as follows (we now use \\(km + 1\\) elements so the window is still centered, but the elements on the ends receive half the weight): \\[\n\\hat{M}_t = \\frac{0.5Y_{t-km/2} + Y_{t-km/2 + 1} +\\ldots+Y_{t+km/2-1} + 0.5Y_{t+km/2}}{km}.\n\\tag{3.5}\\]\nUse \\(Y_t - \\hat{M}_t\\) to estimate the seasonal component – get \\(\\hat{S}_t\\). If needed, correct the estimates to sum up to 0 in each period. Let \\(\\hat{S}^*_t\\) be the corrected values.\nUse deseasonalized data \\(Y_t - \\hat{S}^*_t\\) to re-estimate the trend. Let \\(\\hat{M}^*_t\\) be the corrected trend estimate.\nThe estimated random noise is then: \\(\\hat{\\epsilon}_t = Y_t - \\hat{M}^*_t - \\hat{S}^*_t\\).\n\n\n\n\n\n\n\nNote\n\n\n\nFor the multiplicative case, replace subtractions with divisions, let the product of \\(\\hat{S}^*_t\\) be 1 within each seasonal period.\n\n\nThe above algorithm is implemented in stats::decompose(), but its main disadvantage is in the centered moving average filter used to estimate \\(M_t\\) that does not produce smoothed values for the most recent observations. We can elaborate this algorithm, first, by using different estimators for \\(M_t\\); second, by also replacing estimators for \\(S_t\\) (e.g., see the next section).\n\n\n\n\n\n\nExample: Secchi automatic (simple moving average-based) decomposition\n\n\n\nConsider monthly data of Secchi disk depth measured at the station CB1.1 in Chesapeake Bay (Figure 3.4), with the decomposition in Figure 3.5.\n\n\nCode\nload(\"./data/Secchi_CB1.1.RData\")\npSecchi <- ggplot2::autoplot(Secchi, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Secchi depth (m)\") + \n    theme_light()\np2 <- forecast::ggAcf(Secchi) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\npSecchi + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.4: Monthly average Secchi disk depth at station CB1.1 and its ACF.\n\n\n\n\n\n\nCode\nggplot2::autoplot(stats::decompose(Secchi))  + \n    theme_light()\n\n\n\n\n\nFigure 3.5: Trend-seasonal decomposition of monthly average Secchi disk depth at station CB1.1."
  },
  {
    "objectID": "l03_smoothing.html#lowess-smoothing",
    "href": "l03_smoothing.html#lowess-smoothing",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.3 Lowess smoothing",
    "text": "3.3 Lowess smoothing\nOne of the alternatives for estimating trends \\(M_t\\) is locally weighted regression (shortened as ‘lowess’ or ‘loess’) (Cleveland 1979). We can apply lowess as a regression of \\(Y_t\\) on time, for example, using the algorithm outlined by Berk (2016):\n\nChoose the smoothing parameter such as bandwidth, \\(f\\), which is a proportion between 0 and 1.\nChoose a point \\(t_0\\) and its \\(w = f n\\) nearest points on the time axis.\nFor these \\(w\\) nearest neighbor points, compute a weighted least squares regression line for \\(Y_t\\) on \\(t\\). The coefficients \\(\\boldsymbol{\\beta}\\) of such regression are estimated by minimizing the residual sum of squares \\[\n\\text{RSS}^*(\\boldsymbol{\\beta}) = (\\boldsymbol{Y}^* - \\boldsymbol{X}^* \\boldsymbol{\\beta})^{\\top} \\boldsymbol{W}^* (\\boldsymbol{Y}^* - \\boldsymbol{X}^* \\boldsymbol{\\beta}),\n\\] where the asterisk indicates that only the observations in the window are included. The regressor matrix \\(\\boldsymbol{X}^*\\) can contain polynomial terms of time, \\(t\\). The \\(\\boldsymbol{W}^*\\) is a diagonal matrix conforming to \\(\\boldsymbol{X}^*\\), with diagonal elements being a function of distance from \\(t_0\\) (observations closer to \\(t_0\\) receive higher weights).\nCalculate the fitted value \\(\\tilde{Y}_t\\) for that single \\(t_0\\).\nRepeat Steps 2–4 for each \\(t_0 = 1,\\ldots,n\\).\n\nBy selecting large bandwidth \\(f\\) in the lowess algorithm, we can obtain greater smoothing (see Figure 3.6).\n\n\nCode\ndata(WWWusage)\nps <- lapply(c(0.1, 0.3, 0.5, 0.75), function(w){\n    ggplot2::autoplot(WWWusage, colour = \"grey50\")  + \n        ggtitle(paste0(\"span = \", w)) +\n        theme_light() + \n        geom_smooth(method = \"loess\", span = w, se = FALSE)\n})\nwrap_plots(ps, ncol = 2) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.6: A lowess illustration, adapted from Berk (2016).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function ggplot2::geom_smooth() has the default setting se = TRUE that produces a confidence interval around the smooth. In time series applications, autocorrelated residuals may lead to underestimated standard errors and incorrect (typically, too narrow) confidence intervals. Hence, we set se = FALSE. For testing significance of the identified trends, see lectures on trend testing (detection).\n\n\n\n3.3.1 Lowess smoothing for seasonal data\nNow use the Secchi time series (Figure 3.4) and implement the procedure of detrending and deseasonalizing from Section 3.2.1, but replace the simple moving average estimates of \\(M_t\\) with lowess estimates.\n\n\n\n\n\n\nExample: Secchi lowess-based decomposition\n\n\n\n\n\nCode\nYt <- Secchi\nt <- as.vector(time(Yt))\nmonth <- as.factor(cycle(Yt))\n# 1. Initial trend estimate\nMt <- loess(Yt ~ t, span = 0.25)$fitted\n# 2. Estimate of a cycle, corrected to sum up to 0\nSt <- tapply(Yt - Mt, month, mean)\nSt\n\n\n#>       1       2       3       4       5       6       7       8       9      10 \n#> -0.0359  0.0457 -0.2483 -0.2683 -0.0716 -0.0129  0.1411  0.2024  0.1276  0.1108 \n#>      11      12 \n#>  0.0391 -0.0165\n\n\nCode\nsum(St)\n\n\n#> [1] 0.0131\n\n\nCode\nSt_star <- St - sum(St)/12\nsum(St_star)\n\n\n#> [1] 3.47e-18\n\n\nCode\n# 3. Refine trend estimate\nMt_star <- loess((Yt - St_star[month]) ~ t, span = 0.25)$fitted\n# 4. Noise\net <- Yt - Mt_star - St_star[month]\n# Convert back to ts format for plotting\net <- ts(as.vector(et), start = start(Secchi), frequency = 12)\n\n\nSee Figure 3.7 for the results.\n\n\nCode\np1 <- pSecchi + \n    geom_line(aes(y = Mt_star), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 <- pSecchi + \n    geom_line(aes(y = Mt_star + St_star[month]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 <- ggplot2::autoplot(et) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\") +\n    theme_light()\np4 <- forecast::ggAcf(et) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") +\n    theme_light()\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.7: Detrending using lowess and deseasonalizing the Secchi depth data.\n\n\n\n\nIn Figure 3.8, we use the decomposition function stl() forced to have approximately the same values as we used in the steps above and as shown in Figure 3.7. Hence, Figure 3.7 and Figure 3.8 are similar. However, the function stl() is more flexible – it also automatically smooths the seasonal component (across Januaries, across Februaries, etc.) – and can provide finer estimates (see Figure 3.9).\n\n\nCode\n# Span we used above (the fraction)\nspan = 0.25 \n# Window size in number of lags (observations)\nw <- span * length(Secchi)/2 \nD <- stl(Yt, s.window = \"periodic\", t.window = w)\np1 <- pSecchi +\n    geom_line(aes(y = D$time.series[,\"trend\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 <- pSecchi + \n    geom_line(aes(y = D$time.series[,\"trend\"] + D$time.series[,\"seasonal\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 <- ggplot2::autoplot(D$time.series[,\"remainder\"]) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\") +\n    theme_light()\np4 <- forecast::ggAcf(D$time.series[,\"remainder\"]) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") +\n    theme_light()\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.8: Detrending and deseasonalizing the Secchi depth data using lowess within the function stl() forced to behave similarly to Figure 3.7.\n\n\n\n\n\n\nCode\nD <- stl(Yt, s.window = 24, s.degree = 1, t.window = w)\np1 <- pSecchi +\n    geom_line(aes(y = D$time.series[,\"trend\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Trend estimate: Mt_star\")\np2 <- pSecchi + \n    geom_line(aes(y = D$time.series[,\"trend\"] + D$time.series[,\"seasonal\"]), col = 4, lwd = 1.5) + \n    ggtitle(\"Final trend-cycle: Mt_star + St_star\")\np3 <- ggplot2::autoplot(D$time.series[,\"remainder\"]) + \n    xlab(\"Year\") + \n    ggtitle(\"Residuals: et\") +\n    theme_light()\np4 <- forecast::ggAcf(D$time.series[,\"remainder\"]) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") +\n    theme_light()\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.9: Detrending and deseasonalizing the Secchi depth data using lowess within the function stl() with default settings."
  },
  {
    "objectID": "l03_smoothing.html#exponential-smoothing",
    "href": "l03_smoothing.html#exponential-smoothing",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.4 Exponential smoothing",
    "text": "3.4 Exponential smoothing\nExponential smoothing (ES) is a successful forecasting technique. It turns out that ES can be modified to be used effectively for time series with\n\nslowly drifting trend (double exponential smoothing);\ntrends (Holt’s method);\nseasonal patterns;\ncombination of trend and seasonality (Holt–Winters method).\n\nES is easy to adjust for past errors and easy to prepare follow-on forecasts. ES is ideal for situations where many forecasts must be prepared. Several different functional forms of ES are used depending on presence of trend or cyclical variations.\nIn short, an ES is an averaging technique that uses unequal weights while the weights applied to past observations decline in an exponential manner.\nSingle exponential smoothing calculates the smoothed series as a damping coefficient \\(\\alpha\\) (\\(\\alpha \\in [0, 1]\\)) times the actual series plus \\(1 - \\alpha\\) times the lagged value of the smoothed series. For the model \\[\nY_{t} = M_t + \\epsilon_{t},\n\\]\nthe updating equation is \\[\n\\begin{split}\n\\hat{M}_1 &= Y_1\\\\\n\\hat{M}_t &= \\alpha Y_t + (1 - \\alpha) \\hat{M}_{t - 1}\n\\end{split}\n\\]\nand forecast is \\[\n\\hat{Y}_{t+1} = \\hat{M}_t.\n\\]\nAn exponential smoothing over an already smoothed time series is called double exponential smoothing. In some cases, it might be necessary to extend it even to a triple exponential smoothing.\nHolt’s linear exponential smoothing Suppose that the series \\(Y_t\\) is non-seasonal but displays a trend. Now we need to estimate both the current mean (or level in some professional jargon) and the current trend.\nThe updating equations express ideas similar to those for exponential smoothing. However, now we have two smoothing parameters, \\(\\alpha\\) and \\(\\beta\\) (\\(\\alpha \\in [0, 1]\\); \\(\\beta \\in [0, 1]\\)).\nThe updating equations are \\[\na_{t} = \\alpha Y_{t} + \\left( 1- \\alpha \\right) \\left( a_{t - 1} + b_{t - 1} \\right)\n\\] for the mean and \\[\nb_{t} = \\beta \\left( a_{t} - a_{t-1} \\right) + \\left( 1 - \\beta \\right) b_{t-1}\n\\] for the trend.\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = a_{t} + kb_{t}.\n\\]\nUsually, the initial (starting) values are \\[\n\\begin{split}\na_{1} & = Y_{2}, \\\\\nb_{1} & = Y_{2} - Y_{1}.\n\\end{split}\n\\]\n\n\n\n\n\n\nExample: Shampoo exponential smoothing\n\n\n\nSee an example in Figure 3.10.\n\n\nCode\nclrs <- c(\"0.3\" = 4, \"0.7\" = 2)\nm1 <- HoltWinters(shampoo, alpha = 0.3, beta = FALSE, gamma = FALSE)\nm2 <- HoltWinters(shampoo, alpha = 0.7, beta = FALSE, gamma = FALSE)\np1 <- pshampoo + \n    geom_line(data = fitted(m1)[,1], aes(col = \"0.3\"), lwd = 1.5) + \n    geom_line(data = fitted(m2)[,1], aes(col = \"0.7\"), lwd = 1.5) +\n    ggtitle(\"Simple exponential smoothing\") +\n    labs(color = \"\\u03b1\") +\n    scale_color_manual(values = clrs)\nclrs <- c(\"(0.3, 0.3)\" = 4, \"(0.7, 0.7)\" = 2)\nm1 <- HoltWinters(shampoo, alpha = 0.3, beta = 0.3, gamma = FALSE)\nm2 <- HoltWinters(shampoo, alpha = 0.7, beta = 0.7, gamma = FALSE)\np2 <- pshampoo + \n    geom_line(data = fitted(m1)[,1], aes(col = \"(0.3, 0.3)\"), lwd = 1.5) + \n    geom_line(data = fitted(m2)[,1], aes(col = \"(0.7, 0.7)\"), lwd = 1.5) +\n    ggtitle(\"Holt's method\") +\n    labs(color = \"(\\u03b1, \\u03b2)\") +\n    scale_color_manual(values = clrs)\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.10: Shampoo sales over three years smoothed with different exponential smoothing filters.\n\n\n\n\n\n\n\n3.4.1 Exponential smoothing for seasonal data\nMultiplicative Holt–Winters procedure\nNow in addition to the Holt parameters, suppose that the series exhibits multiplicative seasonality and let \\(S_{t}\\) be the multiplicative seasonal factor at time \\(t\\).\nSuppose also that there are \\(m\\) observations in one period (in a year). For example, \\(m = 4\\) for quarterly data and \\(m = 12\\) for monthly data.\nIn some time series, seasonal variation is so strong it obscures any trends or other cycles, which are important for understanding of the observed process. Holt–Winters smoothing method can remove seasonality and makes long-term fluctuations in the series stand out more clearly.\nA simple way of detecting trend in seasonal data is to take averages over a certain period. If these averages change with time we can say that there is evidence of a trend in the series.\nWe now use three smoothing parameters: \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) (\\(\\alpha \\in [0, 1]\\); \\(\\beta \\in [0, 1]\\); \\(\\gamma \\in [0, 1]\\)).\nThe updating equations for the level (\\(a_t\\)), local trend (\\(b_t\\)), and seasonal factor (\\(S_t\\)) are: \\[\n\\begin{split}\na_{t} & = \\alpha Y_{t} / S_{t - m} + (1 - \\alpha) ( a_{t - 1} + b_{t - 1}), \\\\\nb_{t} & = \\beta (a_{t} - a_{t - 1}) + (1 - \\beta) b_{t - 1}, \\\\\nS_{t} & = \\gamma Y_{t} / a_{t} + (1 - \\gamma) S_{t - m}.\n\\end{split}\n\\]\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = (a_{t} + kb_{t}) S_{t+k-m},\n\\] where \\(k = 1, 2, \\ldots, m\\).\nTo obtain starting values, one may use an average over the first few periods (years) of the data.\nThe smoothing parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\gamma\\) are estimated by minimizing the sum of the squared one-step prediction errors.\nAdditive Holt–Winters procedure\nThe updating equations are \\[\n\\begin{split}\na_{t} & = \\alpha (Y_{t} - S_{t - m}) + (1 - \\alpha) (a_{t - 1} + b_{t - 1}), \\\\\nb_{t} & = \\beta (a_{t} - a_{t - 1}) + (1 - \\beta) b_{t - 1} \\\\\nS_{t} & = \\gamma (Y_{t} - a_{t}) + (1 - \\gamma) S_{t - m}.\n\\end{split}\n\\]\nThen the forecasting for \\(k\\) steps into the future is \\[\n\\hat{Y}_{t+k} = a_{t} + kb_{t} + S_{t + k - m},\n\\] where \\(k = 1, 2, \\ldots, m\\).\n\n\n\n\n\n\nExample: Airline passengers non-seasonal and seasonal exponential smoothing\n\n\n\nMonthly totals of international airline passengers, 1949–1960 (see Figure 3.11). This is a classical example and benchmark in time series analysis.\n\n\nCode\npAirPassengers <- ggplot2::autoplot(AirPassengers, col = \"grey50\") + \n    xlab(\"Year\") + \n    ylab(\"Airline passengers (thousand)\") + \n    theme_light()\np2 <- forecast::ggAcf(AirPassengers) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\npAirPassengers + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.11: Monthly AirPassengers data and a corresponding sample ACF.\n\n\n\n\nComparison of various exponential smoothing techniques for the AirPassengers data:\n\n\nCode\nm1 <- HoltWinters(AirPassengers, beta = FALSE, gamma = FALSE)\nm1$SSE\n\n\n#> [1] 162511\n\n\nCode\nc(m1$alpha, m1$beta, m1$gamma)\n\n\n#> [1] 1 0 0\n\n\nCode\nm2 <- HoltWinters(AirPassengers, gamma = FALSE)\nm2$SSE\n\n\n#> [1] 163634\n\n\nCode\nc(m2$alpha, m2$beta, m2$gamma)\n\n\n#>   alpha    beta         \n#> 1.00000 0.00322 0.00000\n\n\nCode\nm3 <- HoltWinters(AirPassengers)\nm3$SSE\n\n\n#> [1] 21860\n\n\nCode\nc(m3$alpha, m3$beta, m3$gamma)\n\n\n#>  alpha   beta  gamma \n#> 0.2480 0.0345 1.0000\n\n\nCode\nm4 <- HoltWinters(AirPassengers, seasonal = \"multiplicative\")\nm4$SSE\n\n\n#> [1] 16571\n\n\nCode\nc(m4$alpha, m4$beta, m4$gamma)\n\n\n#>  alpha   beta  gamma \n#> 0.2756 0.0327 0.8707\n\n\nThe last multiplicative model is the best one, based on the sum of squared errors (SSE) in the training set (see Figure 3.12). For a more thorough comparison, consider using out-of-sample data in a cross-validation.\n\n\nCode\nk = 12\nfm1 <- predict(m1, n.ahead = k)\nfm2 <- predict(m2, n.ahead = k)\nfm3 <- predict(m3, n.ahead = k)\nfm4 <- predict(m4, n.ahead = k)\np1 <- pAirPassengers + \n    geom_line(data = m1$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm1) +\n    ggtitle(\"Exponential smoothing\") +\n    theme(legend.position = \"none\")\np2 <- pAirPassengers + \n    geom_line(data = m2$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm2) +\n    ggtitle(\"Holt's method\") +\n    theme(legend.position = \"none\")\np3 <- pAirPassengers + \n    geom_line(data = m3$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm3) +\n    ggtitle(\"Additive Holt-Winters\") +\n    theme(legend.position = \"none\")\np4 <- pAirPassengers + \n    geom_line(data = m4$fitted[,\"xhat\"], col = 4) +\n    autolayer(fm4) +\n    ggtitle(\"Multiplicative Holt-Winters\") +\n    theme(legend.position = \"none\")\n(p1 + p2) / (p3 + p4) + \n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.12: Plots of the observed, fitted, and predicted AirPassengers data, using various smoothing procedures."
  },
  {
    "objectID": "l03_smoothing.html#sec-regressionTime",
    "href": "l03_smoothing.html#sec-regressionTime",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.5 Polynomial regression on time",
    "text": "3.5 Polynomial regression on time\nThis is a very intuitive procedure for fitting parametric trend functions to time series:\n\nMake a parametric model assumption about \\(M_t\\) as a function of time \\(t\\), for example, quadratic trend: \\[\nM_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2\n\\]\nFit the model using the usual regression estimators (least squares or maximum likelihood)\n\n\n\n\n\n\n\nNote\n\n\n\nA nonstationary time series with a trend represented by a parametric function is a typical example of a trend-stationary time series or a time series with a deterministic trend. It is easy to make such a time series stationary by modeling and extracting the trend.\n\n\n\n\n\n\n\n\nExample: Airline passengers non-seasonal polynomial smoothing\n\n\n\nContinue using the AirPassengers data and fit linear and quadratic trends.\n\n\nCode\nt <- as.vector(time(AirPassengers))\nt2 <- t^2\ntm1 <- lm(AirPassengers ~ t)\ntm2.1 <- lm(AirPassengers ~ t + t2)\ntm2.2 <- lm(AirPassengers ~ t + I(t^2))\ntm2.3 <- lm(AirPassengers ~ poly(t, degree = 2))\n\n\nAfter estimating the trend coefficients with OLS, visualize the results (Figure 3.13).\n\n\nCode\np1 <- pAirPassengers + \n    geom_line(aes(y = tm1$fitted.values), col = 4, lwd = 1.5) + \n    ggtitle(\"Linear trend\")\np2 <- pAirPassengers + \n    geom_line(aes(y = tm2.3$fitted.values), col = 4, lwd = 1.5) + \n    ggtitle(\"Quadratic trend\")\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.13: The plots of the AirPassengers data with estimated parametric linear and quadratic trends.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe models tm2.1 and tm2.2 are equivalent. Model tm2.1 uses a pre-calculated quadratic transformation, while model tm2.2 applies the transformation on-the-fly within the R formula call, using the I() syntax (without the I() wrapper, the output of tm2.2 would be the same as of tm1, not tm2.1). However, both models tm2.1 and tm2.2 can easily violate the assumption of independence of predictors in linear regression, especially when values of the time index \\(t\\) are large. That is the case for us, because \\(t\\) represents decimal years, from 1949 to 1960.917, and \\(\\widehat{\\mathrm{cor}}(t, t^2) =\\) 1. Therefore, model tm2.3 is preferred, because it is evaluated on orthogonal polynomials (basically, centering and normalization is applied to \\(t\\) and \\(t^2\\)).\n\n\n\n3.5.1 Seasonal regression with dummy variables\nTo each time \\(t\\) and each season \\(k\\) we will assign an indicator which ‘turns on’ if and only if \\(t\\) falls into that season. In principle, a seasonal period of any positive integer length \\(m \\geqslant 2\\) is possible, however, in most cases, the length of the seasonal cycle is one year and so the seasonal period of the time series depends on the number of observations per year. The most common periods are \\(m = 12\\) (monthly data) and \\(m = 4\\) (quarterly data). For each \\(k = 1, \\ldots, m\\), we define the indicator \\[\nX_{k,t} = \\left\\{\n\\begin{array}{cl}\n1, & \\mbox{if} ~ t ~ \\text{corresponds to season} ~ k, \\\\\n0, & \\mbox{if} ~ t ~ \\text{does not correspond to season} ~ k. \\\\\n\\end{array}\n\\right.\n\\]\nNote that for each \\(t\\) we have \\(X_{1,t} + X_{2,t} + \\ldots + X_{m,t} = 1\\) since each \\(t\\) corresponds to exactly one season. Thus, given any \\(m - 1\\) of the variables, the remaining variable is known and thus redundant. Because of this linear dependence, we must drop one of the indicator variables (seasons) from the model. It does not matter which season is dropped although sometimes there are choices which are simpler from the point of view of constructing or labeling the design matrix. Thus the general form of a seasonal model looks like \\[\n\\begin{split}\nf (Y_{t}) &= M_{t} + \\beta_{2} X_{2,t} + \\beta_{3} X_{3,t} + \\ldots + \\beta_{m} X_{m,t} + \\epsilon_{t} \\\\\n&= M_{t} + \\sum_{i=2}^{m}\\beta_{i} X_{i,t} + \\epsilon_{t},\n\\end{split}\n\\] where \\(M_{t}\\) is a trend term which may also include some \\(\\beta\\) parameters and explanatory variables. The function \\(f\\) represents the appropriate variance-stabilizing transformations as needed. (Here the 1st season has been dropped from the model.)\n\n\n\n\n\n\nExample: Airline passengers polynomial smoothing with dummy variables for the seasons\n\n\n\nConsider adding the dummy variables to model seasonality additionally to the quadratic trend in the AirPassengers time series. We noticed multiplicative seasonality in this time series before, therefore we apply logarithmic transformation to the original data to transform the multiplicative seasonality into additive. That is, we need to estimate the model \\[\n\\ln(Y_{t}) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\sum_{i=2}^{12}\\beta_{i} X_{i,t} + \\epsilon_{t},\n\\] where \\(X_{i,t}\\) are the dummy variables for the months. (Here the first month has been dropped from the model, the same as in R the first factor level is dropped.)\nIf we were doing the regression analysis by hand, we would create a design matrix with few top rows looking like this:\n\n\n#>          t      t2 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12\n#>  [1,] 1949 3798601  0  0  0  0  0  0  0  0   0   0   0\n#>  [2,] 1949 3798926  1  0  0  0  0  0  0  0   0   0   0\n#>  [3,] 1949 3799251  0  1  0  0  0  0  0  0   0   0   0\n#>  [4,] 1949 3799576  0  0  1  0  0  0  0  0   0   0   0\n#>  [5,] 1949 3799900  0  0  0  1  0  0  0  0   0   0   0\n#>  [6,] 1949 3800225  0  0  0  0  1  0  0  0   0   0   0\n#>  [7,] 1950 3800550  0  0  0  0  0  1  0  0   0   0   0\n#>  [8,] 1950 3800875  0  0  0  0  0  0  1  0   0   0   0\n#>  [9,] 1950 3801200  0  0  0  0  0  0  0  1   0   0   0\n#> [10,] 1950 3801525  0  0  0  0  0  0  0  0   1   0   0\n#> [11,] 1950 3801850  0  0  0  0  0  0  0  0   0   1   0\n#> [12,] 1950 3802175  0  0  0  0  0  0  0  0   0   0   1\n#> [13,] 1950 3802500  0  0  0  0  0  0  0  0   0   0   0\n\n\nIn R, it is enough to have one variable representing the seasons. This variable should be saved as factor. Then we apply the OLS method to find the model coefficients (see results in Figure 3.14):\n\n\nCode\nMonth <- as.factor(cycle(AirPassengers))\nm <- lm(log(AirPassengers) ~ poly(t, degree = 2) + Month)\nsummary(m)\n\n\n#> \n#> Call:\n#> lm(formula = log(AirPassengers) ~ poly(t, degree = 2) + Month)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.12748 -0.03709  0.00418  0.03197  0.11529 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)           5.45716    0.01392  391.91  < 2e-16 ***\n#> poly(t, degree = 2)1  5.02251    0.04837  103.84  < 2e-16 ***\n#> poly(t, degree = 2)2 -0.39837    0.04820   -8.26  1.4e-13 ***\n#> Month2               -0.02227    0.01968   -1.13  0.25984    \n#> Month3                0.10779    0.01968    5.48  2.2e-07 ***\n#> Month4                0.07639    0.01968    3.88  0.00016 ***\n#> Month5                0.07393    0.01968    3.76  0.00026 ***\n#> Month6                0.19603    0.01968    9.96  < 2e-16 ***\n#> Month7                0.29997    0.01969   15.24  < 2e-16 ***\n#> Month8                0.29072    0.01969   14.77  < 2e-16 ***\n#> Month9                0.14617    0.01969    7.42  1.3e-11 ***\n#> Month10               0.00814    0.01970    0.41  0.67991    \n#> Month11              -0.13540    0.01970   -6.87  2.4e-10 ***\n#> Month12              -0.02132    0.01971   -1.08  0.28129    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.0482 on 130 degrees of freedom\n#> Multiple R-squared:  0.989,  Adjusted R-squared:  0.988 \n#> F-statistic:  913 on 13 and 130 DF,  p-value: <2e-16\n\n\n\n\nCode\np1 <- pAirPassengers + \n    geom_line(aes(y = exp(m$fitted.values)), col = 4, lwd = 1.5)\np2 <- forecast::ggAcf(m$residuals) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.14: Plot of the AirPassengers data, with estimated parametric quadratic trend and seasonality (modeled using dummy variables), and ACF of the residuals. Since the model was fitted on the log scale, the trend-cycle estimates need to be exponentiated to match the original scale of the data.\n\n\n\n\n\n\n\n\n3.5.2 Seasonal regression with Fourier series\nIt is a spoiler for the future lecture on spectral analysis of time series, but seasonal regression modeling would not be complete without introducing the trigonometric Fourier series.\nWe can fit a linear regression model using several pairs of trigonometric functions as predictors. For example, for monthly observations \\[\n\\begin{split}\ncos_{k} (i) &= \\cos (2 \\pi ki / 12), \\\\\nsin_{k} (i) &= \\sin (2 \\pi ki / 12),\n\\end{split}\n\\] where \\(i\\) is the month within the year, and the trigonometric function has \\(k\\) cycles per year.\n\n\n\n\n\n\nExample: Airline passengers polynomial smoothing with Fourier series for the seasons\n\n\n\nNow let us apply the method of sinusoids to the AirPassengers time series. We have one prominent and, possibly, one less prominent peak per year. Hence, we can test the model with \\(k = 1\\) and \\(k = 2\\). We will construct the following trigonometric predictors: \\[\n\\begin{split}\ncos_{1,t} &= \\cos(2 \\pi \\text{month}_t/12),\\\\\nsin_{1,t} &= \\sin(2 \\pi \\text{month}_t/12),\\\\\ncos_{2,t} &= \\cos(4 \\pi \\text{month}_t/12),\\\\\nsin_{2,t} &= \\sin(4 \\pi \\text{month}_t/12)\n\\end{split}\n\\] to use in the model \\[\n\\ln(Y_{t}) = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\beta_{1}cos_{1,t} + \\beta_{2}sin_{1,t} + \\beta_{3}cos_{2,t} + \\beta_{4}sin_{2,t} + \\epsilon_{t}.\n\\]\nCalculate the predictors and estimate the model in R (see results in Figure 3.15):\n\n\nCode\nmonth <- as.numeric(cycle(AirPassengers))\ncos1 <- cos(2 * pi * month / 12)\nsin1 <- sin(2 * pi * month / 12)\ncos2 <- cos(4 * pi * month / 12)\nsin2 <- sin(4 * pi * month / 12)\nm <- lm(log(AirPassengers) ~ poly(t, degree = 2) + cos1 + sin1 + cos2 + sin2)\nsummary(m)\n\n\n#> \n#> Call:\n#> lm(formula = log(AirPassengers) ~ poly(t, degree = 2) + cos1 + \n#>     sin1 + cos2 + sin2)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.20078 -0.03539 -0.00056  0.04065  0.13771 \n#> \n#> Coefficients:\n#>                      Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)           5.54218    0.00493 1123.46  < 2e-16 ***\n#> poly(t, degree = 2)1  5.02920    0.05936   84.72  < 2e-16 ***\n#> poly(t, degree = 2)2 -0.39819    0.05920   -6.73  4.3e-10 ***\n#> cos1                 -0.14152    0.00698  -20.28  < 2e-16 ***\n#> sin1                 -0.04923    0.00699   -7.04  8.3e-11 ***\n#> cos2                 -0.02276    0.00698   -3.26   0.0014 ** \n#> sin2                  0.07874    0.00698   11.28  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.0592 on 137 degrees of freedom\n#> Multiple R-squared:  0.983,  Adjusted R-squared:  0.982 \n#> F-statistic: 1.3e+03 on 6 and 137 DF,  p-value: <2e-16\n\n\n\n\nCode\np1 <- pAirPassengers + \n    geom_line(aes(y = exp(m$fitted.values)), col = 4, lwd = 1.5)\np2 <- forecast::ggAcf(m$residuals) + \n    ggtitle(\"ACF of residuals\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.15: Plot of the AirPassengers data, with estimated parametric quadratic trend and seasonality (modeled using trigonometric functions), and ACF of the residuals. Since the model was fitted on the log scale, the trend-cycle estimates need to be exponentiated to match the original scale of the data."
  },
  {
    "objectID": "l03_smoothing.html#sec-differencing",
    "href": "l03_smoothing.html#sec-differencing",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.6 Differencing",
    "text": "3.6 Differencing\nA big subset of time series models (particularly, models dealing with random walk series) applies differencing to eliminate trends.\nLet \\(\\Delta\\) be the difference operator (\\(\\nabla\\) notation is also used sometimes), then the simple first-order differences are: \\[\n\\begin{split}\n\\Delta Y_t &= Y_t - Y_{t-1} \\\\\n&=(1-B)Y_t,\n\\end{split}\n\\] where \\(B\\) is a backshift operator.\nBackshift operator and difference operator are useful for convenient representation of higher-order differences. We will often use backshift operators in future lectures: \\[\n\\begin{split}\nB^0Y_{t} &= Y_{t} \\\\\nBY_{t} &= Y_{t-1} \\\\\nB^{2}Y_{t} &= Y_{t-2} \\\\\n& \\vdots \\\\\nB^{k}Y_{t} &= Y_{t-k}.\n\\end{split}\n\\] The convenience is due to the fact that the powers of the operators can be treated as powers of elements of usual polynomials, so we can make different operations for more complex cases.\nFor example, if we took second-order differences, \\(\\Delta^2Y_t = (1-B)^2Y_t\\), to remove a trend and then used differences with the seasonal lag of 12 months to remove strong seasonal component, what would be the final form of transformed series? The transformed series, \\(Y^*_t\\), will be: \\[\n\\begin{split}\nY^*_t & = (1-B)^2 (1 - B^{12})Y_t \\\\\n& = (1 - 2B + B^2) (1 - B^{12})Y_t \\\\\n& = (1 - 2B + B^2 - B^{12} + 2B^{13} - B^{14})Y_t \\\\\n& = Y_t - 2Y_{t-1} + Y_{t-2} - Y_{t-12} + 2Y_{t-13}-Y_{t-14},\n\\end{split}\n\\] but we will often use just the top-row notations.\nWe will discuss formal tests to identify an appropriate order of differences in future lectures (unit-root tests), but for now use the rule of thumb: for time trends looking linear use 1st order differences (\\(\\Delta Y_t = Y_t - Y_{t-1}\\)), for parabolic shapes use differences of the 2nd order, etc. Apply differencing of higher orders until the time series looks stationary (plot the transformed series and its ACF at each step). In practice, differences of order 3 or higher are rarely needed.\n\n\n\n\n\n\nNote\n\n\n\nA nonstationary time series that can be converted to stationary by taking differences is also sometimes called a difference-stationary time series or a time series with a stochastic trend. Very rarely, a time series may contain both deterministic and stochastic trends that need to be modeled or removed for further analysis.\n\n\n\n\n\n\n\n\nExample: Shampoo detrending by differencing\n\n\n\nRemove trend from the shampoo series by applying the differences.\nBased on Figure 3.16, differencing once is enough to remove the trend. The resulting detrended series is \\[\nY^*_t = \\Delta Y_t = (1 - B)Y_t = Y_t - Y_{t-1}.\n\\]\n\n\nCode\np1 <- pshampoo +\n    ggtitle(\"Yt\")\np2 <- forecast::ggAcf(shampoo) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np3 <- ggplot2::autoplot(diff(shampoo)) + \n    xlab(\"Year\") + \n    ylab(\"Sales\") + \n    ggtitle(\"D1\") +\n    theme_light()\np4 <- forecast::ggAcf(diff(shampoo)) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np5 <- ggplot2::autoplot(diff(shampoo, differences = 2)) + \n    xlab(\"Year\") + \n    ylab(\"Sales\") +\n    ggtitle(\"D2\") +\n    theme_light()\np6 <- forecast::ggAcf(diff(shampoo, differences = 2)) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.16: Time series plot of shampoo sales with an estimated ACF and the differenced series with their ACFs.\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Airline passengers detrending and deseasonalizing by differencing\n\n\n\nRemove trend and seasonality from the log transformed AirPassengers series by applying the differences.\nBased on Figure 3.17, differencing once with the non-seasonal and the seasonal lags is enough to remove the trend and strong seasonality. The final series (denoted as D1D12) is \\[\n\\begin{split}\nD1D12_t & = (1 - B)(1 - B^{12})\\lg Y_t = (1 - B - B^{12} + B^{13})\\lg Y_t\\\\\n& = \\lg Y_t - \\lg Y_{t-1} - \\lg Y_{t-12} + \\lg Y_{t-13}.\n\\end{split}\n\\]\n\n\nCode\nYt <- log10(AirPassengers)\n# Apply first-order (non-seasonal) differences\nD1 <- diff(Yt)\n# Additionally, apply first-order seasonal differences\nD1D12 <- diff(D1, lag = 12)\np1 <- ggplot2::autoplot(Yt) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"Yt\") +\n    theme_light()\np2 <- forecast::ggAcf(Yt) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np3 <- ggplot2::autoplot(D1) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"D1\") +\n    theme_light()\np4 <- forecast::ggAcf(D1) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\np5 <- ggplot2::autoplot(D1D12) + \n    xlab(\"Year\") + \n    ylab(\"log10(Air passangers)\") + \n    ggtitle(\"D1D12\") +\n    theme_light()\np6 <- forecast::ggAcf(D1D12) + \n    ggtitle(\"\") +\n    xlab(\"Lag (months)\") + \n    theme_light()\n(p1 + p2) / (p3 + p4) / (p5 + p6) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 3.17: Time series plot of shampoo sales with an estimated ACF and the detrended (differenced) series with their ACFs."
  },
  {
    "objectID": "l03_smoothing.html#conclusion",
    "href": "l03_smoothing.html#conclusion",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.7 Conclusion",
    "text": "3.7 Conclusion\nWe have implemented various methods for trend visualization, modeling, and trend removal. In each case, the method could be extended to additionally remove from a time series a strong seasonal signal. The choice of a method depends on many things, such as:\n\nThe methods require different skill levels (and time commitment) for their implementation and can be categorized as automatic and non-automatic.\nSome methods are very flexible when dealing with seasonality (e.g., Holt–Winters), while others are not.\nThe goal of smoothing and desired degree of smoothness in the output.\nSome of the methods have a convenient way of getting forecasts (e.g., exponential smoothing and polynomial regression), while other methods are more suitable for interpolation (polynomial regression) and simple visualization (simple moving average), or just trend elimination (all of the considered methods are capable of eliminating trends, but differencing is the method that does nothing else but the elimination).\n\nRemember that forecasted values (point forecasts) are not valuable if their uncertainty is not quantified. To quantify the uncertainty, we provide prediction intervals that are based on the past behavior of residuals and careful residual diagnostics (such as described in the previous lectures).\nThe evaluation of the methods may involve splitting the time series into training and testing periods. Overall, this lecture focused on presenting the methods themselves, while residual diagnostics and evaluation on a testing set were omitted due to time constraints.\nWhile the considered methods are among the most common, there are many more smoothing methods that can be applied to time series: splines (such as used in generalized additive models, GAMs), kernels, machine learning techniques, etc."
  },
  {
    "objectID": "l03_smoothing.html#appendix",
    "href": "l03_smoothing.html#appendix",
    "title": "3  Smoothing (Detrending and Deseasonalizing)",
    "section": "3.8 Appendix",
    "text": "3.8 Appendix\n\n\n\n\n\n\nExample: Smoothing blocks\n\n\n\nA student was concerned about applying smoothing to data that have a block-like structure. Without a real data set on hand, below we simulate a series \\(X_t\\) with the mentioned structure.\n\n\nCode\nset.seed(123)\n# Set number of blocks\nn <- 10 \n# Let the values of blocks come from standard normal distribution,\n# and length of block be a random variable generated from Poisson distribution:\nXt <- rep(rnorm(n), times = rpois(n, 5))\n\n\nFigure 3.18 shows an example of lowess applied to \\(X_t\\) to get a smoothed series \\(M_t\\).\nWe see that lowess estimates smooth trends. If we assume that trends are not smooth, then some other techniques (e.g., piecewise linear estimation) should be used.\n\n\nCode\nt <- c(1:length(Xt))\nggplot(data.frame(t = t, Xt = Xt), aes(x = t, y = Xt)) + \n    geom_line() + \n    geom_smooth(se = FALSE, span = 0.25) + \n    theme_light()\n\n\n\n\n\nFigure 3.18: Smoothing block-like data \\(X_t\\).\n\n\n\n\n\n\n\n\n\n\nBerk RA (2016) Statistical learning from a regression perspective, 2nd edn. Springer, Switzerland\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York\n\n\nCleveland WS (1979) Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association 74:829–836. https://doi.org/10.1080/01621459.1979.10481038\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern time series analysis. Springer-Verlag, Berlin"
  },
  {
    "objectID": "l04_arma.html",
    "href": "l04_arma.html",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "",
    "text": "The goal of this lecture is to introduce a broad class of models for stationary time series – autoregressive moving average (ARMA) models. You should recognize the difference between the AR and MA components and learn how to implement these models in practice.\nObjectives\nReading materials"
  },
  {
    "objectID": "l04_arma.html#introduction",
    "href": "l04_arma.html#introduction",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nAfter removing or modeling trend and strong seasonality in a time series, we need to take care of the stationary remainder, which may be autocorrelated or conditionally heteroskedastic. Here we introduce a broad class of ARMA models that deal with autocorrelation in time series.\nRecall that a time series is (weakly) stationary if its mean and autocovariance do not depend on time, \\(t\\); autocovariance depends only on the lag \\(h\\). We will use these two conditions to show that the time series processes that we will model as AR, MA, or ARMA are stationary. Remember that presence of autocorrelation does not mean that there is a trend.\nWe start by checking the two conditions for other typical univariate processes, white noise and random walk.\n\n\nCode\n# Set seed for reproducible random number generation\nset.seed(123) \nT <- 300L\nLag <- 0:15\n\n\nMost of the examples in this lecture are simulated to show how sample autocorrelations may differ from theoretical ones, even if the simulated time series are sampled from a process of a certain type. All simulated series are of length 300.\n\n\n\n\n\n\nExample: Check stationarity of white noise\n\n\n\nLet \\(X_t\\) be a white noise series, \\(X_t \\sim \\mathrm{WN}(0, \\sigma^2)\\). Show that \\(X_t\\) is weakly stationary.\n\\(\\mathrm{E}(X_t) = 0\\) for all \\(t\\) \\(\\Rightarrow\\) \\(\\mathrm{E}(X_t)\\) is independent of \\(t\\).\nNow check the autocovariance \\[\n\\mathrm{cov}(X_t, X_{t+h}) =\n\\begin{cases}\n\\mathrm{cov}(X_t, X_t) & \\text{if } h = 0\\\\\n0                      & \\text{otherwise}\n\\end{cases} =\n\\begin{cases}\n\\mathrm{var}(X_t) & \\text{if } h = 0\\\\\n0                 & \\text{otherwise}\n\\end{cases} =\n\\begin{cases}\n\\sigma^2 & \\text{if } h = 0\\\\\n0        & \\text{otherwise}\n\\end{cases}\n\\]\n\\(\\Rightarrow\\) \\(\\mathrm{cov}(X_t, X_{t+h})\\) does not depend on \\(t\\), depends only on \\(h\\).\nBoth conditions of weak stationarity are satisfied, \\(X_t\\) is weakly stationary.\n\n\nSee Figure 4.1 with plots for \\(X_t \\sim \\mathrm{WN}(0, \\sigma^2)\\), where \\(\\sigma = 2\\). More specifically, \\[\nX_t \\sim \\text{i.i.d.}\\; N(0, \\sigma^2);\\; \\sigma = 2.\n\\tag{4.1}\\]\n\n\nCode\n# Theoretical ACF, PACF\nRHO <- c(1, rep(0, max(Lag)))\n# Sample data\nX <- ts(rnorm(T, sd = 2))\n# Plots\np1 <- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\") +\n    theme_light()\np2 <- ggplot(data.frame(Lag, PACF = RHO), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\") +\n    theme_light()\np3 <- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\") +\n    theme_light()\np4 <- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\") +\n    theme_light()\np5 <- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\") +\n    theme_light()\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.1: White noise as specified in Equation 4.1.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that ACF(0) = PACF(0) = 1 always. Some R plotting functions show results for the zero lag (e.g., acf()), some do not (e.g., pacf() and the function forecast::ggAcf() used here).\n\n\n\n\n\n\n\n\nExample: Check stationarity of random walk\n\n\n\nCheck the same for random walk \\(S_t\\). \\(S_t = \\sum_{i=1}^t{X_t}\\), where \\(X_t \\sim \\text{i.i.d.}(0,\\sigma^2)\\).\n\\(\\mathrm{E}(S_t) = \\mathrm{E}(\\sum_{i=1}^t{X_t}) = 0\\) \\(\\Rightarrow\\) \\(\\mathrm{E}(X_t)\\) does not depend on \\(t\\).\nNow check the autocovariance \\[\n\\begin{split}\n\\mathrm{cov}(S_t, S_{t+h}) &= \\mathrm{cov}(S_t, S_t + X_{t+1}+\\ldots + X_{t+h})\\\\\n& = \\mathrm{cov}(S_t, S_t) + \\mathrm{cov}(S_t, X_{t+1}+\\ldots + X_{t+h})\\\\\n& = \\mathrm{var}(S_t) + 0\\\\\n& = \\mathrm{var}\\left(\\sum_{i=1}^t{X_t}\\right) = \\sum_{i=1}^t\\mathrm{var}(X_t)\\\\\n& = t\\sigma^2\n\\end{split}\n\\]\nAutocovariance of \\(S_t\\) depends on time, \\(S_t\\) is not stationary. Also, notice that \\(S_t = S_{t-1} + X_t\\), that is, \\(S_t\\) is a nonstationary AR(1) process with \\(\\phi_1 = 1\\).\n\n\nRandom walk patterns are widely found in nature, for example, in the phenomenon of Brownian motion. Notice the linear decay in ACF of non-stationary time series (Figure 4.2).\n\\[\nS_t = \\sum_{i = 1}^t X_i,\\; \\text{where}\\; X_t \\sim \\mathrm{i.i.d.}(0, \\sigma^2);\\; \\sigma = 2.\n\\tag{4.2}\\]\n\n\nCode\n# Sample data\nRW0 <- ts(cumsum(X))\n# Plots\np3 <- forecast::autoplot(RW0) + \n    ggtitle(\"Sample time series\") +\n    theme_light()\np4 <- forecast::ggAcf(RW0) + \n    ggtitle(\"Sample ACF\") +\n    theme_light()\np5 <- forecast::ggAcf(RW0, type = \"partial\") + \n    ggtitle(\"Sample PACF\") +\n    theme_light()\np3 + p4 + p5 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.2: Zero-mean random walk as specified in Equation 4.2.\n\n\n\n\nRandom walk may look very different if the i.i.d. process has non-zero mean. Here is a random walk with drift (Figure 4.3): \\[\nS_t = \\sum_{i = 1}^t X_i,\\; \\text{where}\\; X_t \\sim \\mathrm{i.i.d.}(a, \\sigma^2);\\; a = 0.2,\\; \\sigma = 2.\n\\tag{4.3}\\]\n\n\nCode\n# Sample data\nRW2 <- ts(cumsum(X + 0.2))\n# Plots\np3 <- forecast::autoplot(RW2) + \n    ggtitle(\"Sample time series\") +\n    theme_light()\np4 <- forecast::ggAcf(RW2) + \n    ggtitle(\"Sample ACF\") +\n    theme_light()\np5 <- forecast::ggAcf(RW2, type = \"partial\") + \n    ggtitle(\"Sample PACF\") +\n    theme_light()\np3 + p4 + p5 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.3: White noise with drift as specified in Equation 4.3."
  },
  {
    "objectID": "l04_arma.html#autoregressive-ar-models",
    "href": "l04_arma.html#autoregressive-ar-models",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.2 Autoregressive (AR) models",
    "text": "4.2 Autoregressive (AR) models\nAR(1) model\nWe are already familiar with an AR(1) model \\[\nY_{t} = \\phi_{1}  Y_{t -1} + \\epsilon_{t},\n\\] where\n\n\\(\\epsilon_{t}\\) is white noise (\\(\\mathrm{E}(\\epsilon_{t}) = 0\\) and \\(\\mathrm{E}(\\epsilon_{t}^2) = \\sigma^{2}_{\\epsilon}\\)),\n\\(\\epsilon_{t}\\) is independent of \\(Y_{t - k}, \\ldots, Y_{t - 1}\\),\n\\(\\phi_{1}\\) is the coefficient of autoregression.\n\nThe AR(1) model can be rewritten as \\[\n(1 - \\phi_{1} B) Y_{t} = \\epsilon_{t},\n\\] or in even more compact notation as \\[\n\\phi(B)Y_t = \\epsilon_t,\n\\] where \\(\\phi(\\lambda)\\) is the polynomial \\(\\phi (\\lambda) = 1 - \\phi_{1} \\lambda\\).\nLet us find the variance of \\(Y_{t}\\) \\[\n\\begin{split}\n\\mathrm{var} (Y_{t} ) = \\mathrm{E} ( \\phi_{1} Y_{t - 1} + \\epsilon_{t})^{2} & =  \\mathrm{E} ( \\phi^{2}_{1} Y^{2}_{t - 1} + 2 \\phi_{1} Y_{t-1}  \\epsilon_{t} + \\epsilon^{2}_{ t}) \\\\\n\\\\\n& = \\phi^{2}_{1} \\mathrm{E}(Y^{2}_{t - 1}) + 2\\phi_{1} \\mathrm{E} (Y_{t - 1} \\epsilon_{t}) + \\mathrm{E}(\\epsilon^{2}_{t}) \\\\\n\\\\\n& = \\phi^{2}_{1} \\mathrm{var} \\left( Y_{t -1} \\right) + 0 + \\sigma^{2}_{\\epsilon} \\\\\n\\\\\n& = \\phi^{2}_{1} \\mathrm{var} \\left( Y_{t -1} \\right) + \\sigma^{2}_{\\epsilon}.\n\\end{split}\n\\]\nHence, if \\(\\phi_{1} \\neq \\pm 1\\), then \\[\n\\sigma^{2}_{Y} = \\frac{\\sigma^{2}_{\\epsilon}} {1 - \\phi^{2}_{1}}.\n\\tag{4.4}\\]\n\n\n\n\n\n\nNote\n\n\n\nNote that for Equation 4.4 to make sense, we require \\(|\\phi_{1}| < 1\\). This is the condition under which AR(1) process is (weakly) stationary. E.g., if \\(\\phi_{1} = \\pm 1\\) then the process is a random walk and is also not stationary. We shall see later that all AR processes require some condition of this nature.\n\n\n\n\n\n\n\n\nExample: Check stationarity of AR(1)\n\n\n\nShow that AR(1) process \\(Y_t\\) is stationary, \\(Y_t = \\phi Y_{t-1} + Z_t\\), \\(|\\phi| < 1\\), \\(Z_t \\sim \\mathrm{WN}(0, \\sigma^2)\\).\n\\[\n\\begin{split}\n\\mathrm{E}(Y_t) &= \\mathrm{E}(\\phi Y_{t-1} + Z_t)\\\\\n& = \\phi \\mathrm{E}(Y_{t-1}) + \\mathrm{E}(Z_t)\\\\\n& = \\phi^2 \\mathrm{E}(Y_{t-2}) + 0\\\\\n& = \\phi^3 \\mathrm{E}(Y_{t-3})\\\\\n& = \\ldots\\\\\n& = \\phi^N \\mathrm{E}(Y_{t-N})\\\\\n\\lim_{N \\rightarrow \\infty} \\mathrm{E}(Y_t) &= \\lim_{N \\rightarrow \\infty} \\phi^N \\mathrm{E}(Y_{t-N}) = 0\n\\end{split}\n\\] Therefore, mean of \\(Y_t\\) does not depend on time.\nNow check autocovariance. For \\(h=0\\), \\(\\mathrm{cov}(Y_t, Y_{t}) = \\mathrm{var}(Y_t) = \\sigma^2_Y\\), which is derived in Equation 4.4 and does not depend on time.\nFor \\(h \\neq 0\\), \\[\n\\begin{split}\n\\mathrm{cov}(Y_t, Y_{t-h}) &= \\mathrm{cov}(\\phi Y_{t-1} + Z_t, Y_{t-h})\\\\\n& = \\mathrm{cov}(\\phi Y_{t-1}, Y_{t-h}) + \\mathrm{cov}(Z_t, Y_{t-h})\\\\\n& = \\phi \\mathrm{cov}(Y_{t-1}, Y_{t-h}) + 0\\\\\n& = \\phi \\mathrm{cov}(\\phi Y_{t-2} + Z_{t-1}, Y_{t-h})\\\\\n& = \\phi^2 \\gamma_Y(h-2)\\\\\n& = \\ldots\\\\\n& = \\phi^h \\gamma_Y(0) = \\phi^h \\sigma_Y^2,\n\\end{split}\n\\] thus autocovariance does not depend on time, depends on lag \\(h\\).\nBoth conditions for weak stationarity are satisfied, \\(Y_t\\) specified above is weakly stationary.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdditionally, see that ACF of this process \\[\n\\rho_Y(h) = \\frac{\\gamma_Y(h)}{\\gamma_Y(0)} = \\frac{\\phi^h \\gamma_Y(0)}{\\gamma_Y(0)} = \\phi^h\n\\] shows exponential decay. Remember this about AR processes.\n\n\nAR(\\(p\\)) model\nNow we can extend our AR(1) model to \\[\n\\begin{split}\nY_{t} &= \\phi_{1} Y_{t  - 1} + \\phi_{2} Y_{t - 2} + \\ldots + \\phi_{p} Y_{t - p} + \\epsilon_{t} \\\\\n&=\\sum_{i=1}^p\\phi_iY_{t-i} + \\epsilon_t\n\\end{split}\n\\tag{4.5}\\] with \\(\\epsilon_{t}\\) independent of \\(Y_{t - k}, \\ldots , Y_{t - 1}\\); \\(p \\in \\mathbb{N}^{+}\\). This model is called an autoregressive model of order \\(p\\), or simply an AR(\\(p\\)) model.\nSimilarly to AR(1), every AR(\\(p\\)) model may be rewritten as \\[\n\\left(1 -  \\sum^{p}_{i = 1} \\phi_{i} B^{i} \\right) Y_{t} = \\epsilon_{t},\n\\] or in more compact notation as \\[\n\\phi (B) Y_t = \\epsilon_t,\n\\] where \\(\\phi(\\lambda)\\) is the polynomial \\(\\phi(\\lambda) = 1 - \\phi_{1} \\lambda - \\phi_{2} \\lambda^{2} - \\ldots - \\phi_{p} \\lambda^{p}\\).\nThe stationarity condition for AR(\\(p\\)) model is defined through the polynomial \\(\\phi(\\lambda)\\). Specifically, all the roots of the function \\(\\phi(\\lambda)\\) lie outside the unit circle in the complex plane.\nTo compute ACVF and ACF for the AR(\\(p\\)) model, multiplying both sides of Equation 4.5 by \\(Y_{t - k}\\) for \\(k \\geqslant p\\) and taking expectations yields \\[\n\\begin{split}\n\\mathrm{E} \\left( Y_{t} Y_{t - k} \\right) & =  \\mathrm{E} \\left( \\phi_{1} Y_{t-1} Y_{t-k} + \\phi_{2} Y_{t-2} Y_{t-k} + \\ldots + \\phi_{p} Y_{t -p} Y_{t - k} + \\epsilon_{t} Y_{t -k} \\right) \\\\\n& =  \\phi_{1}  \\mathrm{E} \\left( Y_{t  - 1} Y_{t - k} \\right) + \\phi_{2} \\mathrm{E} \\left( Y_{t - 2} Y_{t - k} \\right) + \\ldots+ \\phi_{p} \\mathrm{E} \\left(Y_{t - p} Y_{t - k} \\right) + \\mathrm{E} \\left( \\epsilon_{t} Y_{t  - k} \\right).\n\\end{split}\n\\]\nThus, we get autocovariances \\[\n\\gamma(k) = \\phi_{1}\\gamma(k -1) + \\phi_{2}\\gamma(k - 2) + \\ldots + \\phi_{p} \\gamma(k  - p),\n\\] which turns into a mixed exponential decay of order \\(p\\) for the ACF \\[\n\\rho(k) = \\phi_{1} \\rho(k - 1) + \\phi_{2} \\rho(k - 2) + \\ldots + \\phi_{p} \\rho(k - p).\n\\]\nTo start off the decay we need the \\(p\\) starting correlations \\(\\rho(1), \\rho(2), \\ldots , \\rho(p)\\), which we can solve for from the parameters \\(\\phi_{1} ,\\phi_{2} , \\ldots , \\phi_{p}\\).\nPACF of AR processes\nAn AR(\\(p\\)) model implies \\(Y_{t+h} = \\sum_{i=1}^p\\phi_{i} Y_{t + h - i} + \\epsilon_{t+h}\\), where the roots of \\(\\phi(\\lambda)\\) are outside the unit circle. When \\(h > p\\), the regression of \\(Y_{t+h}\\) on \\(\\{ Y_{t+1},\\ldots, Y_{t+h-1}\\}\\), is \\[\n\\hat{Y}_{t+h} = \\sum^p_{i=1} \\phi_i Y_{t+h-i}.\n\\] Thus, when \\(h > p\\), \\[\n\\rho_{hh} = \\mathrm{cor}(Y_{t+h}-\\hat{Y}_{t+h}, Y_t - \\hat{Y}_t) = \\mathrm{cor}(\\epsilon_{t+h}, Y_t - \\hat{Y}_t) = 0,\n\\] because, by causality of the process, \\(Y_t - \\hat{Y}_t\\), depends only on \\(\\{ \\epsilon_{t+h-1}, \\epsilon_{t+h-2},\\ldots \\}\\).\nWhen \\(h\\leqslant p\\), \\(\\rho_{hh}\\) is not zero, and \\(\\rho_{11}, \\ldots, \\rho_{p-1,p-1}\\) are not necessarily zero.\n\n\n\n\n\n\nNote\n\n\n\nThe important conclusion is that PACF of an AR(\\(p\\)) process necessarily cuts off (PACF = 0) after lag \\(p\\).\n\n\nSee the plots for the following AR(1) process in Figure 4.4: \\[\nX_t = \\phi_1 X_{t-1} + \\epsilon_t;\\; \\epsilon_t \\sim N(0,\\sigma^2);\\; \\phi_1=0.6,\\; \\sigma = 1.\n\\tag{4.6}\\]\n\n\nCode\n# Theoretical ACF, PACF\nphi <- 0.6\nRHO <- phi^Lag\nALPHA <- c(1, phi, rep(0, max(Lag) - 1))\n# Sample data\nX <- arima.sim(list(order = c(1, 0, 0), ar = phi), n = T)\n# Plots\np1 <- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\") +\n    theme_light()\np2 <- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\") +\n    theme_light()\np3 <- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\") +\n    theme_light()\np4 <- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\") +\n    theme_light()\np5 <- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\") +\n    theme_light()\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.4: AR(1) process as specified in Equation 4.6.\n\n\n\n\nSee the plots for the following AR(1) process in Figure 4.5 (only the coefficient \\(\\phi_1\\) changed from the previous specification): \\[\nX_t = \\phi_1 X_{t-1} + \\epsilon_t;\\; \\epsilon_t \\sim N(0,\\sigma^2);\\; \\phi_1=-0.6,\\; \\sigma = 1.\n\\tag{4.7}\\]\n\n\nCode\n# Theoretical ACF, PACF\nphi <- -0.6\nRHO <- phi^Lag\nALPHA <- c(1, phi, rep(0, max(Lag) - 1))\n# Sample data\nX <- arima.sim(list(order = c(1, 0, 0), ar = phi), n = T)\n# Plots\np1 <- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\") +\n    theme_light()\np2 <- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\") +\n    theme_light()\np3 <- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\") +\n    theme_light()\np4 <- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\") +\n    theme_light()\np5 <- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\") +\n    theme_light()\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.5: AR(1) process as specified in Equation 4.7.\n\n\n\n\nACF of AR(1) model takes two graphical forms depending on whether the coefficient \\(\\phi_{1}\\) is positive or negative. In the first case, the decay occurs through the positive axis only (Figure 4.4). In the second case, the decay alternates between the negative and positive axes (Figure 4.5). This is so-called mixed exponential decay. Although both forms are examples of exponential decay, they appear different visually and you must learn to recognize both.\nAs the order \\(p\\) of an AR(\\(p\\)) model increases, the number of different visual presentations of the ACF increases as well. The ACF of an AR(\\(p\\)) model can take \\(2^{p}\\) different graphical forms, depending on the signs (positive or negative) of the parameters \\(\\phi_{1} , \\phi_{2}, \\ldots, \\phi_{p}\\). Thus, the ACF of an AR(2) model can take \\(2^{2} = 4\\) different graphical forms."
  },
  {
    "objectID": "l04_arma.html#the-yulewold-representation-and-linear-processes",
    "href": "l04_arma.html#the-yulewold-representation-and-linear-processes",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.3 The Yule–Wold representation and linear processes",
    "text": "4.3 The Yule–Wold representation and linear processes\nA process \\(X_{t}\\), (\\(t = 0, \\pm 1, \\pm 2, \\ldots\\)) is said to be linear if it has a representation of the form \\[\nX_{t} = \\mu + \\sum^{\\infty}_{r = - \\infty} c_{r} \\epsilon_{t - r},\n\\] where\n\n\\(\\mu\\) is a common mean,\n\\(c_{r}\\) is a sequence of fixed constants,\n\\(\\epsilon_{t}\\) are uncorrelated random variables with mean 0 and common variance.\n\nWe assume \\(\\sum c^{2}_{r} < \\infty\\) to ensure that the variances of individual \\(X_{t}\\) are finite (stationarity and existence condition). Then the process \\(X_{t}\\) is necessarily (weakly) stationary.\nIf in addition we require that \\(\\epsilon_{t}\\) are identically distributed, then \\(X_{t}\\) is strictly stationary. For example, see the case of normally distributed \\(\\epsilon_{t}\\).\nIf \\(c_r = 0\\) for all \\(r < 0\\), \\(X_t\\) is called causal (i.e., the process at time \\(t\\) does not depend on the future, yet unobserved, values of \\(\\epsilon_{t}\\)).\nThe representation of a causal stationary process \\(X_{t}\\) with 0 mean in the form \\[\nX_{t} = \\sum^{\\infty}_{r = 0}  c_{r} \\epsilon_{t - r},\n\\tag{4.8}\\] where \\(\\epsilon_{t} \\sim \\mathrm{WN}(0, \\sigma^{2}\\)) and \\(\\sum c^{2}_{r} < \\infty\\), is sometimes called the Yule–Wald representation."
  },
  {
    "objectID": "l04_arma.html#moving-average-ma-models",
    "href": "l04_arma.html#moving-average-ma-models",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.4 Moving average (MA) models",
    "text": "4.4 Moving average (MA) models\nMA(1) model\nA zero mean stationary process \\(Y_{t}\\) is called a moving average process of order 1, or an MA(1) model, if \\(Y_{t}\\) satisfies \\[\nY_{t} = \\epsilon_{t} + \\theta_{1}  \\epsilon_{t - 1},\n\\] where \\(\\epsilon_{t}\\) is white noise.\nIn other words, an MA(1) process has \\(c_{r} = 0\\) for every \\(r > 1\\) in its Yule–Wold representation Equation 4.8.\nThe MA(1) model may be rewritten as \\[\nY_{t} = (1 + \\theta_{1} B) \\epsilon_{t},\n\\] or in the compact form as \\[\nY_t = \\theta (B) \\epsilon_t,\n\\] where \\(\\theta (\\lambda)\\) is the polynomial \\(\\theta(\\lambda) = 1 + \\theta_{1} \\lambda\\).\nMA(q) model\nWe can extend MA(1) model further and consider \\[\n\\begin{split}\nY_{t} &= \\epsilon_{t} + \\theta_{1} \\epsilon_{t -1} + \\theta_{2} \\epsilon_{t - 2} + \\ldots + \\theta_{q} \\epsilon_{t - q} \\\\\n&= \\epsilon_{t} + \\sum_{i=1}^q \\theta_i \\epsilon_{t-i}.\n\\end{split}\n\\] This model is called a moving average model of order \\(q\\), or a MA(\\(q\\)).\nWe can write down MA(\\(q\\)) as \\[\nY_{t} = \\left(1 + \\theta_{1} B + \\theta_{2} B^{2} + \\ldots + \\theta_{q} B^{q} \\right) \\epsilon_{t} ,\n\\] or in the compact form as \\[\nY_t = \\theta (B) \\epsilon_t,\n\\] where \\(\\theta(\\lambda)\\) is the polynomial \\(\\theta(\\lambda) = 1 + \\theta_{1} \\lambda + \\theta_{2} \\lambda^{2} + \\ldots + \\theta_{q} \\lambda^{q}\\).\nNow compute ACVF and ACF of the MA(\\(q\\)) process \\(Y_{t}\\). Using the Yule–Wald representation of \\(Y_{t}\\), \\[\nY_{t} = \\sum^{q}_{r = 0} \\theta_{r} \\epsilon_{t - r},\n\\] the autocovariance \\[\n\\begin{split}\n\\gamma_Y(h) & = \\mathrm{cov}(Y_{t}, Y_{t+h}) = \\mathrm{cov} \\left( \\sum^{q}_{r = 0} \\theta_{r }\\epsilon_{t - r}, \\sum^{q}_{\\ell = 0} \\theta_{\\ell} \\epsilon_{t + h - \\ell} \\right) \\\\\n\\\\\n& =  \\sum^{q}_{r = 0} \\sum^{q}_{\\ell = 0} \\mathrm{cov} \\left( \\theta_{r } \\epsilon_{t - r},  \\theta_{\\ell} \\epsilon_{t + h - \\ell} \\right) \\\\\n\\\\\n& =   \\sum^{q}_{r = 0}   \\sum^{q}_{\\ell = 0} \\theta_{r } \\theta_{\\ell} \\mathrm{cov}  \\left( \\epsilon_{t - r}, \\epsilon_{t - r}  \\right) ~~ \\text{since cov} \\left( \\epsilon_{u}, \\epsilon_{v}  \\right) = 0 ~ \\text{unless} ~ u = v \\\\\n\\\\\n& =  \\sigma_{\\epsilon}^{2} \\sum^{q}_{r = 0} \\theta_{r} \\theta_{r+h}.\n\\end{split}\n\\] Note that the case of no time shift between the noise series, i.e., when \\(t-r = t+h-l\\), leads to \\(l=r+h\\), which is used in the last row.\nHence, \\[\n\\gamma_Y(h) = \\left\\{\n\\begin{array}{lcl}\n\\sigma_{\\epsilon}^{2} \\sum^{q  - h}_{r = 0} \\theta_{r} \\theta_{r+h} & \\text{for} &  h \\leqslant q;\\\\\n0 & \\text{for} & h > q.\n\\end{array}\n\\right.\n\\tag{4.9}\\]\nThis leads us to the most important result about the correlation structure for MA(\\(q\\)) processes. For an MA(\\(q\\)) process, \\(\\gamma(h) = 0\\) for all \\(h > q\\). Equivalently, the ACF has \\(q\\) ‘starting’ correlations or spikes, \\(\\rho(1), \\rho(2), \\ldots, \\rho(q)\\), and then \\(\\rho(h) = 0\\) for \\(h > q\\). We say that the ACF ‘cuts off’ after lag \\(q\\).\nNow compute the actual values of the starting correlations \\(\\rho(1), \\ldots, \\rho(q)\\) in terms of the parameters \\(\\theta_{0} = 1, \\theta_{1}, \\theta_{2}, \\ldots, \\theta_{q}\\). Using Equation 4.9, we get\n\nFor the MA(1) model \\(q = 1\\). Hence \\[\n\\begin{split}\nh  =  0 &\\rightarrow \\gamma(0) = \\sigma^{2} \\sum^{1}_{ r=0}  \\theta^{2}_{r} = \\sigma^{2} \\left( \\theta^{2}_{0} + \\theta^{2}_{1} \\right) = \\sigma^{2} \\left(1 + \\theta^{2}_{1}\\right). \\\\\n\\\\\nh  =  1 &\\rightarrow \\gamma(1) = \\sigma^{2}  \\sum^{0}_{ r=0}  \\theta_{r} \\theta_{r + 1} = \\sigma^{2} \\theta_{0} \\theta_{1} = \\sigma^{2} \\theta_{1}.\n\\end{split}\n\\]\nTherefore \\(\\rho(1) = \\gamma(1) / \\gamma(0) = \\theta_{1} / (1 + \\theta^{2}_{1})\\).\nFor the MA(2) model \\(q = 2\\). Hence \\[\n\\begin{split}\nh =  0 &\\rightarrow \\gamma(0) = \\sigma^{2} \\sum^{2}_{r=0}  \\theta^{2}_{r} = \\sigma^{2} \\left(1 + \\theta^{2}_{1} + \\theta^{2}_{2}\\right). \\\\\n\\\\\nh  =  1 &\\rightarrow \\gamma(1) = \\sigma^{2} \\sum^{1}_{r=0}  \\theta_{r} \\theta_{r + 1} = \\sigma^{2} \\left(\\theta_{1} + \\theta_{1} \\theta_{2} \\right) = \\sigma^{2} \\theta_{1}  \\left(1 + \\theta_{2}\\right). \\\\\n\\\\\nh  =  2 &\\rightarrow \\gamma(2) = \\sigma^{2} \\sum^{0}_{r=0}  \\theta_{r} \\theta_{r + 2} = \\sigma^{2} \\theta_{2}.\n\\end{split}\n\\] Therefore \\(\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\frac{\\theta_{1} \\left(1 + \\theta_{2} \\right)}{1+ \\theta^{2}_{1} + \\theta^{2}_{2}}\\) and \\(\\rho(2) = \\frac{\\gamma(2)}{\\gamma(0)} = \\frac{\\theta_{2}}{1+ \\theta^{2}_{1} + \\theta^{2}_{2}}\\).\nProceed analogously for the MA(\\(q\\)) model.\n\nAnother method of obtaining the correlation coefficients of the MA(\\(q\\)) is to derive the covariance \\(\\gamma(h) = \\mathrm{cov} (Y_{t}, Y_{t - h} )\\) directly by using the bilinear properties of covariance and determining the interaction between the white noise terms. The advantage of this method is that you do not have to memorize anything. Moreover, since white noise has the property that \\(\\mathrm{cov} (\\epsilon_{u}, \\epsilon_{v}) = 0\\) whenever \\(u \\neq v\\), interactions occur only when the time indices match up, i.e., only when \\(u = v\\).\n\n\n\n\n\n\nExample: Calculate ACF of MA(\\(q\\))\n\n\n\nConsider an MA(1) model. Let \\(h \\geqslant 0\\). Applying the covariance properties we obtain the general expression: \\[\n\\begin{split}\n\\gamma(h) & =  \\mathrm{cov}\\left( Y_{t}, Y_{t - h} \\right) =  \\mathrm{cov} \\left( \\epsilon_{t} + \\theta_{1} \\epsilon_{t - 1}, \\epsilon_{t - h} + \\theta_{1} \\epsilon_{t - h - 1}  \\right) \\\\\n& =  \\mathrm{cov} \\left( \\epsilon_{t},  \\epsilon_{t - h} \\right) + \\theta_{1} \\mathrm{cov} \\left(  \\epsilon_{t},  \\epsilon_{t - h - 1}  \\right) + \\theta_{1} \\mathrm{cov} \\left( \\epsilon_{t - 1}, \\epsilon_{t - h} \\right) + \\theta^{2}_{1} \\mathrm{cov} \\left(\\epsilon_{t - 1}, \\epsilon_{t - h - 1} \\right).\n\\end{split}\n\\]\nNote that the covariances in the last line of the above expression will be nonzero if the time indices match up. Plugging different values of \\(h\\) into the above equation we obtain \\[\n\\begin{split}\nh  =  0 &\\rightarrow \\gamma(0) =   \\mathrm{cov} \\left( \\epsilon_{t},   \\epsilon_{t} \\right)  + 0 + 0 + \\theta^{2}_{1}  \\mathrm{cov} \\left( \\epsilon_{t - 1},  \\epsilon_{t - 1}  \\right) = \\sigma^{2} + \\theta^{2}_{1} \\sigma^{2} = \\sigma^{2} \\left( 1 + \\theta^{2}_{1} \\right). \\\\\n\\\\\nh  =  1 &\\rightarrow \\gamma(1) = 0 + 0 + \\theta_{1}  \\mathrm{cov} \\left( \\epsilon_{t - 1},  \\epsilon_{t - 1}  \\right) + 0 = \\sigma^{2} \\theta_{1}.\\\\\n\\\\\nh \\geqslant 2 &\\rightarrow \\gamma(h) = 0 + 0 + 0 + 0 = 0.\n\\end{split}\n\\]\nThis method generalizes easily to computing covariances for MA(\\(q\\)) models of higher order.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have explicitly determined the structure of the ACF of an MA(\\(q\\)) process. Its most important feature is that \\(\\rho(h) = 0\\) for \\(h > q\\). Remember this about MA processes.\n\n\nFiner features, such as expressing the \\(\\rho(h)\\) in terms of the \\(\\theta_{r}\\) coefficients, can be determined as above.\nPACF of MA processes\nFor an MA(\\(q\\)), we can write \\(Y_t = -\\sum_{j=1}^{\\infty} \\pi_j Y_{t-j} + \\epsilon_t\\). Moreover, no finite representation exists. From this result, it should be apparent that the PACF will never cut off, as in the case of an AR(\\(p\\)). For an MA(1), \\(Y_t = \\epsilon_t +\\theta \\epsilon_{t-1}\\), with \\(|\\theta| < 1\\), it can be shown that (Shumway and Stoffer 2014): \\[\n\\rho_{hh} = - \\frac{(-\\theta)^h (1 - \\theta^2)}{1 - \\theta^{2(h+1)}}, \\; h \\geqslant 1.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nEvery MA(\\(q\\)) process has an infinite autoregressive expansion and its PACF never cuts off. In fact, it can be shown that the PACF of an MA(\\(q\\)) process damps out according to a mixed exponential decay of order \\(q\\).\n\n\nWe do not have to compute the PACF by performing numerous regressions first. The computations are done via a recursive formula.\nSimilar to AR(1) examples above, now consider two sets of plots for MA(1) processes with positive and negative coefficient. For the MA(1) process \\[\nX_t = \\theta_1 \\epsilon_{t-1} + \\epsilon_t;\\; \\epsilon_t \\sim N(0,\\sigma^2);\\; \\theta_1=0.3,\\; \\sigma = 1,\n\\tag{4.10}\\] see plots in Figure 4.6.\n\n\nCode\n# Theoretical ACF, PACF\ntheta <- 0.3\nRHO <- c(1, theta/(1 + theta^2), rep(0, max(Lag) - 1))\nALPHA <- c(1, ARMAacf(ma = theta, lag.max = max(Lag), pacf = TRUE))\n# Sample data\nX <- arima.sim(list(order = c(0, 0, 1), ma = theta), n = T)\n# Plots\np1 <- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\") +\n    theme_light()\np2 <- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\") +\n    theme_light()\np3 <- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\") +\n    theme_light()\np4 <- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\") +\n    theme_light()\np5 <- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\") +\n    theme_light()\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.6: MA(1) as specified in Equation 4.10.\n\n\n\n\nFor the MA(1) process with negative \\(\\theta_1\\), \\[\nX_t = \\theta_1 \\epsilon_{t-1} + \\epsilon_t;\\; \\epsilon_t \\sim N(0,\\sigma^2);\\; \\theta_1=-0.3,\\; \\sigma = 1,\n\\tag{4.11}\\] see plots in Figure 4.7.\n\n\nCode\n# Theoretical ACF, PACF\ntheta <- -0.3\nRHO <- c(1, theta/(1 + theta^2), rep(0, max(Lag) - 1))\nALPHA <- c(1, ARMAacf(ma = theta, lag.max = max(Lag), pacf = TRUE))\n# Sample data\nX <- arima.sim(list(order = c(0, 0, 1), ma = theta), n = T)\n# Plots\np1 <- ggplot(data.frame(Lag, ACF = RHO), aes(x = Lag, y = ACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical ACF\") +\n    theme_light()\np2 <- ggplot(data.frame(Lag, PACF = ALPHA), aes(x = Lag, y = PACF)) +\n    geom_bar(stat = \"identity\") +\n    ggtitle(\"Theoretical PACF\") +\n    theme_light()\np3 <- forecast::autoplot(X) + \n    ggtitle(\"Sample time series\") +\n    theme_light()\np4 <- forecast::ggAcf(X) + \n    ggtitle(\"Sample ACF\") +\n    theme_light()\np5 <- forecast::ggAcf(X, type = \"partial\") + \n    ggtitle(\"Sample PACF\") +\n    theme_light()\n(plot_spacer() + p1 + p2) / (p3 + p4 + p5) +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.7: MA(1) as specified in Equation 4.11."
  },
  {
    "objectID": "l04_arma.html#arma",
    "href": "l04_arma.html#arma",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.5 ARMA",
    "text": "4.5 ARMA\nAn ARMA(\\(p, q\\)) is a mixture of \\(p\\) autoregressive components and \\(q\\) moving average components. It can be expressed as \\[\n\\phi^{p} (B)Y_{t} = \\theta^{q} (B) \\epsilon_{t},\n\\] where \\(\\epsilon_{t}\\) is white noise, \\(\\phi^{p}(\\lambda)\\) and \\(\\theta^{q}(\\lambda)\\) are the polynomials of degree \\(p\\) and \\(q\\), respectively.\nARMA(1,2)\nShort notation \\(\\phi^{1} (B)Y_{t} = \\theta^{2} (B) \\epsilon_{t}\\) expands to \\[\n\\left(1 - \\phi_{1} B \\right) Y_{t} = \\left( 1 + \\theta_{1} B + \\theta_{2} B^{2} \\right) \\epsilon_{t}\n\\] and gives \\[\nY_{t} - \\phi_{1} Y_{t - 1} = \\epsilon_{t} + \\theta_{1} \\epsilon_{t - 1} + \\theta_{2}\\epsilon_{t - 2}.\n\\]\nIsolate \\(Y_{t}\\) on the left side of the equation to get the final form of the expansion \\[\nY_{t} = \\phi_{1} Y_{t -1} + \\epsilon_{t} + \\theta_{ 1} \\epsilon_{t - 1} + \\theta_{2} \\epsilon_{t-2}.\n\\]\nARMA(3,1)\nShort notation \\(\\phi^{3} (B)Y_{t} = \\theta^{1} (B) \\epsilon_{t}\\) expands to \\[\n\\left( 1 -  \\phi_{1} B - \\phi_{2} B^{2}  - \\phi_{3} B^{3} \\right) Y_{t} = \\left( 1 + \\theta_{1} B \\right) \\epsilon_{t}\n\\] or \\[\nY_{t} - \\phi_{1} Y_{t - 1} - \\phi_{2} Y_{t - 2} - \\phi_{3} Y_{t - 3} = \\epsilon_{t} + \\theta_{1} \\epsilon_{t-1}.\n\\]\nIsolating \\(Y_{t}\\) gives the final form \\[\nY_{t} = \\phi_{1} Y_{t - 1} + \\phi_{2} Y_{t - 2} + \\phi_{3} Y_{t - 3} + \\epsilon_{t} + \\theta_{1} \\epsilon_{t-1}.\n\\]\nIn a mixed ARMA(\\(p, q\\)) process (that is, \\(p > 0\\), \\(q > 0\\)), neither the ACF nor the PACF cuts off abruptly. Both the ACF and PACF exhibit mixed exponential decay. This occurs because the AR component introduces mixed exponential decay into the ACF, while the MA component introduces mixed exponential decay into the PACF."
  },
  {
    "objectID": "l04_arma.html#summary-of-armaarma-models",
    "href": "l04_arma.html#summary-of-armaarma-models",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.6 Summary of AR/MA/ARMA models",
    "text": "4.6 Summary of AR/MA/ARMA models\nUse the following list or Table 4.1 to decide on appropriate model specification for your data.\nAR(\\(p\\)) models: \\(\\phi^{p} (B)Y_{t} = \\epsilon_{t}\\)\n\nACF: \\(p\\) initial spikes, then damps out as a mixed exponential decay of order \\(p\\) (never 0)\nPACF: \\(p\\) initial spikes, then cuts off; PACF(\\(h\\)) = 0 for \\(h > p\\)\n\nMA(\\(q\\)) models: \\(Y_{t} = \\theta^{q} (B) \\epsilon_{t}\\)\n\nACF: \\(q\\) initial spikes, then cuts off; ACF(\\(h\\)) = 0 for \\(h > q\\)\nPACF: \\(q\\) initial spikes, then damps out as a mixed exponential decay of order \\(q\\) (never 0)\n\nARMA(\\(p,q\\)) models (\\(p>0\\), \\(q>0\\)): \\(\\phi^{p} (B)Y_{t} = \\theta^{q} (B) \\epsilon_{t}\\)\n\nACF: has max (\\(p, q\\)) initial spikes, then damps out as a mixed exponential decay driven by the AR(\\(p\\)) component (never 0)\nPACF: has max(\\(p, q\\)) initial spikes, then damps out as a mixed exponential decay driven by the MA(\\(q\\)) component (never 0)\n\n\n\nTable 4.1: Summary of ARMA(\\(p, q\\)) models\n\n\nModel\nACF\nPACF\n\n\n\n\nMA(\\(q\\))\nZero after lag \\(q\\)\nExponential decay\n\n\nAR(\\(p\\))\nExponential decay\nZero after lag \\(p\\)\n\n\nARMA(\\(p\\),\\(q\\))\nMixed exponential decay\nMixed exponential decay"
  },
  {
    "objectID": "l04_arma.html#example-analysis",
    "href": "l04_arma.html#example-analysis",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.7 Example analysis",
    "text": "4.7 Example analysis\n\n\n\nSuppose you are given a univariate time series \\(Y_t\\).\nFirst, plot the data (Figure 4.8).\n\n\nCode\nggplot2::autoplot(Y) +\n    theme_light()\n\n\n\n\n\nFigure 4.8: Plot of the time series \\(Y_t\\).\n\n\n\n\nIf no obvious trend, plot sample ACF and PACF (Figure 4.9).\n\n\nCode\np1 <- forecast::ggAcf(Y) +\n    ggtitle(\"\") +\n    theme_light()\np2 <- forecast::ggAcf(Y, type = \"partial\") +\n    ggtitle(\"\") +\n    theme_light()\np1 + p2 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.9: Estimated ACF and PACF of \\(Y_t\\).\n\n\n\n\nBased on the plots (Figure 4.9), ACF decays faster than linearly (some indication of stationarity), PACF is non-zero up to lag 2 (cuts off after lag 2). Knowing the expected behavior of ACF and PACF for ARMA models (see Table 4.1), ARMA(2,0) can be suggested in this case.\nGiven the potential sample variability affecting the estimates of ACF and PACF, there could be other model specifications close to the selected one that may perform similarly or even slightly better when compared to the selected based on the visual analysis. For example, we can check other suitable combinations within the maximal lag of 2 we identified: ARMA(1,0), (2,0), (0,1), (1,1), (2,1), (0,2), (1,2), and ARMA(2,2). With so many choices, we need some quantitative criteria to select the best model.\nThe code below uses a ‘manual’ approach of calculating Akaike and Bayesian information criteria (AIC and BIC) to select the model from the specified set, but see the next section for R functions that do it more automatically.\n\n\nCode\n# Combinations of p and q, without p = q = 0\nresults <- expand.grid(p = c(0, 1, 2), q = c(0, 1, 2))[-1, ] \nfor (i in 1:nrow(results)) {\n    mod <- arima(Y, order = c(results$p[i], 0, results$q[i]))\n    results$aic[i] <- mod$aic\n    results$bic[i] <- BIC(mod)\n}\nresults\n\n\n#>   p q aic bic\n#> 2 1 0 851 862\n#> 3 2 0 838 853\n#> 4 0 1 987 998\n#> 5 1 1 839 854\n#> 6 2 1 840 859\n#> 7 0 2 915 929\n#> 8 1 2 841 859\n#> 9 2 2 842 864\n\n\n\n4.7.1 Automatic model selection\nFor example, the function stats::ar() by default uses the Yule–Walker method to estimate the coefficients and AIC to select the best order for AR(\\(p\\)) model, where the maximum of \\(p\\) can be set by user.\n\n\nCode\nar(Y, order.max = 11)\n\n\n#> \n#> Call:\n#> ar(x = Y, order.max = 11)\n#> \n#> Coefficients:\n#>     1      2  \n#> 0.612  0.220  \n#> \n#> Order selected 2  sigma^2 estimated as  0.939\n\n\nThe function funtimes::ARest() is a wrapper for stats::ar(). It adds a difference-based estimator by Hall and Van Keilegom (2003), which is now used by default, and also switches to BIC as the model selection criterion. The selection is still done only for the order \\(p\\) in AR(\\(p\\)), without considering the possibility of MA terms.\n\n\nCode\n# Order selection with BIC and estimation with HVK\nfuntimes::ARest(Y)\n\n\n#> [1] 0.571 0.176\n\n\nCode\n# To check the correspondence with ar(), change the method and fix the ar.order\n# (turn off automatic selection):\nfuntimes::ARest(Y, ar.method = \"yw\", ar.order = 2, BIC = FALSE)\n\n\n#> [1] 0.612 0.220\n\n\n\n\n\n\n\n\nNote\n\n\n\nBIC selects more parsimonious models than AIC does. Such simpler models are usually preferred for forecasting tasks.\n\n\nThe function fable::ARIMA() searches over possible ARMA models within some order constraints (the default maximal orders and search strategy are different from the two functions above – see help files for each function in R). This function is a successor of the function forecast::auto.arima() and is probably the most comprehensive R function for estimating and selecting ARMA models. It allows to fit and select not only ARMA(\\(p,q\\)) models, but also more complex models that will be described later in the course.\n\n\nCode\nlibrary(fable)\nm <- as_tsibble(Y) %>% \n    model(ARIMA(Y ~ 1, ic = \"bic\"))\nreport(m)\n\n\n#> Series: Y \n#> Model: ARIMA(2,0,0) w/ mean \n#> \n#> Coefficients:\n#>          ar1     ar2  constant\n#>       0.6100  0.2184    0.0466\n#> s.e.  0.0562  0.0562    0.0545\n#> \n#> sigma^2 estimated as 0.9383:  log likelihood=-415\n#> AIC=838   AICc=838   BIC=853\n\n\nIn all considered cases the model ARMA(2,0), which is equivalent to AR(2), was selected. Our next step is the residual diagnostics – check the residuals for homoscedasticity, uncorrelatedness, and normality (Figure 4.10).\n\n\nCode\nfeasts::gg_tsresiduals(m)\n\n\n\n\n\nFigure 4.10: Residual diagnostics for the ARMA(2,0) model estimated using the package fable.\n\n\n\n\nBased on the plots, the residuals do not violate the assumptions, so proceed with forecasting (Figure 4.11).\n\n\nCode\nm %>%\n    forecast(h = 10) %>%\n    autoplot(Y) + \n    theme_light()\n\n\n\n\n\nFigure 4.11: Predictions 10 steps ahead from the ARMA(2,0) model estimated using the package fable.\n\n\n\n\nHowever, the syntax of the package fable is quite different from its alternatives. See Figure 4.12 with results still plotted using the package ggplot2, but obtained using the base-R functions. Also see the base-R function tsdiag() for residual diagnostics.\n\n\nCode\nm <- arima(Y, order = c(2, 0, 0))\ne <- m$residuals\np1 <- ggplot2::autoplot(e) + \n    ylab(\"Residuals\") +\n    theme_light()\np2 <- forecast::ggAcf(e) +\n    ggtitle(\"\") +\n    theme_light()\np3 <- ggplot(data.frame(e = e), aes(sample = e)) + \n    stat_qq() + \n    stat_qq_line() +\n    xlab(\"Quantiles of N(0,1)\") +\n    ylab(\"Sample quantiles\") +\n    theme_light()\np4 <- ggplot2::autoplot(forecast(m, h = 10)) + \n    theme_light()\np1 / (p2 + p3) / p4 +\n    plot_annotation(tag_levels = 'A')\n\n\n\n\n\nFigure 4.12: Residual diagnostics and predictions 10 steps ahead from the ARMA(2,0) model estimated using the base-R function stats::arima().\n\n\n\n\nUsing the estimated residuals, \\(\\hat{\\epsilon}_t = Y_t - \\hat{Y}_t\\), we can get the ‘fitted’ values, \\(\\hat{Y}_t\\) (technically, these are one-step-ahead predictions), see Figure 4.13.\n\n\nCode\nFit <- Y - e\nggplot2::autoplot(Y) + \n    autolayer(Fit) +\n    ylab(\"Y\") +\n    theme_light()\n\n\n\n\n\nFigure 4.13: Oberved time series (black) and one-step-ahead predictions (red) from the ARMA(2,0) model."
  },
  {
    "objectID": "l04_arma.html#conclusion",
    "href": "l04_arma.html#conclusion",
    "title": "4  Autoregressive Moving Average (ARMA) Models",
    "section": "4.8 Conclusion",
    "text": "4.8 Conclusion\nWe defined AR, MA, and ARMA models and showed that time series processes corresponding to such models satisfy the conditions of weak stationarity.\nUsing ACF and PACF plots, we can select the orders \\(p\\) and \\(q\\) to specify an ARMA model, but we also can use a variety of functions running different evaluations and tests for competitive models.\nWe face the choice of different methods to estimate (i.e., estimators) and methods to compare models.\nMost often used estimators for ARMA models are\n\nmaximum likelihood,\nYule–Walker, and\nBurg.\n\nThe OLS-based estimator is used less frequently.\nMethods to compare competing models may involve\n\nlog-likelihood, MAE, RMSE and information criteria to assess in-sample (training) goodness-of-fit;\ndiagnostics checks of residuals (discard models if their residuals do not pass the tests);\ncomparisons of models on a testing set (PMAE, PRMSE, coverage and width of prediction intervals).\n\nFor each of these options, there are multiple R functions available or can be written from scratch.\n\n\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and forecasting, 2nd edn. Springer, New York\n\n\nHall P, Van Keilegom I (2003) Using difference-based methods for inference in nonparametric regression with time series errors. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 65:443–456. https://doi.org/10.1111/1467-9868.00395\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications with r examples, 3-EZ. Free Texts in Statistics"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "The R packages used in this book include (in alphabetic order):\n\n\n\n\ncar (Fox et al. 2021)\ndplyr (Wickham et al. 2022b)\nEcdat (Croissant and Graves 2020)\nfable (O’Hara-Wild et al. 2022a)\nfeasts (O’Hara-Wild et al. 2022b)\nfma (Hyndman 2020)\nforecast (Hyndman et al. 2022)\nfuntimes (Lyubchich and Gel 2022)\nggplot2 (Wickham et al. 2022a)\nknitr (Xie 2022)\nlawstat (Gastwirth et al. 2020)\nlmtest (Hothorn et al. 2022)\npatchwork (Pedersen 2020)\nplotly (Sievert et al. 2022)\npracma (Borchers 2021)\nrandtests (Caeiro and Mateus 2014)\n\n\n\n\n\n\n\n\n\n\n\nBorchers HW (2021) Pracma: Practical numerical math functions. R package version 2.3.6, https://CRAN.R-project.org/package=pracma\n\n\nCaeiro F, Mateus A (2014) Randtests: Testing randomness in r. R package version 1.0, https://CRAN.R-project.org/package=randtests\n\n\nCroissant Y, Graves S (2020) Ecdat: Data sets for econometrics. R package version 0.3-9, https://www.r-project.org\n\n\nFox J, Weisberg S, Price B (2021) Car: Companion to applied regression. R package version 3.0-12, https://CRAN.R-project.org/package=car\n\n\nGastwirth JL, Gel YR, Hui WLW, et al (2020) Lawstat: Tools for biostatistics, public policy, and law. R package version 3.4, https://CRAN.R-project.org/package=lawstat\n\n\nHothorn T, Zeileis A, Farebrother RW, Cummins C (2022) Lmtest: Testing linear regression models. R package version 0.9-40, https://CRAN.R-project.org/package=lmtest\n\n\nHyndman R (2020) Fma: Data sets from \"forecasting: Methods and applications\" by makridakis, wheelwright & hyndman (1998). R package version 2.4, https://CRAN.R-project.org/package=fma\n\n\nHyndman R, Athanasopoulos G, Bergmeir C, et al (2022) Forecast: Forecasting functions for time series and linear models. R package version 8.16, https://CRAN.R-project.org/package=forecast\n\n\nLyubchich V, Gel YR (2022) Funtimes: Functions for time series analysis. R package version 8.3\n\n\nO’Hara-Wild M, Hyndman R, Wang E (2022a) Fable: Forecasting models for tidy time series. R package version 0.3.2, https://CRAN.R-project.org/package=fable\n\n\nO’Hara-Wild M, Hyndman R, Wang E (2022b) Feasts: Feature extraction and statistics for time series. R package version 0.3.0, https://CRAN.R-project.org/package=feasts\n\n\nPedersen TL (2020) Patchwork: The composer of plots. R package version 1.1.1, https://CRAN.R-project.org/package=patchwork\n\n\nSievert C, Parmer C, Hocking T, et al (2022) Plotly: Create interactive web graphics via plotly.js. R package version 4.10.1, https://CRAN.R-project.org/package=plotly\n\n\nWickham H, Chang W, Henry L, et al (2022a) ggplot2: Create elegant data visualisations using the grammar of graphics. R package version 3.3.6, https://CRAN.R-project.org/package=ggplot2\n\n\nWickham H, François R, Henry L, Müller K (2022b) Dplyr: A grammar of data manipulation. R package version 1.0.10, https://CRAN.R-project.org/package=dplyr\n\n\nXie Y (2022) Knitr: A general-purpose package for dynamic report generation in r. R package version 1.38, https://yihui.org/knitr/"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Berk RA (2016) Statistical learning\nfrom a regression perspective, 2nd edn. Springer, Switzerland\n\n\nBorchers HW (2021) Pracma: Practical numerical math functions. R package\nversion 2.3.6, https://CRAN.R-project.org/package=pracma\n\n\nBrockwell PJ, Davis RA (2002) Introduction to time series and\nforecasting, 2nd edn. Springer, New York\n\n\nCaeiro F, Mateus A (2014) Randtests: Testing randomness in r. R package\nversion 1.0, https://CRAN.R-project.org/package=randtests\n\n\nChatfield C (2000) Time-series forecasting. CRC Press, Boca Raton\n\n\nChatterjee S, Hadi AS (2006) Regression analysis by example, 4th edn.\nJohn Wiley & Sons, Hoboken, New Jersey\n\n\nChatterjee S, Simonoff JS (2013) Handbook of regression analysis. John\nWiley & Sons, Hoboken, New Jersey\n\n\nCleveland WS (1979) Robust locally weighted regression and smoothing\nscatterplots. Journal of the American Statistical Association\n74:829–836. https://doi.org/10.1080/01621459.1979.10481038\n\n\nCroissant Y, Graves S (2020) Ecdat: Data sets for econometrics. R\npackage version 0.3-9, https://www.r-project.org\n\n\nFox J, Weisberg S, Price B (2021) Car: Companion to applied regression.\nR package version 3.0-12, https://CRAN.R-project.org/package=car\n\n\nGastwirth JL, Gel YR, Hui WLW, et al (2020) Lawstat: Tools for\nbiostatistics, public policy, and law. R package version 3.4, https://CRAN.R-project.org/package=lawstat\n\n\nHall P, Van Keilegom I (2003) Using difference-based methods for\ninference in nonparametric regression with time series errors. Journal\nof the Royal Statistical Society: Series B (Statistical Methodology)\n65:443–456. https://doi.org/10.1111/1467-9868.00395\n\n\nHothorn T, Zeileis A, Farebrother RW, Cummins C (2022) Lmtest: Testing\nlinear regression models. R package version 0.9-40, https://CRAN.R-project.org/package=lmtest\n\n\nHyndman R (2020) Fma: Data sets from \"forecasting: Methods and\napplications\" by makridakis, wheelwright & hyndman (1998). R package\nversion 2.4, https://CRAN.R-project.org/package=fma\n\n\nHyndman R, Athanasopoulos G, Bergmeir C, et al (2022) Forecast:\nForecasting functions for time series and linear models. R package\nversion 8.16, https://CRAN.R-project.org/package=forecast\n\n\nKirchgässner G, Wolters J (2007) Introduction to modern\ntime series analysis. Springer-Verlag, Berlin\n\n\nLyubchich V, Gel YR (2022) Funtimes: Functions for time series analysis.\nR package version 8.3\n\n\nO’Hara-Wild M, Hyndman R, Wang E (2022a) Fable: Forecasting models for\ntidy time series. R package version 0.3.2, https://CRAN.R-project.org/package=fable\n\n\nO’Hara-Wild M, Hyndman R, Wang E (2022b) Feasts: Feature extraction and\nstatistics for time series. R package version 0.3.0, https://CRAN.R-project.org/package=feasts\n\n\nPedersen TL (2020) Patchwork: The composer of plots. R package version\n1.1.1, https://CRAN.R-project.org/package=patchwork\n\n\nShumway RH, Stoffer DS (2011) Time series analysis\nand its applications with r examples, 3rd edn. Springer, New York\n\n\nShumway RH, Stoffer DS (2014) Time series analysis and its applications\nwith r examples, 3-EZ. Free Texts in Statistics\n\n\nSievert C, Parmer C, Hocking T, et al (2022) Plotly: Create interactive\nweb graphics via plotly.js. R package version 4.10.1, https://CRAN.R-project.org/package=plotly\n\n\nWickham H, Chang W, Henry L, et al (2022a) ggplot2: Create elegant data\nvisualisations using the grammar of graphics. R package version 3.3.6,\nhttps://CRAN.R-project.org/package=ggplot2\n\n\nWickham H, François R, Henry L, Müller K (2022b) Dplyr: A grammar of\ndata manipulation. R package version 1.0.10, https://CRAN.R-project.org/package=dplyr\n\n\nXie Y (2022) Knitr: A general-purpose package for dynamic report\ngeneration in r. R package version 1.38, https://yihui.org/knitr/"
  }
]
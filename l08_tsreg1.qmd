---
output: html_document
editor_options:
    chunk_output_type: console
---

```{r, echo=FALSE}
library(dplyr)
library(ggplot2)
library(patchwork)
options(digits = 3)
```

# Time Series Regression with Trends

The goal of this lecture is to introduce common methods of handling nonstationary time series in regression models. You should become familiar with the problem of spurious correlation (regression) and approaches helping to avoid this problem.

**Objectives**

1. Learn three alternative ways of handling trends and/or seasonality to avoid spurious results: incorporate time effects into regression model, use deviations from trend or differenced series.
1. Introduce the concept of cointegration, learn how to detect it and model using an error correction model.

**Reading materials**

* 
* Chapter 6 in @Kirchgassner:Wolters:2007 on cointegration


## Spurious correlation}

Using the standard normal distribution, $N(0,1)$, simulate two series of length $T=300$ (Figure~\ref{f:orig}):
<<>>=
set.seed(123) #set seed for reproducible random number generation
T <- 300
X <- rnorm(T)
Y <- rnorm(T)
@


<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))
par(pty = "m") #s for square plotting region
@
\begin{figure}
<<>>=
plot.ts(Y, las = 1)
plot.ts(X, las = 1, col = 'blue')
@
\caption{Original, stationary and independent, time series.}
\label{f:orig}
\end{figure}


These are independent and identically distributed (i.i.d.) random variables: each of the $1,\ldots,T$ values in $X$ and $Y$ was drawn independently from other values from the same distribution. Two random variables are independent if the realization of one does not affect the probability distribution of the other. Independence is a strong condition, it also implies (includes) that the values are not correlated. This is true both for the values within the series $X$ and $Y$, and across $X$ and $Y$ (i.e., $X$ and $Y$ are not autocorrelated, nor correlated with each other). This is in asymptotics (as the sample size increases infinitely).

In finite samples, we may observe that a point estimate of correlation coefficient even for such ideal series is not equal to zero, but it will be usually not statistically significant. See the results below (confidence interval and $p$-value):
<<>>=
cor.test(X, Y)
@

Not many time series behave like that in real life -- often we observe some trends. Let's add linear trends to our simulated data, for example, trends going in the same direction, but magnitudes of the slopes may be different. Here we use linear increasing trends, i.e., with positive slopes\footnote{Refrain from writing ``positive trends'' (or ``negative trends'') because it could be confused with ``good'' or ``beneficial'' trends. For example, a linear decrease in unemployment rate is a \emph{positive} (good) trend for that region, but it is a trend with a \emph{negative} slope; a linear trend in pollutant concentrations with a \emph{positive} slope (going upward) shows a \emph{negative} (worsening) tendency.} (Figure~\ref{f:TSwithTrends}).
<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))
par(pty = "m") #s for square plotting region
@
\begin{figure}
<<>>=
X <- X + c(1:T)/95
Y <- Y + c(1:T)/50
plot.ts(Y, las = 1, ylim = c(-2, 8))
plot.ts(X, las = 1, ylim = c(-2, 8), col = 'blue')
@
\caption{Time series with trends.}
\label{f:TSwithTrends}
\end{figure}

<<>>=
cor.test(X, Y)
@
Now correlation of $X$ and $Y$ is strong and `statistically significant,' but this is not necessarily because these series are so strongly related with each other. Likely, some other factors are just driving the dynamics (trends) of $X$ and $Y$ in the same direction. Also, recall the general formula for computing correlation (or autocorrelation) of time series and that we need to subtract time-dependent means (not just a mean calculated over the whole period), for example, as in~\eqref{eq:Cov}.

For example, city's growing population may result in more police officers and heavier pollution at the same time, partly because more people will drive their vehicles in the city. An attempt to correlate/regress pollution with number of police officers will produce so-called \emph{spurious correlation/regression}. The results of such calculations will be likely statistically significant, however, is pollution directly related to the number of police officers? Will dismissal of a police officer help to make the air cleaner?

Not only common increasing/decreasing trends, but also other systematic changes (such as seasonality) may be responsible for spurious correlation effects. For example, both high ice cream sales and harmful algal blooms occur in warm weather conditions and may be `significantly' correlated, suggesting to ban ice cream for the sake of a safer environment. See more interesting examples of spurious correlation on \url{http://www.tylervigen.com/spurious-correlations}.

Sometimes, some simple tricks might be applicable to avoid spurious results. For example, analyze not the raw numbers, but the \emph{rates} that remove the effect of population growth in a city: crime rate per 100,000 inhabitants, number of people older 70 per 1,000 inhabitants, etc. For more general approaches, see the next section.


## Common approaches to regressing time series with trends}

What if we are given time series with trends (like in Figure~\ref{f:TSwithTrends}), we do not know the data generating process (DGP), and we want to use these series in regression analysis.

In general, there are three alternative ways of dealing with trends in regression:
\begin{enumerate}
\item Incorporate time effect into the model;
\item Use deviations from trends (i.e., model and remove trends), or
\item Use differenced series (i.e., remove trends by differencing).
\end{enumerate}

After these three approaches, we consider a special case of cointegration (Section~\ref{sec:cointegration}).


### Incorporate time effects}
\label{sec:incorporateTime}

Based on the time series plots (Figure~\ref{f:TSwithTrends}), linear time trend would fit these data, since we see a linear increase of values with time (so we add linear time effect $t$ in our model). Alternatively, e.g., if we observed parabolic structure, we could include $t+t^2$ or other form of trend.
<<>>=
t <- c(1:T)
mod_time <- lm(Y ~ X + t)
summary(mod_time)
@
This model looks like this:
$$
Y_t = \beta_0 + \beta_1X_t + \beta_2t + \epsilon_t,
$$
estimated as:
\begin{align}
\widehat{Y}_t &= \hat{\beta}_0 + \hat{\beta}_1X_t + \hat{\beta}_2t,\\
\widehat{Y}_t &=  0.0008 - 0.0637X_t + 0.0207t.
\end{align}

In the above model, the (highly statistically significant) time term took over the trend influence, thus, the coefficient for $X$ shows the `real' relationship between $Y$ and $X$. Notice, the coefficient $\beta_1$ is not statistically significant, what we expected.

### Use deviations from trends}

Here we fit a separate time trend (may be of different form for each time series: linear, quadratic, log, etc.) for each variable and find deviations from these trends. Based on the plots (Figure~\ref{f:TSwithTrends}), linear trends are appropriate here:
$$
Y_t = a_0 + a_1t + e_{(Y)t}; \quad X_t = b_0 + b_1t + e_{(X)t},
$$
where $e_{(Y)t}$ and $e_{(X)t}$ are the trend residuals for the series $Y_t$ and $X_t$, respectively.

After the trend coefficients $a_0$, $a_1$, $b_0$, and $b_1$ are estimated,
<<>>=
MY <- lm(Y ~ t)
MX <- lm(X ~ t)
@
the smoothed series are
$$
\widetilde{Y}_t = \hat{a}_0 + \hat{a}_1t; \quad \widetilde{X}_t = \hat{b}_0 + \hat{b}_1t
$$
and the estimated trend residuals are (Figure~\ref{f:trendresid})
$$
\hat{e}_{(Y)t} = Y_t - \widetilde{Y}_t\quad\text{and}\quad \hat{e}_{(X)t} = X_t - \widetilde{X}_t.
$$


<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))
par(pty = "m") #s for square plotting region
@
\begin{figure}[!h]
<<>>=
ts.plot(MY$resid)
ts.plot(MX$resid, col = 'blue')
@
\caption{Residuals from individually estimated linear trends.}
\label{f:trendresid}
\end{figure}


Use the residuals in our regression model in place of the original variables:
$$
\hat{e}_{(Y)t} = \beta_0 + \beta_1\hat{e}_{(X)t} + \epsilon_t.
$$
<<>>=
mod_devTrend <- lm(MY$resid ~ MX$resid)
summary(mod_devTrend)
@

We got the result (the relationship between the variables is not statistically significant) similar to running the model incorporating time trends in Section~\ref{sec:incorporateTime}. Again, it is a reasonable result judging based on how the time series were simulated (independent, although with trends in the same direction).


### Use differenced series}
\label{sec:RegTrendDiff}

Instead of assuming a deterministic trend as in the previous subsections, we can try to eliminate a stochastic trend by differencing the time series. We define the lag-1 difference operator $\Delta$ by
$$
\Delta X_t = X_t - X_{t-1} = (1-B)X_t,
$$
where $B$ is the backward shift operator, $BX_t = X_{t-1}$.

There are tests developed in econometrics to find the appropriate order of differences (unit-root tests). Here, however, we will use the rule of thumb: for time trends looking linear (our case, see Figure~\ref{f:TSwithTrends}) use 1st order differences, for parabolic shapes -- 2nd order differences. After differencing, the series should look stationary.

1st order differences for our series (Figure~\ref{f:TSdiffed}):
<<>>=
D1X <- diff(X)
D1Y <- diff(Y)
@


<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))
par(pty = "m") #s for square plotting region
@
\begin{figure}[!h]
<<>>=
ts.plot(D1Y)
ts.plot(D1X, col='blue')
@
\caption{First-order differences of the individual time series.}
\label{f:TSdiffed}
\end{figure}

The series of first-order differences look stationary (see Figure~\ref{f:TSdiffed}). Use these differenced series instead of the original time series in a regression model:
$$
\Delta Y_t = \beta_0 + \beta_1 \Delta X_t + \epsilon_t.
$$

<<>>=
mod_diff <- lm(D1Y ~ D1X)
summary(mod_diff)
@

As expected (since the original series $X$ and $Y$, before adding the trend, are uncorrelated), the coefficient $\beta_1$ and the whole model are not statistically significant.

### Wrong approach (do not repeat at home): spurious regression}

What if we forget about the three approaches above and just use the time series with trends in a regression model:
$$
Y_t = \beta_0 + \beta_1X_t + \epsilon_t.
$$

<<>>=
badModel <- lm(Y ~ X)
summary(badModel)
@

The bad model shows spurious `statistically significant' effects, which are not true.

\textbf{Beware of trends!}



### Example: predicting sales of home appliances}
\label{s:EXappl_regTS1}
Recall the dishwashers example from the first lecture. Let's use the above methods for a regression of time series of dishwasher shipments ($DISH_t$) and residential investments ($RES_t$).

First, look at the time series plots of the raw data (Figure~\ref{f:TSdish}).
<<>>=
D <- read.delim("./data/dish.txt")
@


<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))
par(pty = "m") #s for square plotting region
@
\begin{figure}[!h]
<<>>=
plot.ts(D$DISH, las = 1)
plot.ts(D$RES, las = 1)
@
\caption{Time series plots of the dishwasher shipments and residential investments.}
\label{f:TSdish}
\end{figure}


\subsubsection{Incorporate time effects}
\label{sec:ExTimeEff}

$$\label{eq:dish_timetrend}
DISH_t = \beta_0 + \beta_1 RES_t + \beta_2 t + \epsilon_t
$$

<<>>=
t <- c(1:nrow(D))
M_time <- lm(DISH ~ RES + t, data = D)
summary(M_time)
@

<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 3))
par(pty = "m") #s for square plotting region
@
\begin{figure}[!h]
<<>>=
plot.ts(M_time$residuals, las = 1)
abline(h = 0, col = "blue", lwd = 2)
acf(M_time$residuals, las = 1)
qqnorm(M_time$residuals, las = 1)
qqline(M_time$residuals)
@
\caption{Diagnostics plots for residuals $\hat{\epsilon}_t$ from~\eqref{eq:dish_timetrend}.}
\label{f:dish_timetrend}
\end{figure}


<<>>=
shapiro.test(M_time$residuals)
lawstat::runs.test(M_time$residuals, plot.it = FALSE)
@

\subsubsection{Use differenced time series}

$$\label{eq:dish_diff}
\Delta DISH_t = \beta_0 + \beta_1 \Delta RES_t + \epsilon_t
$$

<<>>=
D_DISH <- diff(D$DISH)
D_RES <- diff(D$RES)
@

<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 3))
par(pty = "m") #s for square plotting region
@
\begin{figure}[!h]
<<>>=
plot.ts(D_DISH)
plot.ts(D_RES)
plot(x = D_RES, y = D_DISH)
@
\caption{Time differences of dishwasher shimpents and resedential investments, and a scatterplot of the differences.}
\label{f:TSdish_timetrend}
\end{figure}


<<>>=
M_diff <- lm(D_DISH ~ D_RES)
summary(M_diff)
@

<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 3))
par(pty = "m") #s for square plotting region
@
\begin{figure}[!h]
<<>>=
plot.ts(M_diff$residuals, las = 1)
abline(h = 0, col = "blue", lwd = 2)
acf(M_diff$residuals, las = 1)
qqnorm(M_diff$residuals, las = 1)
qqline(M_diff$residuals)
@
\caption{Diagnostics plots for residuals $\hat{\epsilon}_t$ from~\eqref{eq:dish_diff}.}
\label{f:dish_diff}
\end{figure}


<<>>=
shapiro.test(M_diff$residuals)
lawstat::runs.test(M_diff$residuals, plot.it = FALSE)
@


\paragraph{Conclusion} Now we can incorporate trend effects into our models, using the three considered approaches. Next step would be also to incorporate autocorrelation structure in the residuals (the simulated example considered here used independent normally distributed noise, so it was an artificial ideal case of no autocorrelation -- we usually encounter autocorrelations, e.g., see residual diagnostics in Section~\ref{sec:ExTimeEff}).


## Cointegration}
\label{sec:cointegration}

Generally, cointegration might be characterized by two or more I(1)
variables indicating a common long-run development, i.e., the variables do not drift
away from each other except for transitory fluctuations. This defines a statistical
equilibrium that, in empirical analysis, can often be interpreted
as a long-run [economic] relation.

In other words, two I(1) series $X_t$ and $Y_t$ are cointegrated if their linear combination $u_t$ is I(0):
$$\label{eq:longrun}
Y_t - \beta X_t = u_t.
$$

Cointegration means a common stochastic trend. The vector $(1, -\beta)^{\top}$ is called the cointegration vector.

### Two-step Engle--Granger method}

\begin{enumerate}
\item Estimate long-run relationship, i.e., regression in levels as in~\eqref{eq:longrun}, and
test residuals for I(0).
\item If the residual series $u_t$ is I(0), use it in \emph{error correction model} (ECM) regression
$$
\begin{split}\label{eq:ecm}
\Delta Y_t &= a_0 -\gamma_Y(Y_{t-1}-\beta X_{t-1})+\sum_{j=1}^{n_X}a_{Xj}\Delta X_{t-j}+\sum_{j=1}^{n_Y}a_{Yj}\Delta Y_{t-j} + u_{Y,t},\\
\Delta X_t &= b_0 +\gamma_X(Y_{t-1}-\beta X_{t-1})+\sum_{j=1}^{k_X}b_{Xj}\Delta X_{t-j}+\sum_{j=1}^{k_Y}b_{Yj}\Delta Y_{t-j} + u_{X,t},
\end{split}
$$
where $u_X$ and $u_Y$ are pure random processes. If $X_t$ and $Y_t$ are cointegrated, at least one $\gamma_i$, $i = X, Y$, has to be different from zero.
\end{enumerate}

OLS estimator is super consistent, convergence $T$. However, OLS can be biased in small samples.

The representation~(\ref{eq:ecm}) has the advantage that it only
contains stationary variables, although the underlying relation is between
nonstationary (I(1)) variables. Thus, if the variables are cointegrated and
the cointegration vector is known, the traditional statistical procedures
can be applied for estimation and testing.

Simulate time series $X_t$ and $Y_t$ (Figure~\ref{f:XY_i1}):
<<>>=
set.seed(1)
e1 <- rnorm(250, mean = 0, sd = 0.5)
e2 <- rnorm(250, mean = 0, sd = 0.5)
u.ar3 <- arima.sim(model =  list(ar = c(0.6, -0.2, 0.1)), n = 250, innov = e1)
X <- cumsum(e2)
Y <- u.ar3 + 0.5*X
@


<<echo = FALSE>>=
par(mar = c(4, 5, 1, 1) + 0.1, mgp = c(3, 1, 0), mfrow = c(1, 2))
par(pty = "m") #s for square plotting region
@
\begin{figure}[h!]
<<>>=
plot.ts(X, las = 1)
plot.ts(Y, las = 1)
@
\caption{Simulated I(1) time series.}
\label{f:XY_i1}
\end{figure}


Apply unit-root test to check integration order of each series, using R package \texttt{tseries} \citep{R-tseries}:
<<>>=
library(tseries)
adf.test(X)
adf.test(diff(X))
adf.test(Y)
adf.test(diff(Y))
@
With the confidence 95\%, the ADF test results show that each of the time series, $X_t$ and $Y_t$, are I(1). (However, we have used the test 4 times, without controlling the overall Type~I error.)

Fit the linear regression
$$\label{eqlongr}
Y_t = a + bX_t + u_t.
$$
Vector $[1, -b]$ is the cointegration vector.
Find the linear combination:
<<>>=
U <- lm(Y ~ X)$resid
adf.test(U)
@
While each of the time series $X_t$ and $Y_t$ is I(1), the resulting residual series $u_t \sim \mathrm{I}(0)$, thus we conclude, $X_t$ and $Y_t$ are cointegrated.

Apply a simple error correction model (with $n_X = n_Y = 1$), using  R package \texttt{dynlm} \citep{R-dynlm} or just specify lags using R package \texttt{dplyr} \citep{R-dplyr}:
<<>>=
library(dynlm)
# Error correction term
ect <- U[-length(U)]
# Differenced series
dy <- diff(Y)
dx <- diff(X)
# Model using dynlm
ecmdat1 <- cbind(dy, dx, ect)
ecm1 <- dynlm(dy ~ L(ect, 1) + L(dy, 1) + L(dx, 1), data = ecmdat1)
summary(ecm1)
# Model using lm
ecm2 <- lm(dy ~ dplyr::lag(ect, 1) + dplyr::lag(as.vector(dy), 1) + dplyr::lag(as.vector(dx), 1))
summary(ecm2)
@

There is also R package \texttt{ecm} \citep{R-ecm}, but it uses a modified formulation of the model, see details for the function \texttt{ecm::ecm}.


Notice how the series were simulated. If instead of $Y_t$ we simulate just another I(1) process $W_t$ (without a common stochastic trend), the results will be different.
<<>>=
W <- cumsum(rnorm(250))
U2 <- lm(W ~ X)$resid
adf.test(U2)
@
The linear combination of individually integrated $W_t$ and $X_t$ does not produce a stationary time series, thus, $W_t$ and $X_t$ are not cointegrated.


### Johansen test}

This test allows for more than one cointegrating relationship. The null hypothesis for the trace test is that the number of cointegration vectors is $r<k$, vs. the alternative that $r=k$. Testing proceeds sequentially for $k=1,2,\ldots$; and the first non-rejection of the null hypothesis is taken as an estimate of $r$.

Using R package \texttt{urca} \citep{R-urca}
<<>>=
library(urca)
vecm <- ca.jo(cbind(Y, X, W))
summary(vecm)
@


If two time series are cointegrated, then the usual regression~\eqref{eqlongr} is the so-called long-run equilibrium relation, or attractor, i.e., relationship between $X_t$ and $Y_t$ can be explained by~\eqref{eqlongr} in a long run. Model~\eqref{eqlongr} is applied for estimation, not for testing (see Figure~6.1 in \citealp{Kirchgassner:Wolters:2007} on highly dispersed $t$-statistic). The error correction model~\eqref{eq:ecm} should be estimated for testing ($p$-values from the ECM can be used for testing; also see Chapter~6 in \citealp{Kirchgassner:Wolters:2007}).

%https://www.quantstart.com/articles/Johansen-Test-for-Cointegrating-Time-Series-Analysis-in-R/

